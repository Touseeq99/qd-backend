2025-07-07 22:33:43,790 - root - INFO - Pre-warming the model...
2025-07-07 22:33:47,838 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 22:33:50,859 - root - INFO - Loading Groq LLM...
2025-07-07 22:33:52,492 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04415750503540039, 'init_vectorstore': 7.024350881576538, 'setup_retriever': 0.0, 'load_llm': 1.6278815269470215, 'create_chains': 0.00484919548034668}
2025-07-07 22:33:52,493 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.70s
2025-07-07 22:33:55,787 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:33:55,813 - root - INFO - Model pre-warmed successfully
2025-07-07 22:34:19,596 - root - INFO - Received question: if i have 14 more excess leave and my salary is 200k how much will be the deduction
2025-07-07 22:34:19,596 - root - INFO - Chain init: 0.00s
2025-07-07 22:34:22,719 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:34:22,723 - root - INFO - Question processing: 3.13s
2025-07-07 22:34:22,723 - root - INFO - ⏱️ ask_hr took 3.13 seconds
2025-07-07 22:35:15,381 - root - INFO - Received question: if i have 14 more excess leave and my salary is 200k how much will be the deduction
2025-07-07 22:35:15,382 - root - INFO - Chain init: 0.00s
2025-07-07 22:35:18,369 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:35:18,374 - root - INFO - Question processing: 2.99s
2025-07-07 22:35:18,374 - root - INFO - ⏱️ ask_hr took 2.99 seconds
2025-07-07 22:51:15,304 - root - INFO - Pre-warming the model...
2025-07-07 22:51:17,936 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 22:51:21,074 - root - INFO - Loading Groq LLM...
2025-07-07 22:51:22,781 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0826268196105957, 'init_vectorstore': 5.6863017082214355, 'setup_retriever': 0.0, 'load_llm': 1.6676530838012695, 'create_chains': 0.03957176208496094}
2025-07-07 22:51:22,783 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.48s
2025-07-07 22:51:26,750 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:51:26,782 - root - INFO - Model pre-warmed successfully
2025-07-07 22:52:34,300 - root - INFO - Pre-warming the model...
2025-07-07 22:52:36,534 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 22:52:38,900 - root - INFO - Loading Groq LLM...
2025-07-07 22:52:40,528 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03748679161071777, 'init_vectorstore': 4.561020851135254, 'setup_retriever': 0.0, 'load_llm': 1.623732328414917, 'create_chains': 0.0034945011138916016}
2025-07-07 22:52:40,528 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.23s
2025-07-07 22:52:43,066 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:52:43,089 - root - INFO - Model pre-warmed successfully
2025-07-07 22:52:54,534 - root - INFO - Pre-warming the model...
2025-07-07 22:52:56,945 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 22:52:59,412 - root - INFO - Loading Groq LLM...
2025-07-07 22:53:00,970 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.030800819396972656, 'init_vectorstore': 4.84671425819397, 'setup_retriever': 0.0, 'load_llm': 1.5557043552398682, 'create_chains': 0.002702951431274414}
2025-07-07 22:53:00,970 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.44s
2025-07-07 22:53:04,633 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:53:04,662 - root - INFO - Model pre-warmed successfully
2025-07-07 22:54:48,115 - root - INFO - Pre-warming the model...
2025-07-07 22:54:51,044 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 22:54:54,243 - root - INFO - Loading Groq LLM...
2025-07-07 22:54:55,774 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04067397117614746, 'init_vectorstore': 6.08765172958374, 'setup_retriever': 0.0, 'load_llm': 1.526214838027954, 'create_chains': 0.004527568817138672}
2025-07-07 22:54:55,775 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.66s
2025-07-07 22:54:58,762 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:54:58,784 - root - INFO - Model pre-warmed successfully
2025-07-07 22:55:14,669 - root - INFO - Upload request received - folder: Data, file count: 1
2025-07-07 22:55:14,672 - root - WARNING - File type .doc not allowed. Allowed types: .pdf, .md, .txt, .docx - File: Data/9. QG Code of Conduct.doc
2025-07-07 22:55:14,673 - root - WARNING - HTTPException in upload_folder: File type .doc not allowed. Allowed types: .pdf, .md, .txt, .docx
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 167, in upload_folder
    raise HTTPException(
fastapi.exceptions.HTTPException: 400: File type .doc not allowed. Allowed types: .pdf, .md, .txt, .docx
2025-07-07 22:56:25,303 - root - INFO - Pre-warming the model...
2025-07-07 22:56:27,497 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 22:56:29,694 - root - INFO - Loading Groq LLM...
2025-07-07 22:56:31,187 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0494227409362793, 'init_vectorstore': 4.3413567543029785, 'setup_retriever': 0.0, 'load_llm': 1.49092435836792, 'create_chains': 0.0028471946716308594}
2025-07-07 22:56:31,187 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.88s
2025-07-07 22:56:34,793 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 22:56:34,818 - root - INFO - Model pre-warmed successfully
2025-07-07 22:59:01,573 - root - INFO - Upload request received - folder: Data, file count: 1
2025-07-07 22:59:01,575 - root - ERROR - Unexpected error in upload_folder: [Errno 2] No such file or directory: 'uploaded_folders\\Data\\Data/9. QG Code of Conduct.doc'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 191, in upload_folder
    with open(file_path, "wb") as buffer:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'uploaded_folders\\Data\\Data/9. QG Code of Conduct.doc'
2025-07-07 22:59:59,624 - root - INFO - Pre-warming the model...
2025-07-07 23:00:02,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 23:00:05,546 - root - INFO - Loading Groq LLM...
2025-07-07 23:00:07,120 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03922414779663086, 'init_vectorstore': 5.882715463638306, 'setup_retriever': 0.0, 'load_llm': 1.569838523864746, 'create_chains': 0.0037736892700195312}
2025-07-07 23:00:07,121 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.50s
2025-07-07 23:00:10,166 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:00:10,186 - root - INFO - Model pre-warmed successfully
2025-07-07 23:00:22,498 - root - INFO - Upload request received - folder: Data, file count: 1
2025-07-07 23:00:22,500 - root - INFO - Successfully uploaded 1 files to Data
2025-07-07 23:03:19,787 - root - INFO - Pre-warming the model...
2025-07-07 23:03:22,149 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 23:03:25,832 - root - INFO - Loading Groq LLM...
2025-07-07 23:03:27,363 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04285717010498047, 'init_vectorstore': 6.0008227825164795, 'setup_retriever': 0.0, 'load_llm': 1.5276379585266113, 'create_chains': 0.0036911964416503906}
2025-07-07 23:03:27,364 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.58s
2025-07-07 23:03:30,327 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:03:30,348 - root - INFO - Model pre-warmed successfully
2025-07-07 23:04:01,667 - root - INFO - Ingest request received - directory: uploaded_folders/Data/9. QG Code of Conduct.doc
2025-07-07 23:04:01,668 - root - WARNING - Path is not a directory: uploaded_folders/Data/9. QG Code of Conduct.doc
2025-07-07 23:04:01,668 - root - WARNING - HTTPException in ingest_api: Path is not a directory: uploaded_folders/Data/9. QG Code of Conduct.doc
2025-07-07 23:07:02,545 - root - INFO - Pre-warming the model...
2025-07-07 23:07:05,831 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 23:07:08,792 - root - INFO - Loading Groq LLM...
2025-07-07 23:07:10,334 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.036740779876708984, 'init_vectorstore': 6.2102906703948975, 'setup_retriever': 0.0, 'load_llm': 1.5392723083496094, 'create_chains': 0.0026237964630126953}
2025-07-07 23:07:10,334 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.79s
2025-07-07 23:07:13,820 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:07:13,841 - root - INFO - Model pre-warmed successfully
2025-07-07 23:08:34,759 - root - INFO - Ingest request received - path: uploaded_folders/Data/9. QG Code of Conduct.doc
2025-07-07 23:08:34,760 - root - INFO - Processing single file: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\9. QG Code of Conduct.doc
2025-07-07 23:08:34,765 - root - INFO - Starting document ingestion from: C:\Users\user\AppData\Local\Temp\tmpcn9hk0ou
2025-07-07 23:08:41,709 - langchain_community.document_loaders.directory - ERROR - Error loading file C:\Users\user\AppData\Local\Temp\tmpcn9hk0ou\9. QG Code of Conduct.doc
2025-07-07 23:08:41,711 - root - ERROR - Error during document ingestion: soffice command was not found. Please install libreoffice
on your system and try again.

- Install instructions: https://www.libreoffice.org/get-help/install-howto/
- Mac: https://formulae.brew.sh/cask/libreoffice
- Debian: https://wiki.debian.org/LibreOffice
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\unstructured\partition\common\common.py", line 298, in convert_office_doc
    output = subprocess.run(command, capture_output=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\subprocess.py", line 548, in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\subprocess.py", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\subprocess.py", line 1538, in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [WinError 2] The system cannot find the file specified

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 301, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 70, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 45, in sync_ingest
    documents = loader.load()
                ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_community\document_loaders\directory.py", line 117, in load
    return list(self.lazy_load())
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_community\document_loaders\directory.py", line 195, in lazy_load
    yield from self._lazy_load_file(i, p, pbar)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_community\document_loaders\directory.py", line 233, in _lazy_load_file
    raise e
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_community\document_loaders\directory.py", line 223, in _lazy_load_file
    for subdoc in loader.lazy_load():
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_community\document_loaders\unstructured.py", line 107, in lazy_load
    elements = self._get_elements()
               ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_community\document_loaders\unstructured.py", line 228, in _get_elements
    return partition(filename=self.file_path, **self.unstructured_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\unstructured\partition\auto.py", line 290, in partition
    elements = partition(filename=filename, file=file, **partitioning_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\unstructured\partition\doc.py", line 74, in partition_doc
    convert_office_doc(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\unstructured\partition\common\common.py", line 311, in convert_office_doc
    raise FileNotFoundError(
FileNotFoundError: soffice command was not found. Please install libreoffice
on your system and try again.

- Install instructions: https://www.libreoffice.org/get-help/install-howto/
- Mac: https://formulae.brew.sh/cask/libreoffice
- Debian: https://wiki.debian.org/LibreOffice
2025-07-07 23:08:41,913 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: soffice command was not found. Please install libreoffice
on your system and try again.

- Install instructions: https://www.libreoffice.org/get-help/install-howto/
- Mac: https://formulae.brew.sh/cask/libreoffice
- Debian: https://wiki.debian.org/LibreOffice
2025-07-07 23:20:27,567 - root - INFO - Pre-warming the model...
2025-07-07 23:20:30,258 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 23:20:32,879 - root - INFO - Loading Groq LLM...
2025-07-07 23:20:34,546 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.1333920955657959, 'init_vectorstore': 5.177969217300415, 'setup_retriever': 0.0, 'load_llm': 1.627577304840088, 'create_chains': 0.0398869514465332}
2025-07-07 23:20:34,548 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.98s
2025-07-07 23:20:37,422 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:20:37,471 - root - INFO - Model pre-warmed successfully
2025-07-07 23:20:52,145 - root - INFO - Upload request received - folder: Data, file count: 1
2025-07-07 23:20:52,148 - root - INFO - Successfully uploaded 1 files to Data
2025-07-07 23:21:20,097 - root - INFO - Ingest request received - path: uploaded_folders/Data/9. QG Code of Conduct.docx
2025-07-07 23:21:20,097 - root - INFO - Processing single file: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\9. QG Code of Conduct.docx
2025-07-07 23:21:20,104 - root - INFO - Starting document ingestion from: C:\Users\user\AppData\Local\Temp\tmp11kkwgpf
2025-07-07 23:21:30,960 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 23:21:31,969 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-07 23:21:39,448 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-07 23:21:39,453 - root - INFO - Successfully ingested documents. Chunks ingested: 48
2025-07-07 23:22:01,434 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 23:22:02,339 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-07 23:22:02,516 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-07 23:22:58,967 - root - INFO - Pre-warming the model...
2025-07-07 23:23:01,653 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-07 23:23:04,629 - root - INFO - Loading Groq LLM...
2025-07-07 23:23:06,476 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.06682825088500977, 'init_vectorstore': 5.593188524246216, 'setup_retriever': 0.0, 'load_llm': 1.8431000709533691, 'create_chains': 0.004211902618408203}
2025-07-07 23:23:06,477 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.51s
2025-07-07 23:23:09,918 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:23:09,939 - root - INFO - Model pre-warmed successfully
2025-07-07 23:23:47,760 - root - INFO - Received question: Under what circumstances can the company bypass the issuance of a show cause notice and proceed directly to disciplinary action such as termination or warning? Explain how the severity of misconduct influences this decision.
2025-07-07 23:23:47,762 - root - INFO - Chain init: 0.00s
2025-07-07 23:23:51,657 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:23:51,665 - root - INFO - Question processing: 3.90s
2025-07-07 23:23:51,666 - root - INFO - ⏱️ ask_hr took 3.91 seconds
2025-07-07 23:25:39,222 - root - INFO - Received question: How does the company maintain discretion in its disciplinary procedure, and what implications does this have for employees' expectations of progressive discipline (e.g., warnings, probation, suspension)?
2025-07-07 23:25:39,223 - root - INFO - Chain init: 0.00s
2025-07-07 23:25:43,015 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:25:43,015 - root - INFO - Question processing: 3.79s
2025-07-07 23:25:43,015 - root - INFO - ⏱️ ask_hr took 3.79 seconds
2025-07-07 23:26:39,305 - root - INFO - Received question: What are the responsibilities of an employee who receives a show cause notice, and what potential consequences may arise if they fail to respond within the specified timeframe? Discuss how this aligns with the company’s emphasis on accountability and order.
2025-07-07 23:26:39,342 - root - INFO - Chain init: 0.00s
2025-07-07 23:26:43,503 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-07 23:26:43,505 - root - INFO - Question processing: 4.16s
2025-07-07 23:26:43,505 - root - INFO - ⏱️ ask_hr took 4.20 seconds
2025-07-08 00:44:19,198 - root - INFO - Pre-warming the model...
2025-07-08 00:44:22,148 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 00:44:24,592 - root - INFO - Loading Groq LLM...
2025-07-08 00:44:26,288 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.09421730041503906, 'init_vectorstore': 5.298069715499878, 'setup_retriever': 0.0, 'load_llm': 1.6874747276306152, 'create_chains': 0.008322477340698242}
2025-07-08 00:44:26,288 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.09s
2025-07-08 00:44:29,962 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 00:44:30,007 - root - INFO - Model pre-warmed successfully
2025-07-08 00:44:46,430 - root - INFO - Received question: Under what circumstances can the company bypass the issuance of a show cause notice and proceed directly to disciplinary action such as termination or warning? Explain how the severity of misconduct influences this decision.
2025-07-08 00:44:46,432 - root - INFO - Chain init: 0.00s
2025-07-08 00:44:50,152 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 00:44:50,156 - root - INFO - Question processing: 3.72s
2025-07-08 00:44:50,156 - root - INFO - ⏱️ ask_hr took 3.73 seconds
2025-07-08 07:51:26,906 - root - INFO - Pre-warming the model...
2025-07-08 07:51:26,906 - root - INFO - Pre-warming the model...
2025-07-08 07:51:26,922 - root - INFO - Pre-warming the model...
2025-07-08 07:51:27,180 - root - INFO - Pre-warming the model...
2025-07-08 07:51:27,287 - root - INFO - Pre-warming the model...
2025-07-08 07:51:27,483 - root - INFO - Pre-warming the model...
2025-07-08 07:51:27,972 - root - INFO - Pre-warming the model...
2025-07-08 07:51:28,380 - root - INFO - Pre-warming the model...
2025-07-08 07:51:28,691 - root - INFO - Pre-warming the model...
2025-07-08 07:51:29,385 - root - INFO - Pre-warming the model...
2025-07-08 07:51:30,773 - root - INFO - Pre-warming the model...
2025-07-08 07:51:32,877 - root - INFO - Pre-warming the model...
2025-07-08 07:51:33,274 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:33,279 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:33,285 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:33,491 - root - INFO - Pre-warming the model...
2025-07-08 07:51:34,868 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:35,186 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:35,482 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:35,576 - root - INFO - Pre-warming the model...
2025-07-08 07:51:37,775 - root - INFO - Pre-warming the model...
2025-07-08 07:51:38,174 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:38,275 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:39,369 - root - INFO - Pre-warming the model...
2025-07-08 07:51:39,677 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:39,770 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:39,889 - root - INFO - Pre-warming the model...
2025-07-08 07:51:40,471 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:40,480 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:40,480 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:40,672 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:41,878 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:42,776 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:43,580 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:44,480 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:46,289 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:46,471 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.18459200859069824, 'init_vectorstore': 13.378309726715088, 'setup_retriever': 0.007261514663696289, 'load_llm': 5.69523811340332, 'create_chains': 0.2958548069000244}
2025-07-08 07:51:46,472 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.11852741241455078, 'init_vectorstore': 12.584557294845581, 'setup_retriever': 0.0937795639038086, 'load_llm': 6.50093674659729, 'create_chains': 0.20099949836730957}
2025-07-08 07:51:46,476 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.50s
2025-07-08 07:51:46,473 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.56s
2025-07-08 07:51:46,582 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.8889524936676025, 'init_vectorstore': 11.294265985488892, 'setup_retriever': 0.007688283920288086, 'load_llm': 6.893334627151489, 'create_chains': 0.011642217636108398}
2025-07-08 07:51:46,584 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.10s
2025-07-08 07:51:47,073 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:47,171 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:47,172 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.593480110168457, 'init_vectorstore': 12.499932527542114, 'setup_retriever': 0.09333276748657227, 'load_llm': 6.303543567657471, 'create_chains': 0.3885612487792969}
2025-07-08 07:51:47,175 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.88s
2025-07-08 07:51:47,671 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.17516875267028809, 'init_vectorstore': 13.481697797775269, 'setup_retriever': 0.1043705940246582, 'load_llm': 6.708604097366333, 'create_chains': 0.2904520034790039}
2025-07-08 07:51:47,678 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 20.77s
2025-07-08 07:51:47,698 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.2976529598236084, 'init_vectorstore': 14.393780708312988, 'setup_retriever': 0.005288600921630859, 'load_llm': 5.594918727874756, 'create_chains': 0.22454166412353516}
2025-07-08 07:51:47,701 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 20.52s
2025-07-08 07:51:47,877 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:47,879 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:47,882 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:48,930 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2912406921386719, 'init_vectorstore': 16.502275466918945, 'setup_retriever': 0.10747385025024414, 'load_llm': 2.614802360534668, 'create_chains': 0.025864839553833008}
2025-07-08 07:51:48,932 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 20.54s
2025-07-08 07:51:49,133 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.4124479293823242, 'init_vectorstore': 17.59447431564331, 'setup_retriever': 0.18698716163635254, 'load_llm': 1.9415886402130127, 'create_chains': 0.020147085189819336}
2025-07-08 07:51:49,134 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 21.16s
2025-07-08 07:51:49,498 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.114269256591797, 'init_vectorstore': 16.985506534576416, 'setup_retriever': 0.0068166255950927734, 'load_llm': 1.5732462406158447, 'create_chains': 0.045205116271972656}
2025-07-08 07:51:49,500 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 20.73s
2025-07-08 07:51:49,978 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:49,982 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:50,008 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:51:50,784 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:50,788 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:51,276 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.5091805458068848, 'init_vectorstore': 16.89514183998108, 'setup_retriever': 0.003324270248413086, 'load_llm': 1.2233097553253174, 'create_chains': 0.07091212272644043}
2025-07-08 07:51:51,278 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 21.70s
2025-07-08 07:51:51,978 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.5865659713745117, 'init_vectorstore': 15.309314250946045, 'setup_retriever': 0.009200572967529297, 'load_llm': 1.1516356468200684, 'create_chains': 0.03898739814758301}
2025-07-08 07:51:51,980 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.10s
2025-07-08 07:51:52,011 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.901959180831909, 'init_vectorstore': 15.955280542373657, 'setup_retriever': 0.04865455627441406, 'load_llm': 1.1979377269744873, 'create_chains': 0.029079437255859375}
2025-07-08 07:51:52,013 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 21.14s
2025-07-08 07:51:54,289 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:54,499 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:54,590 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:54,982 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:54,989 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:55,190 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:55,572 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:55,670 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:55,675 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:55,682 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:55,683 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:55,877 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:55,970 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:55,975 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:56,069 - root - INFO - Loading Groq LLM...
2025-07-08 07:51:56,172 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:56,171 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:56,183 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.4020612239837646, 'init_vectorstore': 18.294464111328125, 'setup_retriever': 0.011716842651367188, 'load_llm': 1.585242509841919, 'create_chains': 0.30764174461364746}
2025-07-08 07:51:56,186 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 22.61s
2025-07-08 07:51:56,369 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:56,370 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:56,468 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:56,570 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:56,582 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:51:56,875 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.1046063899993896, 'init_vectorstore': 14.600698709487915, 'setup_retriever': 0.1040945053100586, 'load_llm': 1.9882259368896484, 'create_chains': 0.29558420181274414}
2025-07-08 07:51:56,878 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.10s
2025-07-08 07:51:56,881 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.8104348182678223, 'init_vectorstore': 15.102590560913086, 'setup_retriever': 0.006123542785644531, 'load_llm': 2.079648733139038, 'create_chains': 0.3019280433654785}
2025-07-08 07:51:56,884 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 21.30s
2025-07-08 07:51:57,078 - root - INFO - Model pre-warmed successfully
2025-07-08 07:51:57,686 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.913485527038574, 'init_vectorstore': 12.092435836791992, 'setup_retriever': 0.01464080810546875, 'load_llm': 2.6329355239868164, 'create_chains': 0.06340670585632324}
2025-07-08 07:51:57,688 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 17.72s
2025-07-08 07:51:57,997 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.102302074432373, 'init_vectorstore': 14.508844375610352, 'setup_retriever': 0.08170342445373535, 'load_llm': 1.9117350578308105, 'create_chains': 0.015517473220825195}
2025-07-08 07:51:57,998 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.62s
2025-07-08 07:51:58,678 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:51:58,682 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:51:58,682 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds
2025-07-08 07:51:58,689 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds
2025-07-08 07:51:59,986 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:51:59,987 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:51:59,988 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-07-08 07:51:59,989 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-07-08 07:52:00,530 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:52:00,591 - root - INFO - Model pre-warmed successfully
2025-07-08 07:52:01,320 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:01,324 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-07-08 07:52:01,887 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:01,889 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:01,890 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-07-08 07:52:01,892 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-07-08 07:52:01,898 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:01,908 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-07-08 07:52:02,466 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:02,471 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 3.000000 seconds
2025-07-08 07:52:05,887 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:52:05,945 - root - INFO - Model pre-warmed successfully
2025-07-08 07:52:06,253 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:06,254 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:06,253 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:06,253 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:52:06,259 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 12.000000 seconds
2025-07-08 07:52:06,256 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 12.000000 seconds
2025-07-08 07:52:06,257 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 12.000000 seconds
2025-07-08 07:52:06,265 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:52:06,267 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:52:06,311 - root - INFO - Model pre-warmed successfully
2025-07-08 07:52:06,315 - root - INFO - Model pre-warmed successfully
2025-07-08 07:52:06,321 - root - INFO - Model pre-warmed successfully
2025-07-08 07:52:19,259 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:19,259 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:19,259 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-08 07:52:19,275 - root - ERROR - Pre-warming failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jz7v5v8nef893e7jyp6q9mk8` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 6382, Requested 727. Please try again in 11.099s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/app/main.py", line 111, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_groq/chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
  File "/opt/venv/lib/python3.10/site-packages/groq/resources/chat/completions.py", line 368, in create
    return self._post(
  File "/opt/venv/lib/python3.10/site-packages/groq/_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/opt/venv/lib/python3.10/site-packages/groq/_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jz7v5v8nef893e7jyp6q9mk8` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 6382, Requested 727. Please try again in 11.099s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-07-08 07:52:19,269 - root - ERROR - Pre-warming failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jz7v5v8nef893e7jyp6q9mk8` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 6384, Requested 727. Please try again in 11.112s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/app/main.py", line 111, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_groq/chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
  File "/opt/venv/lib/python3.10/site-packages/groq/resources/chat/completions.py", line 368, in create
    return self._post(
  File "/opt/venv/lib/python3.10/site-packages/groq/_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/opt/venv/lib/python3.10/site-packages/groq/_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jz7v5v8nef893e7jyp6q9mk8` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 6384, Requested 727. Please try again in 11.112s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-07-08 07:52:19,272 - root - ERROR - Pre-warming failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jz7v5v8nef893e7jyp6q9mk8` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 6381, Requested 727. Please try again in 11.087s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/app/main.py", line 111, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_groq/chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
  File "/opt/venv/lib/python3.10/site-packages/groq/resources/chat/completions.py", line 368, in create
    return self._post(
  File "/opt/venv/lib/python3.10/site-packages/groq/_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/opt/venv/lib/python3.10/site-packages/groq/_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jz7v5v8nef893e7jyp6q9mk8` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 6381, Requested 727. Please try again in 11.087s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-07-08 07:53:04,558 - root - INFO - Received question: Under what circumstances can the company bypass the issuance of a show cause notice and proceed directly to disciplinary action such as termination or warning? Explain how the severity of misconduct influences this decision.
2025-07-08 07:53:04,560 - root - INFO - Chain init: 0.00s
2025-07-08 07:53:11,046 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:53:11,077 - root - INFO - Question processing: 6.52s
2025-07-08 07:53:11,079 - root - INFO - ⏱️ ask_hr took 6.52 seconds
2025-07-08 07:53:58,175 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 07:53:59,290 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 07:53:59,737 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 07:55:17,298 - root - INFO - Received question: How many keave are allowed to grade M-5 annually
2025-07-08 07:55:17,300 - root - INFO - Chain init: 0.00s
2025-07-08 07:55:22,019 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 07:55:22,027 - root - INFO - Question processing: 4.73s
2025-07-08 07:55:22,055 - root - INFO - ⏱️ ask_hr took 4.76 seconds
2025-07-08 09:05:49,789 - root - INFO - Received question: M5 leave policay
2025-07-08 09:05:49,834 - root - INFO - Chain init: 0.00s
2025-07-08 09:05:59,121 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 09:05:59,221 - root - INFO - Question processing: 9.38s
2025-07-08 09:05:59,223 - root - INFO - ⏱️ ask_hr took 9.44 seconds
2025-07-08 09:06:53,815 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 09:06:54,972 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 09:06:55,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 09:31:15,191 - root - INFO - Received question: m7 leave policy
2025-07-08 09:31:15,241 - root - INFO - Chain init: 0.00s
2025-07-08 09:31:24,030 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 09:31:24,083 - root - INFO - Question processing: 8.84s
2025-07-08 09:31:24,085 - root - INFO - ⏱️ ask_hr took 8.90 seconds
2025-07-08 22:55:53,150 - root - INFO - Starting HR Assistant...
2025-07-08 22:55:53,150 - root - INFO - Configuration validated successfully
2025-07-08 22:55:53,150 - root - INFO - Pre-warming the model...
2025-07-08 22:55:53,150 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 22:55:55,429 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 22:55:57,401 - root - INFO - Loading groq LLM...
2025-07-08 22:55:59,034 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.15255379676818848, 'init_vectorstore': 4.098674297332764, 'setup_retriever': 0.0, 'load_llm': 1.616257667541504, 'create_chains': 0.0160672664642334}
2025-07-08 22:55:59,034 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.88s
2025-07-08 22:56:05,722 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 22:56:05,795 - root - INFO - Model pre-warmed successfully
2025-07-08 22:56:05,796 - root - INFO - HR Assistant started successfully
2025-07-08 22:57:18,953 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 22:57:20,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 22:57:20,568 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 22:57:43,656 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 22:57:43,656 - admin_interface - INFO - Cache invalidated during configuration reload
2025-07-08 22:58:18,272 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 22:58:19,247 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 22:58:19,429 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 22:58:51,891 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 22:58:51,893 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:00:18,423 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-08 23:00:18,427 - root - INFO - Chain init: 0.00s
2025-07-08 23:00:23,612 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:00:23,628 - root - INFO - Question processing: 5.20s
2025-07-08 23:00:23,630 - root - INFO - ⏱️ ask_hr took 5.21 seconds
2025-07-08 23:00:59,595 - root - INFO - Starting HR Assistant...
2025-07-08 23:00:59,595 - root - INFO - Configuration validated successfully
2025-07-08 23:00:59,595 - root - INFO - Pre-warming the model...
2025-07-08 23:00:59,595 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:01:02,074 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:01:04,424 - root - INFO - Loading groq LLM...
2025-07-08 23:01:06,026 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05261564254760742, 'init_vectorstore': 4.774713754653931, 'setup_retriever': 0.0010764598846435547, 'load_llm': 1.599496841430664, 'create_chains': 0.0031538009643554688}
2025-07-08 23:01:06,027 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.43s
2025-07-08 23:01:09,008 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:01:09,032 - root - INFO - Model pre-warmed successfully
2025-07-08 23:01:09,033 - root - INFO - HR Assistant started successfully
2025-07-08 23:01:21,002 - root - INFO - Starting HR Assistant...
2025-07-08 23:01:21,003 - root - INFO - Configuration validated successfully
2025-07-08 23:01:21,003 - root - INFO - Pre-warming the model...
2025-07-08 23:01:21,004 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:01:23,443 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:01:29,676 - root - INFO - Loading groq LLM...
2025-07-08 23:01:31,288 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03194379806518555, 'init_vectorstore': 8.64012622833252, 'setup_retriever': 0.0, 'load_llm': 1.6084258556365967, 'create_chains': 0.0029675960540771484}
2025-07-08 23:01:31,289 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.28s
2025-07-08 23:01:34,291 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:01:34,317 - root - INFO - Model pre-warmed successfully
2025-07-08 23:01:34,318 - root - INFO - HR Assistant started successfully
2025-07-08 23:01:45,561 - root - INFO - Starting HR Assistant...
2025-07-08 23:01:45,562 - root - INFO - Configuration validated successfully
2025-07-08 23:01:45,562 - root - INFO - Pre-warming the model...
2025-07-08 23:01:45,563 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:01:48,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:01:53,188 - root - INFO - Loading groq LLM...
2025-07-08 23:01:54,738 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.033350467681884766, 'init_vectorstore': 7.592101812362671, 'setup_retriever': 0.0, 'load_llm': 1.546154499053955, 'create_chains': 0.003940105438232422}
2025-07-08 23:01:54,739 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.18s
2025-07-08 23:01:58,704 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:01:58,723 - root - INFO - Model pre-warmed successfully
2025-07-08 23:01:58,725 - root - INFO - HR Assistant started successfully
2025-07-08 23:02:11,531 - root - INFO - Starting HR Assistant...
2025-07-08 23:02:11,532 - root - INFO - Configuration validated successfully
2025-07-08 23:02:11,532 - root - INFO - Pre-warming the model...
2025-07-08 23:02:11,533 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:02:14,035 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:02:18,222 - root - INFO - Loading groq LLM...
2025-07-08 23:02:19,819 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03903555870056152, 'init_vectorstore': 6.648851633071899, 'setup_retriever': 0.0, 'load_llm': 1.5878572463989258, 'create_chains': 0.007522106170654297}
2025-07-08 23:02:19,820 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.29s
2025-07-08 23:02:28,032 - root - ERROR - Startup failed: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 126, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 651, in max_marginal_relevance_search
    return self.max_marginal_relevance_search_by_vector(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 683, in max_marginal_relevance_search_by_vector
    results = self.max_marginal_relevance_search_with_score_by_vector(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 716, in max_marginal_relevance_search_with_score_by_vector
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 593, in query_points
    return self._client.query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 607, in query_points
    res: grpc.QueryResponse = self.grpc_points.Query(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 277, in __call__
    response, ignored_call = self._with_call(
                             ^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 332, in _with_call
    return call.result(), call
           ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 440, in result
    raise self
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 315, in continuation
    response, call = self._thunk(new_method).with_call(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1198, in with_call
    return _end_unary_response_blocking(state, call, True, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
2025-07-08 23:03:56,523 - root - INFO - Starting HR Assistant...
2025-07-08 23:03:56,523 - root - INFO - Configuration validated successfully
2025-07-08 23:03:56,523 - root - INFO - Pre-warming the model...
2025-07-08 23:03:56,523 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:04:03,742 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:04:05,556 - root - INFO - Loading groq LLM...
2025-07-08 23:04:07,086 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05074167251586914, 'init_vectorstore': 8.982535362243652, 'setup_retriever': 0.0, 'load_llm': 1.5260097980499268, 'create_chains': 0.004029989242553711}
2025-07-08 23:04:07,086 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.56s
2025-07-08 23:04:12,629 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:04:12,649 - root - INFO - Model pre-warmed successfully
2025-07-08 23:04:12,651 - root - INFO - HR Assistant started successfully
2025-07-08 23:04:38,214 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:04:38,807 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:04:39,057 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:06:06,604 - root - INFO - Starting HR Assistant...
2025-07-08 23:06:06,604 - root - INFO - Configuration validated successfully
2025-07-08 23:06:06,604 - root - INFO - Pre-warming the model...
2025-07-08 23:06:06,604 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:06:09,061 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:06:10,506 - root - INFO - Loading groq LLM...
2025-07-08 23:06:12,348 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03517270088195801, 'init_vectorstore': 3.86665940284729, 'setup_retriever': 0.0, 'load_llm': 1.8424220085144043, 'create_chains': 0.0}
2025-07-08 23:06:12,356 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.75s
2025-07-08 23:06:15,245 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:06:15,290 - root - INFO - Model pre-warmed successfully
2025-07-08 23:06:15,292 - root - INFO - HR Assistant started successfully
2025-07-08 23:06:30,513 - root - INFO - Starting HR Assistant...
2025-07-08 23:06:30,513 - root - INFO - Configuration validated successfully
2025-07-08 23:06:30,513 - root - INFO - Pre-warming the model...
2025-07-08 23:06:30,513 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:06:33,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:06:39,280 - root - INFO - Loading groq LLM...
2025-07-08 23:06:40,924 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05173921585083008, 'init_vectorstore': 8.714775800704956, 'setup_retriever': 0.0009922981262207031, 'load_llm': 1.637186050415039, 'create_chains': 0.007193088531494141}
2025-07-08 23:06:40,927 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.41s
2025-07-08 23:06:43,846 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:06:43,887 - root - INFO - Model pre-warmed successfully
2025-07-08 23:06:43,888 - root - INFO - HR Assistant started successfully
2025-07-08 23:06:57,458 - root - INFO - Starting HR Assistant...
2025-07-08 23:06:57,458 - root - INFO - Configuration validated successfully
2025-07-08 23:06:57,474 - root - INFO - Pre-warming the model...
2025-07-08 23:06:57,474 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:06:59,988 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:07:04,156 - root - INFO - Loading groq LLM...
2025-07-08 23:07:05,813 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04407763481140137, 'init_vectorstore': 6.638005256652832, 'setup_retriever': 0.0, 'load_llm': 1.653149127960205, 'create_chains': 0.004006147384643555}
2025-07-08 23:07:05,814 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.34s
2025-07-08 23:07:08,357 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:07:08,380 - root - INFO - Model pre-warmed successfully
2025-07-08 23:07:08,381 - root - INFO - HR Assistant started successfully
2025-07-08 23:07:22,350 - root - INFO - Starting HR Assistant...
2025-07-08 23:07:22,350 - root - INFO - Configuration validated successfully
2025-07-08 23:07:22,350 - root - INFO - Pre-warming the model...
2025-07-08 23:07:22,350 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:07:26,914 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:07:31,427 - root - INFO - Loading groq LLM...
2025-07-08 23:07:33,103 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0517270565032959, 'init_vectorstore': 9.024373054504395, 'setup_retriever': 0.0010421276092529297, 'load_llm': 1.6723964214324951, 'create_chains': 0.0035965442657470703}
2025-07-08 23:07:33,104 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.75s
2025-07-08 23:07:36,055 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:07:36,100 - root - INFO - Model pre-warmed successfully
2025-07-08 23:07:36,104 - root - INFO - HR Assistant started successfully
2025-07-08 23:07:50,326 - root - INFO - Starting HR Assistant...
2025-07-08 23:07:50,326 - root - INFO - Configuration validated successfully
2025-07-08 23:07:50,327 - root - INFO - Pre-warming the model...
2025-07-08 23:07:50,328 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:07:53,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:07:57,331 - root - INFO - Loading groq LLM...
2025-07-08 23:07:59,108 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05087757110595703, 'init_vectorstore': 6.949179649353027, 'setup_retriever': 0.0010006427764892578, 'load_llm': 1.7705438137054443, 'create_chains': 0.006724357604980469}
2025-07-08 23:07:59,108 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.78s
2025-07-08 23:08:02,529 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:08:02,571 - root - INFO - Model pre-warmed successfully
2025-07-08 23:08:02,571 - root - INFO - HR Assistant started successfully
2025-07-08 23:08:24,020 - root - INFO - Starting HR Assistant...
2025-07-08 23:08:24,022 - root - INFO - Configuration validated successfully
2025-07-08 23:08:24,023 - root - INFO - Pre-warming the model...
2025-07-08 23:08:24,023 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:08:30,251 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:08:33,530 - root - INFO - Loading groq LLM...
2025-07-08 23:08:36,496 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.10404396057128906, 'init_vectorstore': 9.401681900024414, 'setup_retriever': 0.0, 'load_llm': 2.9646730422973633, 'create_chains': 0.0015232563018798828}
2025-07-08 23:08:36,496 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 12.47s
2025-07-08 23:08:39,385 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:08:39,434 - root - INFO - Model pre-warmed successfully
2025-07-08 23:08:39,434 - root - INFO - HR Assistant started successfully
2025-07-08 23:08:54,919 - root - INFO - Starting HR Assistant...
2025-07-08 23:08:54,919 - root - INFO - Configuration validated successfully
2025-07-08 23:08:54,920 - root - INFO - Pre-warming the model...
2025-07-08 23:08:54,920 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:08:57,572 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:09:00,371 - root - INFO - Loading groq LLM...
2025-07-08 23:09:02,302 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04033637046813965, 'init_vectorstore': 5.409879207611084, 'setup_retriever': 0.0, 'load_llm': 1.9306724071502686, 'create_chains': 0.0}
2025-07-08 23:09:02,302 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.38s
2025-07-08 23:09:08,099 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:09:08,126 - root - INFO - Model pre-warmed successfully
2025-07-08 23:09:08,127 - root - INFO - HR Assistant started successfully
2025-07-08 23:09:25,592 - root - INFO - Starting HR Assistant...
2025-07-08 23:09:25,592 - root - INFO - Configuration validated successfully
2025-07-08 23:09:25,592 - root - INFO - Pre-warming the model...
2025-07-08 23:09:25,608 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:09:28,148 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:09:30,948 - root - INFO - Loading groq LLM...
2025-07-08 23:09:32,569 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04144024848937988, 'init_vectorstore': 5.2975335121154785, 'setup_retriever': 0.0, 'load_llm': 1.6170706748962402, 'create_chains': 0.004003286361694336}
2025-07-08 23:09:32,570 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.96s
2025-07-08 23:09:35,670 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:09:35,699 - root - INFO - Model pre-warmed successfully
2025-07-08 23:09:35,700 - root - INFO - HR Assistant started successfully
2025-07-08 23:09:46,536 - root - INFO - Starting HR Assistant...
2025-07-08 23:09:46,536 - root - INFO - Configuration validated successfully
2025-07-08 23:09:46,536 - root - INFO - Pre-warming the model...
2025-07-08 23:09:46,536 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:09:49,123 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:09:51,967 - root - INFO - Loading groq LLM...
2025-07-08 23:09:53,548 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.042548418045043945, 'init_vectorstore': 5.388763904571533, 'setup_retriever': 0.0, 'load_llm': 1.5774421691894531, 'create_chains': 0.003009796142578125}
2025-07-08 23:09:53,548 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.01s
2025-07-08 23:09:56,394 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:09:56,418 - root - INFO - Model pre-warmed successfully
2025-07-08 23:09:56,418 - root - INFO - HR Assistant started successfully
2025-07-08 23:12:44,837 - root - INFO - Starting HR Assistant...
2025-07-08 23:12:44,837 - root - INFO - Configuration validated successfully
2025-07-08 23:12:44,839 - root - INFO - Pre-warming the model...
2025-07-08 23:12:44,839 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:12:47,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:12:50,957 - root - INFO - Loading groq LLM...
2025-07-08 23:12:52,534 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04663991928100586, 'init_vectorstore': 6.071302652359009, 'setup_retriever': 0.0, 'load_llm': 1.5728917121887207, 'create_chains': 0.004019737243652344}
2025-07-08 23:12:52,534 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.69s
2025-07-08 23:12:56,449 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:12:56,468 - root - INFO - Model pre-warmed successfully
2025-07-08 23:12:56,468 - root - INFO - HR Assistant started successfully
2025-07-08 23:18:01,463 - root - INFO - Starting HR Assistant...
2025-07-08 23:18:01,464 - root - INFO - Configuration validated successfully
2025-07-08 23:18:01,464 - root - INFO - Pre-warming the model...
2025-07-08 23:18:01,465 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:18:04,432 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:18:07,337 - root - INFO - Loading groq LLM...
2025-07-08 23:18:08,870 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03554487228393555, 'init_vectorstore': 5.836751937866211, 'setup_retriever': 0.0, 'load_llm': 1.5330255031585693, 'create_chains': 0.0}
2025-07-08 23:18:08,886 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.42s
2025-07-08 23:18:12,133 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:18:12,347 - root - INFO - Model pre-warmed successfully
2025-07-08 23:18:12,347 - root - INFO - HR Assistant started successfully
2025-07-08 23:19:45,308 - root - INFO - Starting HR Assistant...
2025-07-08 23:19:45,308 - root - INFO - Configuration validated successfully
2025-07-08 23:19:45,308 - root - INFO - Pre-warming the model...
2025-07-08 23:19:45,308 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:19:47,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:19:49,900 - root - INFO - Loading groq LLM...
2025-07-08 23:19:51,477 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04890751838684082, 'init_vectorstore': 4.537277460098267, 'setup_retriever': 0.0, 'load_llm': 1.5769374370574951, 'create_chains': 0.0}
2025-07-08 23:19:51,477 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.16s
2025-07-08 23:19:54,499 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:19:54,523 - root - INFO - Model pre-warmed successfully
2025-07-08 23:19:54,523 - root - INFO - HR Assistant started successfully
2025-07-08 23:20:01,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:20:01,957 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:20:02,175 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:20:31,166 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:20:32,030 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:20:32,213 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:21:01,581 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:21:02,332 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:21:02,570 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:21:31,284 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:21:32,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:21:32,753 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:22:01,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:22:02,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:22:02,426 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:22:36,438 - root - INFO - Starting HR Assistant...
2025-07-08 23:22:36,438 - root - INFO - Configuration validated successfully
2025-07-08 23:22:36,438 - root - INFO - Pre-warming the model...
2025-07-08 23:22:36,438 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:22:46,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:22:48,497 - root - INFO - Loading groq LLM...
2025-07-08 23:22:50,046 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04632925987243652, 'init_vectorstore': 12.012628555297852, 'setup_retriever': 0.0, 'load_llm': 1.5474183559417725, 'create_chains': 0.002010822296142578}
2025-07-08 23:22:50,048 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 13.61s
2025-07-08 23:22:53,850 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:22:53,873 - root - INFO - Model pre-warmed successfully
2025-07-08 23:22:53,874 - root - INFO - HR Assistant started successfully
2025-07-08 23:22:56,147 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:22:56,970 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:22:57,209 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:22:59,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:23:00,292 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:23:00,513 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:23:30,164 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:23:31,088 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:23:31,310 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:26:07,804 - root - INFO - Starting HR Assistant...
2025-07-08 23:26:07,804 - root - INFO - Configuration validated successfully
2025-07-08 23:26:07,804 - root - INFO - Pre-warming the model...
2025-07-08 23:26:07,804 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:26:10,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:26:12,103 - root - INFO - Loading groq LLM...
2025-07-08 23:26:13,807 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04037594795227051, 'init_vectorstore': 4.257560968399048, 'setup_retriever': 0.0, 'load_llm': 1.6973206996917725, 'create_chains': 0.0074269771575927734}
2025-07-08 23:26:13,811 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.01s
2025-07-08 23:26:17,100 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:26:17,135 - root - INFO - Model pre-warmed successfully
2025-07-08 23:26:17,136 - root - INFO - HR Assistant started successfully
2025-07-08 23:26:28,506 - root - INFO - Starting HR Assistant...
2025-07-08 23:26:28,506 - root - INFO - Configuration validated successfully
2025-07-08 23:26:28,507 - root - INFO - Pre-warming the model...
2025-07-08 23:26:28,507 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:26:30,878 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:26:32,932 - root - INFO - Loading groq LLM...
2025-07-08 23:26:34,514 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.035996437072753906, 'init_vectorstore': 4.389059066772461, 'setup_retriever': 0.0, 'load_llm': 1.5775563716888428, 'create_chains': 0.004029035568237305}
2025-07-08 23:26:34,514 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.01s
2025-07-08 23:26:37,381 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:26:37,433 - root - INFO - Model pre-warmed successfully
2025-07-08 23:26:37,433 - root - INFO - HR Assistant started successfully
2025-07-08 23:27:09,597 - root - INFO - Starting HR Assistant...
2025-07-08 23:27:09,597 - root - INFO - Configuration validated successfully
2025-07-08 23:27:09,597 - root - INFO - Pre-warming the model...
2025-07-08 23:27:09,597 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:27:12,085 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:27:14,026 - root - INFO - Loading groq LLM...
2025-07-08 23:27:15,628 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03895902633666992, 'init_vectorstore': 4.390222072601318, 'setup_retriever': 0.0, 'load_llm': 1.5984463691711426, 'create_chains': 0.002997875213623047}
2025-07-08 23:27:15,629 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.03s
2025-07-08 23:27:19,339 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:27:19,363 - root - INFO - Model pre-warmed successfully
2025-07-08 23:27:19,364 - root - INFO - HR Assistant started successfully
2025-07-08 23:27:30,213 - root - INFO - Starting HR Assistant...
2025-07-08 23:27:30,213 - root - INFO - Configuration validated successfully
2025-07-08 23:27:30,213 - root - INFO - Pre-warming the model...
2025-07-08 23:27:30,213 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:27:32,564 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:27:35,001 - root - INFO - Loading groq LLM...
2025-07-08 23:27:36,577 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04516458511352539, 'init_vectorstore': 4.7429139614105225, 'setup_retriever': 0.0, 'load_llm': 1.5728366374969482, 'create_chains': 0.003521442413330078}
2025-07-08 23:27:36,577 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.36s
2025-07-08 23:27:39,633 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:27:39,660 - root - INFO - Model pre-warmed successfully
2025-07-08 23:27:39,661 - root - INFO - HR Assistant started successfully
2025-07-08 23:28:18,871 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:28:19,860 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:28:20,039 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:31:59,711 - root - INFO - Starting HR Assistant...
2025-07-08 23:31:59,712 - root - INFO - Configuration validated successfully
2025-07-08 23:31:59,713 - root - INFO - Pre-warming the model...
2025-07-08 23:31:59,714 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:32:02,325 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:32:05,371 - root - INFO - Loading groq LLM...
2025-07-08 23:32:06,911 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05769681930541992, 'init_vectorstore': 5.599603176116943, 'setup_retriever': 0.0, 'load_llm': 1.5376334190368652, 'create_chains': 0.002015829086303711}
2025-07-08 23:32:06,911 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.20s
2025-07-08 23:32:10,315 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:32:10,341 - root - INFO - Model pre-warmed successfully
2025-07-08 23:32:10,342 - root - INFO - HR Assistant started successfully
2025-07-08 23:32:21,144 - root - INFO - Starting HR Assistant...
2025-07-08 23:32:21,144 - root - INFO - Configuration validated successfully
2025-07-08 23:32:21,144 - root - INFO - Pre-warming the model...
2025-07-08 23:32:21,144 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:32:23,869 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:32:27,509 - root - INFO - Loading groq LLM...
2025-07-08 23:32:29,056 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04873013496398926, 'init_vectorstore': 6.316014289855957, 'setup_retriever': 0.0, 'load_llm': 1.5468902587890625, 'create_chains': 0.0}
2025-07-08 23:32:29,056 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.91s
2025-07-08 23:32:32,304 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:32:32,340 - root - INFO - Model pre-warmed successfully
2025-07-08 23:32:32,341 - root - INFO - HR Assistant started successfully
2025-07-08 23:32:40,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:32:40,639 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:32:40,810 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:34:04,070 - root - INFO - Starting HR Assistant...
2025-07-08 23:34:04,070 - root - INFO - Configuration validated successfully
2025-07-08 23:34:04,071 - root - INFO - Pre-warming the model...
2025-07-08 23:34:04,071 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:34:06,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:34:09,442 - root - INFO - Loading groq LLM...
2025-07-08 23:34:11,323 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03920292854309082, 'init_vectorstore': 5.33176064491272, 'setup_retriever': 0.0, 'load_llm': 1.878584384918213, 'create_chains': 0.002019166946411133}
2025-07-08 23:34:11,323 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.25s
2025-07-08 23:34:15,151 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:34:15,192 - root - INFO - Model pre-warmed successfully
2025-07-08 23:34:15,194 - root - INFO - HR Assistant started successfully
2025-07-08 23:34:28,359 - root - INFO - Starting HR Assistant...
2025-07-08 23:34:28,360 - root - INFO - Configuration validated successfully
2025-07-08 23:34:28,361 - root - INFO - Pre-warming the model...
2025-07-08 23:34:28,361 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:34:30,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:34:32,255 - root - INFO - Loading groq LLM...
2025-07-08 23:34:33,862 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03661918640136719, 'init_vectorstore': 3.8566629886627197, 'setup_retriever': 0.0, 'load_llm': 1.5910708904266357, 'create_chains': 0.015857458114624023}
2025-07-08 23:34:33,862 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.50s
2025-07-08 23:34:37,695 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:34:37,713 - root - INFO - Model pre-warmed successfully
2025-07-08 23:34:37,713 - root - INFO - HR Assistant started successfully
2025-07-08 23:34:48,865 - root - INFO - Starting HR Assistant...
2025-07-08 23:34:48,865 - root - INFO - Configuration validated successfully
2025-07-08 23:34:48,865 - root - INFO - Pre-warming the model...
2025-07-08 23:34:48,865 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:34:51,256 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:34:53,564 - root - INFO - Loading groq LLM...
2025-07-08 23:34:55,115 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04059648513793945, 'init_vectorstore': 4.658077955245972, 'setup_retriever': 0.0, 'load_llm': 1.5515828132629395, 'create_chains': 0.0}
2025-07-08 23:34:55,115 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.25s
2025-07-08 23:34:58,201 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:34:58,224 - root - INFO - Model pre-warmed successfully
2025-07-08 23:34:58,224 - root - INFO - HR Assistant started successfully
2025-07-08 23:35:10,375 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:35:10,945 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:35:11,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:36:39,478 - root - INFO - Starting HR Assistant...
2025-07-08 23:36:39,479 - root - INFO - Configuration validated successfully
2025-07-08 23:36:39,480 - root - INFO - Pre-warming the model...
2025-07-08 23:36:39,481 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:36:42,931 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:36:45,395 - root - INFO - Loading groq LLM...
2025-07-08 23:36:47,056 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04477262496948242, 'init_vectorstore': 5.868363857269287, 'setup_retriever': 0.0, 'load_llm': 1.6589255332946777, 'create_chains': 0.0019958019256591797}
2025-07-08 23:36:47,057 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.58s
2025-07-08 23:36:53,165 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:36:53,201 - root - INFO - Model pre-warmed successfully
2025-07-08 23:36:53,212 - root - INFO - HR Assistant started successfully
2025-07-08 23:37:06,984 - root - INFO - Starting HR Assistant...
2025-07-08 23:37:06,984 - root - INFO - Configuration validated successfully
2025-07-08 23:37:06,985 - root - INFO - Pre-warming the model...
2025-07-08 23:37:06,985 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:37:09,245 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:37:11,186 - root - INFO - Loading groq LLM...
2025-07-08 23:37:13,109 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.046433210372924805, 'init_vectorstore': 4.153723239898682, 'setup_retriever': 0.0, 'load_llm': 1.9198801517486572, 'create_chains': 0.003004312515258789}
2025-07-08 23:37:13,110 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.12s
2025-07-08 23:37:16,394 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:37:16,414 - root - INFO - Model pre-warmed successfully
2025-07-08 23:37:16,415 - root - INFO - HR Assistant started successfully
2025-07-08 23:37:30,438 - root - INFO - Starting HR Assistant...
2025-07-08 23:37:30,441 - root - INFO - Configuration validated successfully
2025-07-08 23:37:30,442 - root - INFO - Pre-warming the model...
2025-07-08 23:37:30,442 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:37:32,971 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:37:35,320 - root - INFO - Loading groq LLM...
2025-07-08 23:37:36,952 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.044991493225097656, 'init_vectorstore': 4.832351207733154, 'setup_retriever': 0.0, 'load_llm': 1.628767967224121, 'create_chains': 0.0030028820037841797}
2025-07-08 23:37:36,953 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.51s
2025-07-08 23:37:39,764 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:37:39,779 - root - INFO - Model pre-warmed successfully
2025-07-08 23:37:39,779 - root - INFO - HR Assistant started successfully
2025-07-08 23:37:52,155 - root - INFO - Starting HR Assistant...
2025-07-08 23:37:52,155 - root - INFO - Configuration validated successfully
2025-07-08 23:37:52,157 - root - INFO - Pre-warming the model...
2025-07-08 23:37:52,157 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:37:55,949 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:37:58,265 - root - INFO - Loading groq LLM...
2025-07-08 23:37:59,922 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.034592390060424805, 'init_vectorstore': 6.072880506515503, 'setup_retriever': 0.0, 'load_llm': 1.6541836261749268, 'create_chains': 0.002812623977661133}
2025-07-08 23:37:59,922 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.76s
2025-07-08 23:38:02,573 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:38:02,609 - root - INFO - Model pre-warmed successfully
2025-07-08 23:38:02,609 - root - INFO - HR Assistant started successfully
2025-07-08 23:38:14,426 - root - INFO - Starting HR Assistant...
2025-07-08 23:38:14,426 - root - INFO - Configuration validated successfully
2025-07-08 23:38:14,427 - root - INFO - Pre-warming the model...
2025-07-08 23:38:14,427 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:38:16,484 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:38:18,856 - root - INFO - Loading groq LLM...
2025-07-08 23:38:20,435 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03499174118041992, 'init_vectorstore': 4.393124341964722, 'setup_retriever': 0.0, 'load_llm': 1.5709643363952637, 'create_chains': 0.00819087028503418}
2025-07-08 23:38:20,435 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.01s
2025-07-08 23:38:23,629 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:38:23,656 - root - INFO - Model pre-warmed successfully
2025-07-08 23:38:23,656 - root - INFO - HR Assistant started successfully
2025-07-08 23:39:19,972 - root - INFO - Starting HR Assistant...
2025-07-08 23:39:19,972 - root - INFO - Configuration validated successfully
2025-07-08 23:39:19,972 - root - INFO - Pre-warming the model...
2025-07-08 23:39:19,972 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:39:22,368 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:39:25,162 - root - INFO - Loading groq LLM...
2025-07-08 23:39:26,857 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04212331771850586, 'init_vectorstore': 5.147668838500977, 'setup_retriever': 0.0, 'load_llm': 1.692119836807251, 'create_chains': 0.0029959678649902344}
2025-07-08 23:39:26,858 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.89s
2025-07-08 23:39:29,977 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:39:29,999 - root - INFO - Model pre-warmed successfully
2025-07-08 23:39:29,999 - root - INFO - HR Assistant started successfully
2025-07-08 23:39:42,836 - root - INFO - Starting HR Assistant...
2025-07-08 23:39:42,838 - root - INFO - Configuration validated successfully
2025-07-08 23:39:42,838 - root - INFO - Pre-warming the model...
2025-07-08 23:39:42,839 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:39:45,049 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:39:47,940 - root - INFO - Loading groq LLM...
2025-07-08 23:39:49,681 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03556227684020996, 'init_vectorstore': 5.06524658203125, 'setup_retriever': 0.0, 'load_llm': 1.732600450515747, 'create_chains': 0.008023262023925781}
2025-07-08 23:39:49,682 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.84s
2025-07-08 23:39:52,698 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:39:52,721 - root - INFO - Model pre-warmed successfully
2025-07-08 23:39:52,722 - root - INFO - HR Assistant started successfully
2025-07-08 23:40:27,989 - root - INFO - Starting HR Assistant...
2025-07-08 23:40:27,990 - root - INFO - Configuration validated successfully
2025-07-08 23:40:27,990 - root - INFO - Pre-warming the model...
2025-07-08 23:40:27,990 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:40:30,483 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:40:32,441 - root - INFO - Loading groq LLM...
2025-07-08 23:40:34,264 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03801465034484863, 'init_vectorstore': 4.411316871643066, 'setup_retriever': 0.0, 'load_llm': 1.8207638263702393, 'create_chains': 0.002001047134399414}
2025-07-08 23:40:34,265 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.27s
2025-07-08 23:40:37,936 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:40:37,967 - root - INFO - Model pre-warmed successfully
2025-07-08 23:40:37,967 - root - INFO - HR Assistant started successfully
2025-07-08 23:41:16,208 - root - INFO - Starting HR Assistant...
2025-07-08 23:41:16,209 - root - INFO - Configuration validated successfully
2025-07-08 23:41:16,210 - root - INFO - Pre-warming the model...
2025-07-08 23:41:16,210 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:41:18,500 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:41:20,556 - root - INFO - Loading groq LLM...
2025-07-08 23:41:22,287 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.034325599670410156, 'init_vectorstore': 4.311697244644165, 'setup_retriever': 0.0, 'load_llm': 1.7272393703460693, 'create_chains': 0.004010915756225586}
2025-07-08 23:41:22,288 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.08s
2025-07-08 23:41:25,698 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:41:25,722 - root - INFO - Model pre-warmed successfully
2025-07-08 23:41:25,722 - root - INFO - HR Assistant started successfully
2025-07-08 23:41:54,355 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:41:55,673 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:41:56,468 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:42:14,285 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:42:14,290 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:42:14,290 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:42:31,073 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-08 23:42:31,074 - root - INFO - Chain init: 0.00s
2025-07-08 23:42:35,453 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:42:35,453 - root - INFO - Question processing: 4.38s
2025-07-08 23:42:35,453 - root - INFO - ⏱️ ask_hr took 4.38 seconds
2025-07-08 23:45:22,895 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:45:22,895 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:45:50,596 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:45:50,598 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:45:50,598 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:46:19,306 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:46:19,307 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:46:30,148 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-08 23:46:30,149 - root - INFO - Chain init: 0.00s
2025-07-08 23:46:35,612 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:46:35,624 - root - INFO - Question processing: 5.47s
2025-07-08 23:46:35,624 - root - INFO - ⏱️ ask_hr took 5.48 seconds
2025-07-08 23:49:12,732 - root - INFO - Starting HR Assistant...
2025-07-08 23:49:12,732 - root - INFO - Configuration validated successfully
2025-07-08 23:49:12,732 - root - INFO - Pre-warming the model...
2025-07-08 23:49:12,732 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:49:15,983 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:49:19,855 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:49:19,856 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:49:19,856 - root - INFO - Loading groq LLM...
2025-07-08 23:49:21,562 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04017376899719238, 'init_vectorstore': 7.077570199966431, 'setup_retriever': 0.0, 'load_llm': 1.7029190063476562, 'create_chains': 0.0037665367126464844}
2025-07-08 23:49:21,562 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.82s
2025-07-08 23:49:25,959 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:49:25,982 - root - INFO - Model pre-warmed successfully
2025-07-08 23:49:25,985 - root - INFO - HR Assistant started successfully
2025-07-08 23:49:55,205 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:49:56,269 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:49:56,698 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:49:56,705 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:49:56,705 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:49:56,705 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:49:56,705 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:50:21,510 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:50:21,512 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:50:34,369 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:50:34,370 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:50:39,174 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:50:39,176 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:50:39,178 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:50:39,187 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:50:39,187 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:50:39,198 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:50:39,199 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:50:39,199 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:50:39,199 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:52:54,273 - root - INFO - Starting HR Assistant...
2025-07-08 23:52:54,273 - root - INFO - Configuration validated successfully
2025-07-08 23:52:54,273 - root - INFO - Pre-warming the model...
2025-07-08 23:52:54,273 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:52:57,523 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:53:00,454 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:53:00,455 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:53:00,455 - root - INFO - Loading google LLM...
2025-07-08 23:53:00,469 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.056722402572631836, 'init_vectorstore': 6.123739719390869, 'setup_retriever': 0.0004177093505859375, 'load_llm': 0.01063847541809082, 'create_chains': 0.004015684127807617}
2025-07-08 23:53:00,470 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.20s
2025-07-08 23:53:05,597 - root - INFO - Model pre-warmed successfully
2025-07-08 23:53:05,597 - root - INFO - HR Assistant started successfully
2025-07-08 23:53:11,909 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:53:12,611 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:53:12,808 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:53:12,808 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:53:12,822 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:53:12,822 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:53:12,823 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:53:18,012 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:53:18,012 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:53:27,911 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-08 23:53:27,911 - root - INFO - Chain init: 0.00s
2025-07-08 23:53:32,491 - root - INFO - Question processing: 4.58s
2025-07-08 23:53:32,504 - root - INFO - ⏱️ ask_hr took 4.59 seconds
2025-07-08 23:53:41,987 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:53:42,650 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:53:42,826 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:53:42,826 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:53:42,826 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:53:42,841 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:53:42,841 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:53:56,138 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:53:56,138 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:54:02,034 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:54:02,034 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:54:02,036 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:54:02,046 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:54:02,047 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:54:02,055 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:54:02,056 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:54:02,056 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:54:02,056 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:54:25,181 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:54:25,181 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:54:28,821 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:54:28,822 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:56:21,840 - root - INFO - Starting HR Assistant...
2025-07-08 23:56:21,840 - root - INFO - Configuration validated successfully
2025-07-08 23:56:21,840 - root - INFO - Pre-warming the model...
2025-07-08 23:56:21,840 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:56:24,351 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:56:26,310 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:56:26,310 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:56:26,310 - root - INFO - Loading groq LLM...
2025-07-08 23:56:27,993 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05646824836730957, 'init_vectorstore': 4.4134650230407715, 'setup_retriever': 0.0, 'load_llm': 1.6796095371246338, 'create_chains': 0.0040204524993896484}
2025-07-08 23:56:27,993 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.15s
2025-07-08 23:56:33,910 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:56:33,932 - root - INFO - Model pre-warmed successfully
2025-07-08 23:56:33,932 - root - INFO - HR Assistant started successfully
2025-07-08 23:56:40,378 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:56:40,972 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:56:41,161 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:56:41,173 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:56:41,177 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:56:41,177 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:56:41,178 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:56:46,236 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:56:46,236 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:56:53,290 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:56:53,292 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:56:53,302 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:56:53,308 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:56:53,308 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:56:53,318 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:56:53,318 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:56:53,318 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:56:53,318 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:57:02,370 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:57:03,530 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:57:04,350 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:57:04,358 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:57:04,359 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:57:04,360 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:57:04,360 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:57:04,364 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:57:04,364 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:57:09,874 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:57:09,874 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:57:12,783 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:57:12,783 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:57:18,791 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-08 23:57:18,792 - root - INFO - Chain init: 0.00s
2025-07-08 23:57:22,972 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:57:22,972 - root - INFO - Question processing: 4.18s
2025-07-08 23:57:22,972 - root - INFO - ⏱️ ask_hr took 4.18 seconds
2025-07-08 23:57:56,299 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:57:56,305 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:58:00,278 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:58:00,278 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:58:06,934 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-08 23:58:08,087 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-08 23:58:08,745 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-08 23:58:08,750 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:58:08,750 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:58:08,750 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:58:08,750 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:58:12,203 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:58:12,204 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:58:17,769 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-08 23:58:17,771 - admin_interface - INFO - Successfully updated .env file
2025-07-08 23:58:17,779 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-08 23:58:17,787 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:58:17,787 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:58:17,800 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:58:17,801 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:58:17,801 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-08 23:58:17,801 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-08 23:58:28,681 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-08 23:58:28,681 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-08 23:58:39,291 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-08 23:58:39,292 - root - INFO - Chain init: 0.00s
2025-07-08 23:58:43,323 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-08 23:58:43,328 - root - INFO - Question processing: 4.04s
2025-07-08 23:58:43,328 - root - INFO - ⏱️ ask_hr took 4.04 seconds
2025-07-09 00:01:23,678 - root - INFO - Starting HR Assistant...
2025-07-09 00:01:23,678 - root - INFO - Configuration validated successfully
2025-07-09 00:01:23,678 - root - INFO - Pre-warming the model...
2025-07-09 00:01:23,678 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:01:23,678 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:01:25,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:01:29,138 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:01:29,138 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:01:29,138 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:01:29,138 - root - INFO - Loading google LLM...
2025-07-09 00:01:29,157 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04189181327819824, 'init_vectorstore': 5.417781591415405, 'setup_retriever': 0.0, 'load_llm': 0.015726089477539062, 'create_chains': 0.003275156021118164}
2025-07-09 00:01:29,159 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.48s
2025-07-09 00:01:34,211 - root - INFO - Model pre-warmed successfully
2025-07-09 00:01:34,211 - root - INFO - HR Assistant started successfully
2025-07-09 00:01:41,779 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:01:43,533 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:01:43,946 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:01:43,956 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:01:43,956 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:01:43,956 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:01:43,958 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:02:01,112 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 00:02:01,113 - root - INFO - Chain init: 0.00s
2025-07-09 00:02:05,615 - root - INFO - Question processing: 4.50s
2025-07-09 00:02:05,616 - root - INFO - ⏱️ ask_hr took 4.50 seconds
2025-07-09 00:02:12,761 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:02:13,987 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:02:14,180 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:02:14,198 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:02:14,198 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:02:14,201 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:02:14,201 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:02:23,417 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:02:23,417 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:02:29,513 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:02:29,517 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:02:29,523 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:02:29,525 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:02:29,532 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:02:29,532 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:02:29,544 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:02:29,544 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:02:29,544 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:02:29,546 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:02:44,386 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:02:44,386 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:02:54,740 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 00:02:54,741 - root - INFO - Chain init: 0.00s
2025-07-09 00:02:59,222 - root - INFO - Question processing: 4.48s
2025-07-09 00:02:59,222 - root - INFO - ⏱️ ask_hr took 4.48 seconds
2025-07-09 00:05:38,122 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:05:38,124 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:05:38,124 - admin_interface - INFO - Cache invalidated during configuration reload
2025-07-09 00:05:56,655 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:05:56,656 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:06:01,367 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 00:06:01,367 - root - INFO - Chain init: 0.00s
2025-07-09 00:06:06,368 - root - INFO - Question processing: 5.00s
2025-07-09 00:06:06,368 - root - INFO - ⏱️ ask_hr took 5.00 seconds
2025-07-09 00:08:12,278 - root - INFO - Starting HR Assistant...
2025-07-09 00:08:12,278 - root - INFO - Configuration validated successfully
2025-07-09 00:08:12,278 - root - INFO - Pre-warming the model...
2025-07-09 00:08:12,278 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:08:12,278 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:08:15,636 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:08:19,703 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:08:19,705 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:08:19,705 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:08:19,705 - root - INFO - Loading groq LLM...
2025-07-09 00:08:21,408 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.036386966705322266, 'init_vectorstore': 7.388574123382568, 'setup_retriever': 0.0, 'load_llm': 1.6888468265533447, 'create_chains': 0.016124963760375977}
2025-07-09 00:08:21,408 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.13s
2025-07-09 00:08:24,507 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:08:24,529 - root - INFO - Model pre-warmed successfully
2025-07-09 00:08:24,532 - root - INFO - HR Assistant started successfully
2025-07-09 00:08:36,730 - root - INFO - Starting HR Assistant...
2025-07-09 00:08:36,730 - root - INFO - Configuration validated successfully
2025-07-09 00:08:36,730 - root - INFO - Pre-warming the model...
2025-07-09 00:08:36,733 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:08:36,733 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:08:39,309 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:08:42,986 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:08:42,987 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:08:42,988 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:08:42,988 - root - INFO - Loading groq LLM...
2025-07-09 00:08:44,652 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.044889211654663086, 'init_vectorstore': 6.2071144580841064, 'setup_retriever': 0.0, 'load_llm': 1.6653199195861816, 'create_chains': 0.0}
2025-07-09 00:08:44,652 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.92s
2025-07-09 00:08:50,959 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:08:50,981 - root - INFO - Model pre-warmed successfully
2025-07-09 00:08:50,981 - root - INFO - HR Assistant started successfully
2025-07-09 00:09:00,736 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:09:02,019 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:09:02,285 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:09:02,285 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:09:02,298 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:09:02,298 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:09:02,300 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:09:18,777 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:09:18,777 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:09:23,381 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:09:23,382 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:09:23,392 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:09:23,392 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:09:23,399 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:09:23,400 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:09:23,407 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:09:23,409 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:09:23,409 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:09:23,410 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:09:40,967 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:09:40,967 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:13:55,074 - root - INFO - Starting HR Assistant...
2025-07-09 00:13:55,074 - root - INFO - Configuration validated successfully
2025-07-09 00:13:55,074 - root - INFO - Pre-warming the model...
2025-07-09 00:13:55,074 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:13:55,074 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:13:57,171 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:13:58,628 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:13:58,628 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:13:58,628 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:13:58,628 - root - INFO - Loading google LLM...
2025-07-09 00:13:58,644 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.016068220138549805, 'init_vectorstore': 3.538450002670288, 'setup_retriever': 0.0, 'load_llm': 0.0, 'create_chains': 0.015799283981323242}
2025-07-09 00:13:58,644 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 3.57s
2025-07-09 00:14:04,304 - root - INFO - Model pre-warmed successfully
2025-07-09 00:14:04,304 - root - INFO - HR Assistant started successfully
2025-07-09 00:14:19,288 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:14:20,135 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:14:20,360 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:14:20,366 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:14:20,367 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:14:20,368 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:14:20,368 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:14:24,282 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:14:24,283 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:14:28,802 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:14:28,806 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:14:28,814 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:14:28,814 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:14:28,829 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:14:28,829 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:14:28,851 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:14:28,851 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:14:28,851 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:14:28,851 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:14:39,263 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:14:39,264 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:14:49,018 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:14:49,682 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:14:49,936 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:14:49,936 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:14:49,951 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:14:49,951 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:14:49,951 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:15:19,537 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:15:20,116 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:15:20,309 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:15:20,317 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:15:20,318 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:15:20,318 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:15:20,319 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:15:49,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:15:50,032 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:15:50,210 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:15:50,217 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:15:50,217 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:15:50,217 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:15:50,217 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:16:12,659 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:16:12,660 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:16:25,668 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 00:16:25,668 - root - INFO - Chain init: 0.00s
2025-07-09 00:16:30,898 - root - INFO - Question processing: 5.23s
2025-07-09 00:16:30,898 - root - INFO - ⏱️ ask_hr took 5.23 seconds
2025-07-09 00:17:16,148 - root - INFO - Starting HR Assistant...
2025-07-09 00:17:16,148 - root - INFO - Configuration validated successfully
2025-07-09 00:17:16,148 - root - INFO - Pre-warming the model...
2025-07-09 00:17:16,148 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:17:16,148 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:17:19,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:17:21,401 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:17:21,404 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:17:21,404 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:17:21,405 - root - INFO - Loading groq LLM...
2025-07-09 00:17:23,075 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0412144660949707, 'init_vectorstore': 5.209949493408203, 'setup_retriever': 0.00202178955078125, 'load_llm': 1.6715247631072998, 'create_chains': 0.002009153366088867}
2025-07-09 00:17:23,077 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.93s
2025-07-09 00:17:26,628 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:17:26,651 - root - INFO - Model pre-warmed successfully
2025-07-09 00:17:26,652 - root - INFO - HR Assistant started successfully
2025-07-09 00:17:41,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:17:42,256 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:17:42,493 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:17:42,509 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:17:42,509 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:17:42,517 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:17:42,518 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:17:52,656 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:17:52,656 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:17:56,661 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:17:56,664 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:17:56,672 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:17:56,672 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:17:56,682 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:17:56,684 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:17:56,700 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:17:56,700 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:17:56,705 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:17:56,705 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:18:08,521 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:18:09,082 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:18:09,262 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:18:09,269 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:18:09,270 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:18:09,271 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:18:09,271 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:18:14,091 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:18:14,091 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:18:39,424 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:18:40,762 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:18:40,975 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:18:40,980 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:18:40,980 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:18:40,981 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:18:40,981 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:19:08,490 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:19:09,936 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:19:10,441 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:19:10,446 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:19:10,447 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:19:10,447 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:19:10,448 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:19:43,424 - root - INFO - Starting HR Assistant...
2025-07-09 00:19:43,424 - root - INFO - Configuration validated successfully
2025-07-09 00:19:43,424 - root - INFO - Pre-warming the model...
2025-07-09 00:19:43,424 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:19:43,424 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:19:47,860 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:19:52,132 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:19:52,133 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:19:52,133 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:19:52,134 - root - INFO - Loading google LLM...
2025-07-09 00:19:52,146 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.046022653579711914, 'init_vectorstore': 8.662877082824707, 'setup_retriever': 0.0, 'load_llm': 0.010130167007446289, 'create_chains': 0.0028693675994873047}
2025-07-09 00:19:52,146 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.72s
2025-07-09 00:19:57,727 - root - INFO - Model pre-warmed successfully
2025-07-09 00:19:57,727 - root - INFO - HR Assistant started successfully
2025-07-09 00:19:57,742 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:19:57,743 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:19:57,743 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:19:57,744 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:20:00,236 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:20:01,427 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:20:01,658 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:20:15,177 - root - INFO - Starting HR Assistant...
2025-07-09 00:20:15,178 - root - INFO - Configuration validated successfully
2025-07-09 00:20:15,179 - root - INFO - Pre-warming the model...
2025-07-09 00:20:15,179 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:20:15,180 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:20:18,026 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:20:23,547 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:20:23,548 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:20:23,549 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:20:23,550 - root - INFO - Loading google LLM...
2025-07-09 00:20:23,570 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.038671255111694336, 'init_vectorstore': 8.326555490493774, 'setup_retriever': 0.0, 'load_llm': 0.020003318786621094, 'create_chains': 0.003638744354248047}
2025-07-09 00:20:23,571 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.39s
2025-07-09 00:20:29,590 - root - INFO - Model pre-warmed successfully
2025-07-09 00:20:29,590 - root - INFO - HR Assistant started successfully
2025-07-09 00:20:41,955 - root - INFO - Starting HR Assistant...
2025-07-09 00:20:41,957 - root - INFO - Configuration validated successfully
2025-07-09 00:20:41,957 - root - INFO - Pre-warming the model...
2025-07-09 00:20:41,957 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:20:41,957 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:20:44,582 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:20:49,837 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:20:49,837 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:20:49,843 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:20:49,844 - root - INFO - Loading google LLM...
2025-07-09 00:20:49,862 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.029424190521240234, 'init_vectorstore': 7.848812103271484, 'setup_retriever': 0.0, 'load_llm': 0.013774394989013672, 'create_chains': 0.011365890502929688}
2025-07-09 00:20:49,863 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.91s
2025-07-09 00:20:54,153 - root - INFO - Model pre-warmed successfully
2025-07-09 00:20:54,154 - root - INFO - HR Assistant started successfully
2025-07-09 00:20:54,160 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:20:54,160 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:20:54,162 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:20:54,162 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:20:58,052 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:20:59,610 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:21:00,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:21:09,311 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:21:11,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:21:11,890 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:21:11,894 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:21:11,895 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:21:11,895 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:21:11,896 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:21:38,672 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:21:39,809 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:21:40,310 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:21:40,321 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:21:40,321 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:21:40,321 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:21:40,327 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:22:03,613 - root - INFO - Starting HR Assistant...
2025-07-09 00:22:03,614 - root - INFO - Configuration validated successfully
2025-07-09 00:22:03,615 - root - INFO - Pre-warming the model...
2025-07-09 00:22:03,615 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:22:03,616 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:22:06,668 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:22:11,115 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:22:11,116 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:22:11,117 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:22:11,117 - root - INFO - Loading google LLM...
2025-07-09 00:22:11,131 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.040152788162231445, 'init_vectorstore': 7.45895791053772, 'setup_retriever': 0.0, 'load_llm': 0.013278007507324219, 'create_chains': 0.0030107498168945312}
2025-07-09 00:22:11,132 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.52s
2025-07-09 00:22:16,488 - root - INFO - Model pre-warmed successfully
2025-07-09 00:22:16,488 - root - INFO - HR Assistant started successfully
2025-07-09 00:22:16,496 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:22:16,497 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:22:16,497 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:22:16,498 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:22:19,708 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:22:21,400 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:22:21,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:22:39,414 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:22:40,895 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:22:41,396 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:22:41,397 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:22:41,397 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:22:41,397 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:22:41,397 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:23:09,793 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:23:11,849 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:23:12,263 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:23:12,273 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:23:12,274 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:23:12,275 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:23:12,275 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:23:39,376 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:23:40,610 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:23:40,880 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:23:40,896 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:23:40,896 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:23:40,896 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:23:40,896 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:24:10,196 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:24:11,842 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:24:12,415 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:24:12,415 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:24:12,415 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:24:12,415 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:24:12,415 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:24:36,326 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:24:36,326 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:24:36,327 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:24:36,327 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:24:38,879 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:24:39,794 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:24:40,055 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:25:09,488 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:25:12,230 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:25:12,484 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:25:12,486 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:25:12,486 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:25:12,486 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:25:12,486 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:25:39,390 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:25:41,358 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:25:41,686 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:25:41,704 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:25:41,704 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:25:41,704 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:25:41,704 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:26:08,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:26:08,857 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:26:09,038 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:26:09,050 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:26:09,051 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:26:09,051 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:26:09,052 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:26:39,476 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:26:41,060 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:26:41,648 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:26:41,648 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:26:41,648 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:26:41,648 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:26:41,648 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:27:09,200 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:27:09,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:27:10,286 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:27:10,293 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:27:10,294 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:27:10,294 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:27:10,295 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:27:32,361 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:27:32,361 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:27:37,192 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:27:37,194 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:27:37,200 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:27:37,202 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:27:37,209 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:27:37,209 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:27:37,217 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:27:37,217 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:27:37,217 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:27:37,217 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:27:53,189 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:27:54,504 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:27:54,747 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:27:54,747 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:27:54,747 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:27:54,758 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:27:54,758 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:28:15,157 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:28:15,157 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:28:28,358 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:28:28,358 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:28:31,904 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:28:31,904 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:28:37,412 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:28:37,412 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:28:52,341 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:28:52,341 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:29:05,646 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:29:05,647 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:29:24,100 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:29:25,420 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:29:25,670 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:29:25,675 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:29:25,675 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:29:25,675 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:29:25,675 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:29:53,507 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:29:54,892 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:29:55,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:29:55,320 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:29:55,320 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:29:55,320 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:29:55,320 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:30:24,597 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:30:26,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:30:26,752 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:30:26,760 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:30:26,760 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:30:26,761 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:30:26,761 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:30:53,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:30:54,299 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:30:54,498 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:30:54,498 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:30:54,498 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:30:54,498 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:30:54,508 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:31:15,859 - root - INFO - Starting HR Assistant...
2025-07-09 00:31:15,991 - root - INFO - Configuration validated successfully
2025-07-09 00:31:15,991 - root - INFO - Pre-warming the model...
2025-07-09 00:31:16,007 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:31:16,007 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:31:19,009 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:31:22,131 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:31:22,268 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:31:22,271 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:31:22,271 - root - INFO - Loading groq LLM...
2025-07-09 00:31:24,021 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0285947322845459, 'init_vectorstore': 5.956143379211426, 'setup_retriever': 0.0, 'load_llm': 1.8854632377624512, 'create_chains': 0.003999948501586914}
2025-07-09 00:31:24,022 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.01s
2025-07-09 00:31:26,462 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:31:26,496 - root - INFO - Model pre-warmed successfully
2025-07-09 00:31:26,497 - root - INFO - HR Assistant started successfully
2025-07-09 00:31:26,505 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:31:26,505 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:31:26,509 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:31:26,509 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:31:29,750 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:31:31,670 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:31:32,169 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:31:45,781 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 00:31:45,783 - root - INFO - Chain init: 0.00s
2025-07-09 00:31:50,369 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:31:50,369 - root - INFO - Question processing: 4.59s
2025-07-09 00:31:50,369 - root - INFO - ⏱️ ask_hr took 4.59 seconds
2025-07-09 00:31:53,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:31:55,041 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:31:55,538 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:31:55,552 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:31:55,552 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:31:55,552 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:31:55,554 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:31:59,220 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:31:59,220 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:32:04,124 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:32:04,126 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:32:04,132 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:32:04,133 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:32:04,282 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:32:04,282 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:32:04,302 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:32:04,302 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:32:04,302 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:32:04,302 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:32:09,707 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:32:09,707 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:32:23,273 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:32:24,263 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:32:24,603 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:32:24,611 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:32:24,611 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:32:24,611 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:32:24,611 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:32:54,231 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:32:54,898 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:32:55,220 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:32:55,220 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:32:55,230 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:32:55,232 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:32:55,232 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:33:23,774 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:33:24,839 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:33:25,255 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:33:25,259 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:33:25,260 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:33:25,261 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:33:25,261 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:35:42,314 - root - INFO - Starting HR Assistant...
2025-07-09 00:35:42,462 - root - INFO - Configuration validated successfully
2025-07-09 00:35:42,462 - root - INFO - Pre-warming the model...
2025-07-09 00:35:42,462 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:35:42,472 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:35:45,169 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:35:47,638 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:35:47,777 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:35:47,777 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:35:47,779 - root - INFO - Loading google LLM...
2025-07-09 00:35:47,792 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.040901899337768555, 'init_vectorstore': 4.992186546325684, 'setup_retriever': 0.0, 'load_llm': 0.15091371536254883, 'create_chains': 0.003331422805786133}
2025-07-09 00:35:47,792 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.32s
2025-07-09 00:35:51,580 - root - INFO - Model pre-warmed successfully
2025-07-09 00:35:51,580 - root - INFO - HR Assistant started successfully
2025-07-09 00:35:53,908 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:35:54,781 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:35:55,041 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:35:55,050 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:35:55,051 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:35:55,051 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:35:55,052 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:35:57,515 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:35:58,500 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:35:59,018 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:35:59,034 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:35:59,034 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:35:59,034 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:35:59,034 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:36:17,583 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:36:18,135 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:36:18,317 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:36:18,317 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:36:18,317 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:36:18,317 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:36:18,317 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:36:23,870 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:36:24,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:36:24,943 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:36:24,948 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:36:24,949 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:36:24,949 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:36:24,950 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:36:53,418 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:36:54,719 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:36:55,059 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:36:55,059 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:36:55,059 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:36:55,059 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:36:55,059 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:37:54,726 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:37:55,950 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:37:56,380 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:37:56,394 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:37:56,394 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:37:56,395 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:37:56,395 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:38:41,933 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:38:41,934 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:38:41,934 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:38:41,934 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:38:44,918 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:38:46,566 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:38:46,965 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:38:47,109 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:38:47,109 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:38:51,401 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:38:51,402 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:38:51,408 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:38:51,408 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:38:51,568 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:38:51,568 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:38:51,581 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:38:51,581 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:38:51,581 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 00:38:51,581 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 00:38:59,059 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:38:59,059 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:39:31,412 - root - INFO - Starting HR Assistant...
2025-07-09 00:39:31,553 - root - INFO - Configuration validated successfully
2025-07-09 00:39:31,554 - root - INFO - Pre-warming the model...
2025-07-09 00:39:31,555 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:39:31,555 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:39:36,121 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:39:40,978 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:39:41,161 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:39:41,165 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:39:41,165 - root - INFO - Loading groq LLM...
2025-07-09 00:39:43,034 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.12702012062072754, 'init_vectorstore': 9.164777755737305, 'setup_retriever': 0.0, 'load_llm': 2.056119918823242, 'create_chains': 0.0}
2025-07-09 00:39:43,034 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 11.48s
2025-07-09 00:39:48,743 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:39:48,767 - root - INFO - Model pre-warmed successfully
2025-07-09 00:39:48,767 - root - INFO - HR Assistant started successfully
2025-07-09 00:39:48,771 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:39:48,772 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:39:48,772 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:39:48,773 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:39:52,489 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:39:53,724 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:39:54,039 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:39:54,048 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:39:54,049 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:39:54,049 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:39:54,052 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:39:56,465 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:39:57,540 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:39:57,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:40:23,306 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:40:25,822 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:40:26,120 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:40:26,124 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:40:26,125 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:40:26,125 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:40:26,125 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:40:53,458 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:40:54,359 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:40:54,614 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:40:54,614 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:40:54,628 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:40:54,628 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:40:54,628 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:41:23,371 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:41:25,139 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:41:25,645 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:41:25,649 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:41:25,649 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:41:25,649 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:41:25,649 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:41:53,756 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:41:54,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:41:55,089 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:41:55,089 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:41:55,089 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:41:55,089 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:41:55,089 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:42:23,329 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:42:24,325 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:42:24,580 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:42:24,589 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:42:24,589 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:42:24,589 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:42:24,589 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:42:54,063 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:42:55,552 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:42:55,964 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:42:55,979 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:42:55,979 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:42:55,979 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:42:55,979 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:43:23,756 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:43:25,471 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:43:25,925 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:43:25,925 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:43:25,935 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:43:25,935 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:43:25,935 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:43:30,308 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:43:30,308 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:49:39,232 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:49:39,232 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:49:39,304 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:49:39,304 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:49:39,304 - admin_interface - INFO - Cache invalidated due to QDRANT_URL change
2025-07-09 00:49:59,173 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 00:49:59,174 - admin_interface - INFO - Successfully updated .env file
2025-07-09 00:49:59,180 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:49:59,182 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:49:59,182 - admin_interface - INFO - Cache invalidated due to QDRANT_URL change
2025-07-09 00:50:06,667 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 00:50:06,670 - root - INFO - Chain init: 0.00s
2025-07-09 00:50:17,625 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:50:17,625 - root - INFO - Question processing: 10.95s
2025-07-09 00:50:17,625 - root - INFO - ⏱️ ask_hr took 10.96 seconds
2025-07-09 00:50:24,362 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 00:50:24,364 - root - INFO - Chain init: 0.00s
2025-07-09 00:50:31,453 - root - ERROR - Error: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 394, in ask_hr
    result = qa_chain.invoke({"input": request.question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 651, in max_marginal_relevance_search
    return self.max_marginal_relevance_search_by_vector(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 683, in max_marginal_relevance_search_by_vector
    results = self.max_marginal_relevance_search_with_score_by_vector(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 716, in max_marginal_relevance_search_with_score_by_vector
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 593, in query_points
    return self._client.query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 607, in query_points
    res: grpc.QueryResponse = self.grpc_points.Query(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 277, in __call__
    response, ignored_call = self._with_call(
                             ^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 332, in _with_call
    return call.result(), call
           ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 440, in result
    raise self
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 315, in continuation
    response, call = self._thunk(new_method).with_call(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1198, in with_call
    return _end_unary_response_blocking(state, call, True, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
2025-07-09 00:50:44,427 - root - INFO - Received question: M5 LEAVE POLICY
2025-07-09 00:50:44,427 - root - INFO - Chain init: 0.00s
2025-07-09 00:50:55,704 - root - ERROR - Error: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 394, in ask_hr
    result = qa_chain.invoke({"input": request.question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 651, in max_marginal_relevance_search
    return self.max_marginal_relevance_search_by_vector(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 683, in max_marginal_relevance_search_by_vector
    results = self.max_marginal_relevance_search_with_score_by_vector(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 716, in max_marginal_relevance_search_with_score_by_vector
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 593, in query_points
    return self._client.query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 607, in query_points
    res: grpc.QueryResponse = self.grpc_points.Query(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 277, in __call__
    response, ignored_call = self._with_call(
                             ^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 332, in _with_call
    return call.result(), call
           ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 440, in result
    raise self
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 315, in continuation
    response, call = self._thunk(new_method).with_call(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1198, in with_call
    return _end_unary_response_blocking(state, call, True, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
2025-07-09 00:52:09,810 - root - INFO - Starting HR Assistant...
2025-07-09 00:52:09,951 - root - INFO - Configuration validated successfully
2025-07-09 00:52:09,951 - root - INFO - Pre-warming the model...
2025-07-09 00:52:09,951 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:52:09,951 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:52:14,487 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:52:16,786 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:52:22,220 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:52:22,366 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:52:22,368 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:52:22,368 - root - INFO - Loading groq LLM...
2025-07-09 00:52:24,051 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03765249252319336, 'init_vectorstore': 12.089432954788208, 'setup_retriever': 0.0, 'load_llm': 1.8311145305633545, 'create_chains': 0.0}
2025-07-09 00:52:24,051 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 14.10s
2025-07-09 00:52:25,142 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:52:27,875 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 00:52:32,697 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:52:32,718 - root - INFO - Model pre-warmed successfully
2025-07-09 00:52:32,718 - root - INFO - HR Assistant started successfully
2025-07-09 00:53:03,982 - root - INFO - Received question: M5 LEAVE POLICY
2025-07-09 00:53:03,982 - root - INFO - Chain init: 0.00s
2025-07-09 00:53:06,466 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:53:09,661 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 00:53:14,024 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:53:14,031 - root - INFO - Question processing: 10.05s
2025-07-09 00:53:14,031 - root - INFO - ⏱️ ask_hr took 10.05 seconds
2025-07-09 00:54:09,292 - root - INFO - Starting HR Assistant...
2025-07-09 00:54:09,404 - root - INFO - Configuration validated successfully
2025-07-09 00:54:09,404 - root - INFO - Pre-warming the model...
2025-07-09 00:54:09,420 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:54:09,420 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:54:13,292 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:54:15,100 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:54:18,289 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:54:18,424 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:54:18,426 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:54:18,426 - root - INFO - Loading groq LLM...
2025-07-09 00:54:20,108 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03165030479431152, 'init_vectorstore': 8.707292318344116, 'setup_retriever': 0.0036966800689697266, 'load_llm': 1.8149044513702393, 'create_chains': 0.0032846927642822266}
2025-07-09 00:54:20,110 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.69s
2025-07-09 00:54:21,779 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:54:24,454 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 00:54:28,869 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:54:28,894 - root - INFO - Model pre-warmed successfully
2025-07-09 00:54:28,894 - root - INFO - HR Assistant started successfully
2025-07-09 00:55:05,171 - root - INFO - Starting HR Assistant...
2025-07-09 00:55:05,375 - root - INFO - Configuration validated successfully
2025-07-09 00:55:05,375 - root - INFO - Pre-warming the model...
2025-07-09 00:55:05,375 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:55:05,375 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:55:09,374 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:55:14,376 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 241, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2579, in get_collection
    self.grpc_collections.Get(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 277, in __call__
    response, ignored_call = self._with_call(
                             ^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 332, in _with_call
    return call.result(), call
           ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 440, in result
    raise self
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 315, in continuation
    response, call = self._thunk(new_method).with_call(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1198, in with_call
    return _end_unary_response_blocking(state, call, True, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
2025-07-09 00:55:14,380 - root - ERROR - Startup failed: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 120, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 170, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 241, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2579, in get_collection
    self.grpc_collections.Get(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 277, in __call__
    response, ignored_call = self._with_call(
                             ^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 332, in _with_call
    return call.result(), call
           ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 440, in result
    raise self
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_interceptor.py", line 315, in continuation
    response, call = self._thunk(new_method).with_call(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1198, in with_call
    return _end_unary_response_blocking(state, call, True, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\grpc\_channel.py", line 1006, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = "Deadline Exceeded"
	debug_error_string = "UNKNOWN:Error received from peer  {grpc_message:"Deadline Exceeded", grpc_status:4}"
>
2025-07-09 00:55:44,353 - root - INFO - Starting HR Assistant...
2025-07-09 00:55:44,512 - root - INFO - Configuration validated successfully
2025-07-09 00:55:44,512 - root - INFO - Pre-warming the model...
2025-07-09 00:55:44,513 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 00:55:44,513 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 00:55:48,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:55:50,462 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:55:53,577 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 00:55:53,713 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:55:53,713 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:55:53,713 - root - INFO - Loading groq LLM...
2025-07-09 00:55:55,424 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05072450637817383, 'init_vectorstore': 8.848831176757812, 'setup_retriever': 0.0, 'load_llm': 1.8410074710845947, 'create_chains': 0.005218982696533203}
2025-07-09 00:55:55,426 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.91s
2025-07-09 00:55:56,180 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:56:00,208 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 00:56:04,799 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 00:56:04,831 - root - INFO - Model pre-warmed successfully
2025-07-09 00:56:04,831 - root - INFO - HR Assistant started successfully
2025-07-09 00:56:08,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 00:56:10,398 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 00:56:11,023 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 00:56:11,026 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:56:11,026 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 00:56:11,026 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 00:56:11,026 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 01:00:00,436 - root - INFO - Starting HR Assistant...
2025-07-09 01:00:00,583 - root - INFO - Configuration validated successfully
2025-07-09 01:00:00,583 - root - INFO - Pre-warming the model...
2025-07-09 01:00:00,583 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 01:00:00,583 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 01:00:03,996 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 01:00:04,975 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 01:00:07,959 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 01:00:08,094 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 01:00:08,096 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 01:00:08,096 - root - INFO - Loading groq LLM...
2025-07-09 01:00:09,769 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0727841854095459, 'init_vectorstore': 7.163316249847412, 'setup_retriever': 0.0, 'load_llm': 1.805403709411621, 'create_chains': 0.004023551940917969}
2025-07-09 01:00:09,769 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.19s
2025-07-09 01:00:10,094 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 01:00:11,971 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 01:00:14,302 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 01:00:14,339 - root - INFO - Model pre-warmed successfully
2025-07-09 01:00:14,339 - root - INFO - HR Assistant started successfully
2025-07-09 01:20:51,171 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 01:20:51,176 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 01:20:51,178 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 01:20:51,178 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 01:20:55,056 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 01:20:56,376 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 01:20:56,681 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 01:20:58,092 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 01:20:58,092 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:10:47,639 - root - INFO - Starting HR Assistant...
2025-07-09 03:10:47,787 - root - INFO - Configuration validated successfully
2025-07-09 03:10:47,787 - root - INFO - Pre-warming the model...
2025-07-09 03:10:47,787 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:10:47,787 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:10:52,386 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:10:54,607 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:10:59,170 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:10:59,317 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:10:59,317 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:10:59,317 - root - INFO - Loading groq LLM...
2025-07-09 03:11:01,149 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.173614501953125, 'init_vectorstore': 11.055310726165771, 'setup_retriever': 0.0, 'load_llm': 1.931795597076416, 'create_chains': 0.047820329666137695}
2025-07-09 03:11:01,149 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 13.36s
2025-07-09 03:11:02,832 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:11:05,630 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:11:09,680 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:11:09,720 - root - INFO - Model pre-warmed successfully
2025-07-09 03:11:09,721 - root - INFO - HR Assistant started successfully
2025-07-09 03:11:16,833 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:11:18,299 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:11:18,970 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:11:18,978 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:11:18,978 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:11:18,978 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:11:18,981 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:11:46,966 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:11:47,957 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:11:48,374 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:11:48,382 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:11:48,382 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:11:48,382 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:11:48,382 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:12:16,680 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:12:17,975 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:12:18,717 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:12:18,732 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:12:18,732 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:12:18,732 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:12:18,732 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:12:46,645 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:12:48,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:12:48,701 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:12:48,780 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:12:48,780 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:12:48,780 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:12:48,780 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:13:17,224 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:13:20,012 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:13:20,247 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:13:20,251 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:13:20,253 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:13:20,253 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:13:20,253 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:13:46,847 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:13:48,328 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:13:48,905 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:13:48,910 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:13:48,910 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:13:48,912 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:13:48,913 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:14:14,090 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:14:14,090 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:14:14,090 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:14:14,101 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:14:16,868 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:14:18,350 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:14:18,844 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:21:16,816 - root - INFO - Starting HR Assistant...
2025-07-09 03:21:16,986 - root - INFO - Configuration validated successfully
2025-07-09 03:21:16,986 - root - INFO - Pre-warming the model...
2025-07-09 03:21:16,986 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:21:16,986 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:21:20,931 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:21:23,229 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:21:25,788 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:21:25,990 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:21:25,990 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:21:25,990 - root - INFO - Loading groq LLM...
2025-07-09 03:21:28,009 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04786801338195801, 'init_vectorstore': 8.596091270446777, 'setup_retriever': 0.0, 'load_llm': 2.216865301132202, 'create_chains': 0.0039386749267578125}
2025-07-09 03:21:28,009 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 11.02s
2025-07-09 03:21:28,584 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:21:30,723 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:21:33,679 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:21:33,712 - root - INFO - Model pre-warmed successfully
2025-07-09 03:21:33,712 - root - INFO - HR Assistant started successfully
2025-07-09 03:22:45,084 - root - INFO - Starting HR Assistant...
2025-07-09 03:22:45,246 - root - INFO - Configuration validated successfully
2025-07-09 03:22:45,246 - root - INFO - Pre-warming the model...
2025-07-09 03:22:45,255 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:22:45,255 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:22:55,718 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:22:57,038 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:22:59,101 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:22:59,271 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:22:59,275 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:22:59,275 - root - INFO - Loading groq LLM...
2025-07-09 03:23:01,241 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04440164566040039, 'init_vectorstore': 13.642631769180298, 'setup_retriever': 0.0, 'load_llm': 2.1299655437469482, 'create_chains': 0.009502172470092773}
2025-07-09 03:23:01,241 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 15.99s
2025-07-09 03:23:02,065 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:23:03,951 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:23:06,411 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:23:06,456 - root - INFO - Model pre-warmed successfully
2025-07-09 03:23:06,457 - root - INFO - HR Assistant started successfully
2025-07-09 03:23:20,981 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:23:20,983 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:23:36,688 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:23:36,688 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:25:35,690 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:25:35,690 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:25:37,990 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:25:38,001 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:26:44,405 - root - INFO - Starting HR Assistant...
2025-07-09 03:26:44,581 - root - INFO - Configuration validated successfully
2025-07-09 03:26:44,581 - root - INFO - Pre-warming the model...
2025-07-09 03:26:44,585 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:26:44,585 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:26:48,888 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:26:51,308 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:26:54,821 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:26:55,004 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:26:55,028 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:26:55,033 - root - INFO - Loading groq LLM...
2025-07-09 03:26:56,783 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.06350898742675781, 'init_vectorstore': 9.998784303665161, 'setup_retriever': 0.0, 'load_llm': 1.9558920860290527, 'create_chains': 0.00554656982421875}
2025-07-09 03:26:56,784 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 12.20s
2025-07-09 03:26:58,923 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:27:01,808 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:27:05,354 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:27:05,374 - root - INFO - Model pre-warmed successfully
2025-07-09 03:27:05,376 - root - INFO - HR Assistant started successfully
2025-07-09 03:27:05,548 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:27:05,548 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:28:03,497 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:28:03,497 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:30:01,552 - root - INFO - Starting HR Assistant...
2025-07-09 03:30:01,695 - root - INFO - Configuration validated successfully
2025-07-09 03:30:01,695 - root - INFO - Pre-warming the model...
2025-07-09 03:30:01,695 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:30:01,695 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:30:05,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:30:07,777 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:30:10,740 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:30:10,883 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:30:10,883 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:30:10,883 - root - INFO - Loading groq LLM...
2025-07-09 03:30:12,453 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03215312957763672, 'init_vectorstore': 8.873090028762817, 'setup_retriever': 0.0, 'load_llm': 1.7130272388458252, 'create_chains': 0.0}
2025-07-09 03:30:12,453 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.76s
2025-07-09 03:30:12,800 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:30:14,846 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:30:17,661 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:30:17,687 - root - INFO - Model pre-warmed successfully
2025-07-09 03:30:17,687 - root - INFO - HR Assistant started successfully
2025-07-09 03:38:36,516 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:38:36,518 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:43:51,504 - root - INFO - Starting HR Assistant...
2025-07-09 03:43:51,665 - root - INFO - Configuration validated successfully
2025-07-09 03:43:51,665 - root - INFO - Pre-warming the model...
2025-07-09 03:43:51,673 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:43:51,673 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:43:54,485 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:43:55,437 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:43:57,040 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:43:57,202 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:43:57,202 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:43:57,202 - root - INFO - Loading groq LLM...
2025-07-09 03:43:58,994 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04751896858215332, 'init_vectorstore': 5.188396215438843, 'setup_retriever': 0.0, 'load_llm': 1.9536843299865723, 'create_chains': 0.0}
2025-07-09 03:43:59,005 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.33s
2025-07-09 03:43:59,376 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:44:01,157 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:44:03,615 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:44:03,631 - root - INFO - Model pre-warmed successfully
2025-07-09 03:44:03,631 - root - INFO - HR Assistant started successfully
2025-07-09 03:44:17,748 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:44:17,748 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:45:33,235 - root - INFO - Starting HR Assistant...
2025-07-09 03:45:33,385 - root - INFO - Configuration validated successfully
2025-07-09 03:45:33,385 - root - INFO - Pre-warming the model...
2025-07-09 03:45:33,385 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:45:33,385 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:45:37,750 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:45:39,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:45:40,206 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:45:40,359 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:45:40,359 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:45:40,359 - root - INFO - Loading groq LLM...
2025-07-09 03:45:41,908 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03172755241394043, 'init_vectorstore': 6.632763385772705, 'setup_retriever': 0.0, 'load_llm': 1.6865010261535645, 'create_chains': 0.015683650970458984}
2025-07-09 03:45:41,908 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.52s
2025-07-09 03:45:42,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:45:44,079 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:45:45,804 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:45:45,829 - root - INFO - Model pre-warmed successfully
2025-07-09 03:45:45,829 - root - INFO - HR Assistant started successfully
2025-07-09 03:49:08,857 - root - INFO - Starting HR Assistant...
2025-07-09 03:49:09,015 - root - INFO - Configuration validated successfully
2025-07-09 03:49:09,015 - root - INFO - Pre-warming the model...
2025-07-09 03:49:09,015 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:49:09,015 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:49:11,977 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:49:12,965 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:49:14,123 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:49:14,373 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:49:14,373 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:49:14,373 - root - INFO - Loading groq LLM...
2025-07-09 03:49:15,988 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.035550832748413086, 'init_vectorstore': 4.932308197021484, 'setup_retriever': 0.0, 'load_llm': 1.8630046844482422, 'create_chains': 0.0020148754119873047}
2025-07-09 03:49:15,990 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.97s
2025-07-09 03:49:16,383 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:49:18,066 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:49:19,711 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:49:19,731 - root - INFO - Model pre-warmed successfully
2025-07-09 03:49:19,731 - root - INFO - HR Assistant started successfully
2025-07-09 03:49:19,894 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:49:19,894 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:51:27,139 - root - INFO - Starting HR Assistant...
2025-07-09 03:51:27,424 - root - INFO - Configuration validated successfully
2025-07-09 03:51:27,424 - root - INFO - Pre-warming the model...
2025-07-09 03:51:27,424 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:51:27,424 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:51:31,494 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:51:32,393 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:51:34,279 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:51:34,466 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:51:34,466 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:51:34,466 - root - INFO - Loading groq LLM...
2025-07-09 03:51:36,078 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.045069217681884766, 'init_vectorstore': 6.621376037597656, 'setup_retriever': 0.0, 'load_llm': 1.798492670059204, 'create_chains': 0.0}
2025-07-09 03:51:36,078 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.65s
2025-07-09 03:51:36,444 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:51:38,247 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:51:40,757 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:51:40,772 - root - INFO - Model pre-warmed successfully
2025-07-09 03:51:40,781 - root - INFO - HR Assistant started successfully
2025-07-09 03:51:43,247 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:51:44,417 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:51:44,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:51:44,733 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:51:44,738 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:51:44,738 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:51:44,738 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:51:58,228 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:51:58,228 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:52:07,620 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:52:07,620 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:52:49,516 - root - INFO - Starting HR Assistant...
2025-07-09 03:52:49,663 - root - INFO - Configuration validated successfully
2025-07-09 03:52:49,663 - root - INFO - Pre-warming the model...
2025-07-09 03:52:49,663 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:52:49,663 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:52:52,827 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:52:54,327 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:52:56,234 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:52:56,385 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:52:56,385 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:52:56,385 - root - INFO - Loading groq LLM...
2025-07-09 03:52:58,097 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.051177263259887695, 'init_vectorstore': 6.368806600570679, 'setup_retriever': 0.0, 'load_llm': 1.8588290214538574, 'create_chains': 0.004027605056762695}
2025-07-09 03:52:58,097 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.43s
2025-07-09 03:52:58,454 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:53:00,401 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:53:02,940 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 03:53:02,975 - root - INFO - Model pre-warmed successfully
2025-07-09 03:53:02,975 - root - INFO - HR Assistant started successfully
2025-07-09 03:53:05,500 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:53:06,662 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:53:07,006 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:53:07,014 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:53:07,014 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:53:07,014 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:53:07,014 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:53:11,615 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:53:11,615 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:53:59,435 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:53:59,435 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:54:05,360 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:54:05,373 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:54:11,374 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 03:54:11,374 - admin_interface - INFO - Successfully updated .env file
2025-07-09 03:54:11,408 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:54:11,408 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:54:11,630 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:54:11,630 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:54:11,638 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:54:11,638 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:54:11,641 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:54:11,641 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:54:37,120 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:54:38,539 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:54:38,873 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:54:38,881 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:54:38,883 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:54:38,884 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 03:54:38,886 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 03:54:55,867 - root - INFO - Starting HR Assistant...
2025-07-09 03:54:56,011 - root - INFO - Configuration validated successfully
2025-07-09 03:54:56,011 - root - INFO - Pre-warming the model...
2025-07-09 03:54:56,011 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:54:56,011 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:54:58,742 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:54:59,980 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:55:01,937 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:55:02,065 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:55:02,065 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:55:02,065 - root - INFO - Loading google LLM...
2025-07-09 03:55:02,081 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03157186508178711, 'init_vectorstore': 5.7516419887542725, 'setup_retriever': 0.0, 'load_llm': 0.14377856254577637, 'create_chains': 0.0}
2025-07-09 03:55:02,081 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.07s
2025-07-09 03:55:02,526 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:55:04,431 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:55:07,975 - root - INFO - Model pre-warmed successfully
2025-07-09 03:55:07,975 - root - INFO - HR Assistant started successfully
2025-07-09 03:55:10,570 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:55:12,074 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:55:12,476 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:55:12,486 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:55:12,486 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:55:12,486 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:55:12,486 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:55:35,461 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:55:36,795 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:55:37,192 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:55:37,192 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:55:37,192 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:55:37,192 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:55:37,192 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:56:03,017 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:56:03,017 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:56:03,017 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:56:03,017 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:56:05,493 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:56:06,772 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:56:07,109 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:56:35,244 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:56:36,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:56:36,638 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:56:36,638 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:56:36,638 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:56:36,638 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:56:36,638 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:57:08,371 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:57:09,249 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:57:09,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:57:09,645 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:57:09,645 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:57:09,648 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:57:09,648 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:57:35,062 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:57:35,999 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:57:36,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:57:36,349 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:57:36,349 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:57:36,349 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:57:36,349 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:58:05,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:58:06,308 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:58:06,544 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:58:06,560 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:58:06,560 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:58:06,560 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:58:06,560 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:58:49,970 - root - INFO - Starting HR Assistant...
2025-07-09 03:58:50,113 - root - INFO - Configuration validated successfully
2025-07-09 03:58:50,113 - root - INFO - Pre-warming the model...
2025-07-09 03:58:50,113 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 03:58:50,113 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 03:58:52,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:58:54,054 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:58:56,525 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 03:58:56,653 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:58:56,653 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:58:56,669 - root - INFO - Loading google LLM...
2025-07-09 03:58:56,669 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03252410888671875, 'init_vectorstore': 6.269160032272339, 'setup_retriever': 0.0, 'load_llm': 0.14325881004333496, 'create_chains': 0.0}
2025-07-09 03:58:56,669 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.56s
2025-07-09 03:58:57,095 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:58:59,907 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 03:59:03,686 - root - INFO - Model pre-warmed successfully
2025-07-09 03:59:03,688 - root - INFO - HR Assistant started successfully
2025-07-09 03:59:19,807 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 03:59:20,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 03:59:21,041 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 03:59:21,049 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:59:21,049 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:59:21,049 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:59:21,049 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:59:22,275 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:59:22,275 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 03:59:29,483 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 03:59:29,484 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:05:47,890 - root - INFO - Starting HR Assistant...
2025-07-09 04:05:48,049 - root - INFO - Configuration validated successfully
2025-07-09 04:05:48,051 - root - INFO - Pre-warming the model...
2025-07-09 04:05:48,051 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:05:48,051 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:05:54,354 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:05:55,432 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:05:57,326 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:05:57,496 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:05:57,496 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:05:57,496 - root - INFO - Loading google LLM...
2025-07-09 04:05:57,512 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0368494987487793, 'init_vectorstore': 9.096122741699219, 'setup_retriever': 0.0, 'load_llm': 0.17749571800231934, 'create_chains': 0.008121013641357422}
2025-07-09 04:05:57,512 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.46s
2025-07-09 04:05:58,071 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:06:00,212 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:06:04,819 - root - INFO - Model pre-warmed successfully
2025-07-09 04:06:04,819 - root - INFO - HR Assistant started successfully
2025-07-09 04:07:08,563 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:07:09,385 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:07:09,625 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:07:09,625 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:07:09,625 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:07:09,640 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:07:09,640 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:07:11,932 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:07:12,676 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:07:13,087 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:07:13,090 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:07:13,090 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:07:13,097 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:07:13,097 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:07:16,022 - root - INFO - Upload request received - folder: Data, file count: 1
2025-07-09 04:07:16,022 - root - INFO - Successfully uploaded 1 files to Data
2025-07-09 04:10:03,320 - root - INFO - Starting HR Assistant...
2025-07-09 04:10:03,472 - root - INFO - Configuration validated successfully
2025-07-09 04:10:03,474 - root - INFO - Pre-warming the model...
2025-07-09 04:10:03,474 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:10:03,474 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:10:06,538 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:10:07,442 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:10:09,337 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:10:09,466 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:10:09,466 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:10:09,468 - root - INFO - Loading google LLM...
2025-07-09 04:10:09,481 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04673027992248535, 'init_vectorstore': 5.676127672195435, 'setup_retriever': 0.0, 'load_llm': 0.13855218887329102, 'create_chains': 0.0050203800201416016}
2025-07-09 04:10:09,481 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.01s
2025-07-09 04:10:09,914 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:10:12,053 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:10:15,685 - root - INFO - Model pre-warmed successfully
2025-07-09 04:10:15,685 - root - INFO - HR Assistant started successfully
2025-07-09 04:10:33,537 - root - INFO - Starting HR Assistant...
2025-07-09 04:10:33,711 - root - INFO - Configuration validated successfully
2025-07-09 04:10:33,712 - root - INFO - Pre-warming the model...
2025-07-09 04:10:33,712 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:10:33,712 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:10:36,924 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:10:37,835 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:10:39,473 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:10:39,625 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:10:39,627 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:10:39,627 - root - INFO - Loading google LLM...
2025-07-09 04:10:39,647 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.041115760803222656, 'init_vectorstore': 5.557965040206909, 'setup_retriever': 0.0, 'load_llm': 0.17030739784240723, 'create_chains': 0.004207611083984375}
2025-07-09 04:10:39,647 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.93s
2025-07-09 04:10:39,970 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:10:41,945 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:10:45,149 - root - INFO - Model pre-warmed successfully
2025-07-09 04:10:45,165 - root - INFO - HR Assistant started successfully
2025-07-09 04:10:58,298 - root - INFO - Starting HR Assistant...
2025-07-09 04:10:58,455 - root - INFO - Configuration validated successfully
2025-07-09 04:10:58,455 - root - INFO - Pre-warming the model...
2025-07-09 04:10:58,455 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:10:58,455 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:11:01,174 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:11:02,141 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:11:03,143 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:11:03,288 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:11:03,291 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:11:03,291 - root - INFO - Loading google LLM...
2025-07-09 04:11:03,313 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05295062065124512, 'init_vectorstore': 4.460195779800415, 'setup_retriever': 0.0, 'load_llm': 0.16609954833984375, 'create_chains': 0.002015352249145508}
2025-07-09 04:11:03,314 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 4.86s
2025-07-09 04:11:03,628 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:11:05,693 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:11:08,861 - root - INFO - Model pre-warmed successfully
2025-07-09 04:11:08,861 - root - INFO - HR Assistant started successfully
2025-07-09 04:11:15,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:11:16,461 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:11:16,791 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:11:16,791 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:11:16,791 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:11:16,807 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:11:16,807 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:13:23,743 - root - INFO - Starting HR Assistant...
2025-07-09 04:13:23,871 - root - INFO - Configuration validated successfully
2025-07-09 04:13:23,871 - root - INFO - Pre-warming the model...
2025-07-09 04:13:23,871 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:13:23,873 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:13:27,796 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:13:28,926 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:13:30,234 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:13:30,427 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:13:30,427 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:13:30,427 - root - INFO - Loading google LLM...
2025-07-09 04:13:30,447 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0372462272644043, 'init_vectorstore': 6.197484731674194, 'setup_retriever': 0.0, 'load_llm': 0.20637917518615723, 'create_chains': 0.004032135009765625}
2025-07-09 04:13:30,447 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.57s
2025-07-09 04:13:30,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:13:32,952 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:13:36,159 - root - INFO - Model pre-warmed successfully
2025-07-09 04:13:36,175 - root - INFO - HR Assistant started successfully
2025-07-09 04:13:49,903 - root - INFO - Starting HR Assistant...
2025-07-09 04:13:50,093 - root - INFO - Configuration validated successfully
2025-07-09 04:13:50,093 - root - INFO - Pre-warming the model...
2025-07-09 04:13:50,093 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:13:50,095 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:13:52,892 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:13:54,046 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:13:54,953 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:13:55,111 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:13:55,111 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:13:55,111 - root - INFO - Loading google LLM...
2025-07-09 04:13:55,131 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03984570503234863, 'init_vectorstore': 4.611779689788818, 'setup_retriever': 0.0, 'load_llm': 0.17387008666992188, 'create_chains': 0.004100799560546875}
2025-07-09 04:13:55,131 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.04s
2025-07-09 04:13:55,414 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:13:56,844 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:14:00,288 - root - INFO - Model pre-warmed successfully
2025-07-09 04:14:00,288 - root - INFO - HR Assistant started successfully
2025-07-09 04:14:07,542 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:14:08,591 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:14:08,833 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:14:08,849 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:14:08,849 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:14:08,849 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:14:08,849 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:14:17,954 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-09 04:14:17,968 - root - INFO - Successfully uploaded 3 files to Data
2025-07-09 04:15:55,001 - root - INFO - Starting HR Assistant...
2025-07-09 04:15:55,156 - root - INFO - Configuration validated successfully
2025-07-09 04:15:55,156 - root - INFO - Pre-warming the model...
2025-07-09 04:15:55,161 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:15:55,161 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:15:57,774 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:15:58,801 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:15:59,679 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:15:59,806 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:15:59,806 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:15:59,806 - root - INFO - Loading google LLM...
2025-07-09 04:15:59,830 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04030656814575195, 'init_vectorstore': 4.316900730133057, 'setup_retriever': 0.0066106319427490234, 'load_llm': 0.1484975814819336, 'create_chains': 0.0023429393768310547}
2025-07-09 04:15:59,830 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 4.67s
2025-07-09 04:16:00,090 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:16:01,750 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:16:06,353 - root - INFO - Model pre-warmed successfully
2025-07-09 04:16:06,353 - root - INFO - HR Assistant started successfully
2025-07-09 04:19:01,990 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:19:02,731 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:19:02,974 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:19:02,981 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:19:02,981 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:19:02,981 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:19:02,981 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:19:33,169 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-09 04:19:33,181 - root - INFO - Successfully uploaded 3 files to Data
2025-07-09 04:19:38,676 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-09 04:19:38,676 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:19:38,676 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:19:52,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:19:53,787 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:20:00,209 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:20:04,092 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:20:07,913 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:20:11,551 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:20:11,559 - root - INFO - Successfully ingested documents. Chunks ingested: 249
2025-07-09 04:20:11,570 - root - INFO - Successfully removed directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:20:37,701 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-09 04:20:37,711 - root - INFO - Successfully uploaded 3 files to Data
2025-07-09 04:20:46,984 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-09 04:20:46,984 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:20:46,984 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:20:52,560 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:20:53,384 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:21:01,693 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:21:07,930 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:21:14,698 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:21:21,282 - root - ERROR - Error during document ingestion: The write operation timed out
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 136, in handle_request
    raise exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 88, in handle_request
    self._send_request_body(**kwargs)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 159, in _send_request_body
    self._send_event(event, timeout=timeout)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 166, in _send_event
    self._network_stream.write(bytes_to_send, timeout=timeout)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 135, in write
    with map_exceptions(exc_map):
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.WriteTimeout: The write operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 134, in send_inner
    response = self._client.send(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.WriteTimeout: The write operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 302, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 93, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 89, in sync_ingest
    vectordatabase.add_documents(chunks)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 288, in add_documents
    return self.add_texts(texts, metadatas, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 444, in add_texts
    self.client.upsert(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 1633, in upsert
    return self._client.upsert(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 1911, in upsert
    http_result = self.openapi_client.points_api.upsert_points(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\points_api.py", line 987, in upsert_points
    return self._build_for_upsert_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\points_api.py", line 512, in _build_for_upsert_points
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 112, in send
    response = self.middleware(request, self.send_inner)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 250, in __call__
    return call_next(request)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 136, in send_inner
    raise ResponseHandlingException(e)
qdrant_client.http.exceptions.ResponseHandlingException: The write operation timed out
2025-07-09 04:21:21,605 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: The write operation timed out
2025-07-09 04:22:57,062 - root - INFO - Starting HR Assistant...
2025-07-09 04:22:57,205 - root - INFO - Configuration validated successfully
2025-07-09 04:22:57,205 - root - INFO - Pre-warming the model...
2025-07-09 04:22:57,205 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:22:57,205 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:22:59,808 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:23:00,692 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:23:02,745 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:23:02,887 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:23:02,887 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:23:02,887 - root - INFO - Loading google LLM...
2025-07-09 04:23:02,905 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.0318911075592041, 'init_vectorstore': 5.349577903747559, 'setup_retriever': 0.0, 'load_llm': 0.14195775985717773, 'create_chains': 0.017731189727783203}
2025-07-09 04:23:02,906 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.70s
2025-07-09 04:23:03,148 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:23:04,678 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:23:09,166 - root - INFO - Model pre-warmed successfully
2025-07-09 04:23:09,166 - root - INFO - HR Assistant started successfully
2025-07-09 04:24:07,567 - root - INFO - Starting HR Assistant...
2025-07-09 04:24:07,717 - root - INFO - Configuration validated successfully
2025-07-09 04:24:07,717 - root - INFO - Pre-warming the model...
2025-07-09 04:24:07,717 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:24:07,717 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:24:11,214 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:24:12,201 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:24:13,512 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:24:13,655 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:24:13,655 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:24:13,655 - root - INFO - Loading google LLM...
2025-07-09 04:24:13,678 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.037775278091430664, 'init_vectorstore': 5.607518672943115, 'setup_retriever': 0.0, 'load_llm': 0.16458559036254883, 'create_chains': 0.0021033287048339844}
2025-07-09 04:24:13,678 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.96s
2025-07-09 04:24:14,006 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:24:16,006 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:24:20,237 - root - INFO - Model pre-warmed successfully
2025-07-09 04:24:20,237 - root - INFO - HR Assistant started successfully
2025-07-09 04:27:03,326 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:27:04,479 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:27:04,811 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:27:04,817 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:27:04,817 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:27:04,817 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:27:04,817 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:27:04,830 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-09 04:27:04,830 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:27:04,835 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:27:16,646 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:27:17,389 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:27:22,403 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:27:25,542 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:27:29,636 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:27:33,433 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 04:27:33,433 - root - INFO - Successfully ingested documents. Chunks ingested: 249
2025-07-09 04:27:33,433 - root - INFO - Successfully removed directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 04:32:17,402 - root - INFO - Starting HR Assistant...
2025-07-09 04:32:17,568 - root - INFO - Configuration validated successfully
2025-07-09 04:32:17,571 - root - INFO - Pre-warming the model...
2025-07-09 04:32:17,571 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:32:17,571 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:32:20,236 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:32:21,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:32:23,460 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:32:23,601 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:32:23,601 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:32:23,601 - root - INFO - Loading google LLM...
2025-07-09 04:32:23,616 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.031438350677490234, 'init_vectorstore': 5.711004257202148, 'setup_retriever': 0.0, 'load_llm': 0.1527254581451416, 'create_chains': 0.004033088684082031}
2025-07-09 04:32:23,616 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.05s
2025-07-09 04:32:23,842 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:32:25,205 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:32:34,624 - root - INFO - Model pre-warmed successfully
2025-07-09 04:32:34,624 - root - INFO - HR Assistant started successfully
2025-07-09 04:33:22,566 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:33:22,566 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:33:23,553 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:33:23,553 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:33:23,964 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:33:23,965 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:33:23,974 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:33:23,974 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:33:23,975 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:33:23,975 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:34:25,561 - root - INFO - Starting HR Assistant...
2025-07-09 04:34:25,705 - root - INFO - Configuration validated successfully
2025-07-09 04:34:25,705 - root - INFO - Pre-warming the model...
2025-07-09 04:34:25,705 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 04:34:25,705 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 04:34:29,464 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:34:30,447 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:34:32,208 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 04:34:32,346 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:34:32,346 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:34:32,346 - root - INFO - Loading google LLM...
2025-07-09 04:34:32,368 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03215932846069336, 'init_vectorstore': 6.325373649597168, 'setup_retriever': 0.0, 'load_llm': 0.15603947639465332, 'create_chains': 0.004000663757324219}
2025-07-09 04:34:32,371 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.67s
2025-07-09 04:34:32,609 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:34:33,996 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 04:34:37,051 - root - INFO - Model pre-warmed successfully
2025-07-09 04:34:37,051 - root - INFO - HR Assistant started successfully
2025-07-09 04:34:40,444 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:34:40,444 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:34:41,361 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:34:41,361 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:34:41,912 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:34:41,912 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:34:41,927 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:34:41,927 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:34:41,927 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:34:41,927 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:34:53,768 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:34:54,906 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:34:55,145 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:34:58,922 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:35:00,103 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:35:04,940 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:35:05,953 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:35:06,687 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:35:09,321 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:35:10,305 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:35:15,579 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:35:16,567 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:35:16,979 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:35:20,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 04:35:21,656 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 04:35:22,068 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 04:47:11,700 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:47:11,700 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:47:30,346 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:47:30,348 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 04:49:59,293 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 04:49:59,293 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 05:42:35,479 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 05:42:35,480 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 05:43:36,998 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 05:43:36,998 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:03:46,037 - root - INFO - Starting HR Assistant...
2025-07-09 22:03:46,249 - root - INFO - Configuration validated successfully
2025-07-09 22:03:46,250 - root - INFO - Pre-warming the model...
2025-07-09 22:03:46,251 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:03:46,251 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:03:50,071 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:03:51,105 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:03:52,834 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:03:53,021 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:03:53,024 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:03:53,025 - root - INFO - Loading google LLM...
2025-07-09 22:03:53,091 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.16861248016357422, 'init_vectorstore': 6.167709827423096, 'setup_retriever': 0.0006401538848876953, 'load_llm': 0.24463605880737305, 'create_chains': 0.01206207275390625}
2025-07-09 22:03:53,093 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.84s
2025-07-09 22:03:53,441 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:03:54,774 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:03:59,586 - root - INFO - Model pre-warmed successfully
2025-07-09 22:03:59,586 - root - INFO - HR Assistant started successfully
2025-07-09 22:05:52,673 - root - INFO - Starting HR Assistant...
2025-07-09 22:05:52,838 - root - INFO - Configuration validated successfully
2025-07-09 22:05:52,839 - root - INFO - Pre-warming the model...
2025-07-09 22:05:52,840 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:05:52,841 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:05:56,161 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:05:58,325 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:06:00,310 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:06:00,507 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:06:00,508 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:06:00,509 - root - INFO - Loading google LLM...
2025-07-09 22:06:00,545 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04188799858093262, 'init_vectorstore': 7.252043962478638, 'setup_retriever': 0.0, 'load_llm': 0.22919440269470215, 'create_chains': 0.005579948425292969}
2025-07-09 22:06:00,547 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.70s
2025-07-09 22:06:00,832 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:06:02,167 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:06:04,937 - root - INFO - Model pre-warmed successfully
2025-07-09 22:06:04,937 - root - INFO - HR Assistant started successfully
2025-07-09 22:06:26,101 - root - INFO - Starting HR Assistant...
2025-07-09 22:06:26,377 - root - INFO - Configuration validated successfully
2025-07-09 22:06:26,380 - root - INFO - Pre-warming the model...
2025-07-09 22:06:26,381 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:06:26,382 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:06:29,667 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:06:31,067 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:06:32,835 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:06:32,996 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:06:32,998 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:06:32,999 - root - INFO - Loading google LLM...
2025-07-09 22:06:33,019 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.07274651527404785, 'init_vectorstore': 6.155054807662964, 'setup_retriever': 0.0, 'load_llm': 0.18089509010314941, 'create_chains': 0.002999544143676758}
2025-07-09 22:06:33,020 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.64s
2025-07-09 22:06:33,270 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:06:34,629 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:06:37,781 - root - INFO - Model pre-warmed successfully
2025-07-09 22:06:37,783 - root - INFO - HR Assistant started successfully
2025-07-09 22:07:01,422 - root - INFO - Starting HR Assistant...
2025-07-09 22:07:01,584 - root - INFO - Configuration validated successfully
2025-07-09 22:07:01,585 - root - INFO - Pre-warming the model...
2025-07-09 22:07:01,585 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:07:01,586 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:07:04,641 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:07:05,197 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:07:06,390 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:07:06,591 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:07:06,594 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:07:06,595 - root - INFO - Loading google LLM...
2025-07-09 22:07:06,611 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.13028955459594727, 'init_vectorstore': 4.342235088348389, 'setup_retriever': 0.0, 'load_llm': 0.216902494430542, 'create_chains': 0.00420379638671875}
2025-07-09 22:07:06,612 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.02s
2025-07-09 22:07:06,842 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:07:08,841 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:07:13,219 - root - INFO - Model pre-warmed successfully
2025-07-09 22:07:13,220 - root - INFO - HR Assistant started successfully
2025-07-09 22:07:45,846 - root - INFO - Starting HR Assistant...
2025-07-09 22:07:45,990 - root - INFO - Configuration validated successfully
2025-07-09 22:07:45,995 - root - INFO - Pre-warming the model...
2025-07-09 22:07:45,995 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:07:45,995 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:07:49,178 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:07:50,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:07:52,549 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:07:52,718 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:07:52,719 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:07:52,720 - root - INFO - Loading google LLM...
2025-07-09 22:07:52,742 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05874752998352051, 'init_vectorstore': 6.3279290199279785, 'setup_retriever': 0.0010211467742919922, 'load_llm': 0.18739724159240723, 'create_chains': 0.005430936813354492}
2025-07-09 22:07:52,744 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.75s
2025-07-09 22:07:52,995 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:07:55,235 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:07:58,971 - root - INFO - Model pre-warmed successfully
2025-07-09 22:07:58,971 - root - INFO - HR Assistant started successfully
2025-07-09 22:08:20,217 - root - INFO - Starting HR Assistant...
2025-07-09 22:08:20,384 - root - INFO - Configuration validated successfully
2025-07-09 22:08:20,411 - root - INFO - Pre-warming the model...
2025-07-09 22:08:20,411 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:08:20,412 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:08:23,275 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:08:23,839 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:08:25,052 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:08:25,268 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:08:25,270 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:08:25,271 - root - INFO - Loading google LLM...
2025-07-09 22:08:25,295 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04219341278076172, 'init_vectorstore': 4.422333002090454, 'setup_retriever': 0.0, 'load_llm': 0.23540210723876953, 'create_chains': 0.007061958312988281}
2025-07-09 22:08:25,296 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 4.88s
2025-07-09 22:08:25,537 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:08:26,862 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:08:30,000 - root - INFO - Model pre-warmed successfully
2025-07-09 22:08:30,001 - root - INFO - HR Assistant started successfully
2025-07-09 22:09:04,761 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-09 22:09:04,761 - root - INFO - Chain init: 0.00s
2025-07-09 22:09:05,849 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:09:07,407 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:09:09,420 - root - INFO - Question processing: 4.66s
2025-07-09 22:09:09,420 - root - INFO - ⏱️ ask_hr took 4.66 seconds
2025-07-09 22:10:45,989 - root - INFO - Starting HR Assistant...
2025-07-09 22:10:46,218 - root - INFO - Configuration validated successfully
2025-07-09 22:10:46,219 - root - INFO - Pre-warming the model...
2025-07-09 22:10:46,220 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:10:46,222 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:10:50,082 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:10:50,980 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:10:52,650 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:10:52,980 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:10:52,980 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:10:52,980 - root - INFO - Loading google LLM...
2025-07-09 22:10:53,006 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04895782470703125, 'init_vectorstore': 6.167617082595825, 'setup_retriever': 0.0, 'load_llm': 0.3510897159576416, 'create_chains': 0.004958152770996094}
2025-07-09 22:10:53,008 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.78s
2025-07-09 22:10:53,290 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:10:55,762 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:10:58,906 - root - INFO - Model pre-warmed successfully
2025-07-09 22:10:58,906 - root - INFO - HR Assistant started successfully
2025-07-09 22:11:04,135 - root - INFO - Received question: WHAT IS M7 LEAVE POLICY
2025-07-09 22:11:04,136 - root - INFO - Chain init: 0.00s
2025-07-09 22:11:04,916 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:11:06,585 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:11:08,666 - root - INFO - Question processing: 4.53s
2025-07-09 22:11:08,667 - root - INFO - ⏱️ ask_hr took 4.53 seconds
2025-07-09 22:11:57,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:11:57,106 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:11:57,646 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 22:11:57,646 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 22:11:57,841 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:11:57,842 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:11:57,858 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:11:57,858 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:11:57,858 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:11:57,861 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:12:09,758 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-09 22:12:09,758 - root - WARNING - Path does not exist: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 22:12:09,760 - root - WARNING - HTTPException in ingest_api: Path does not exist: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 22:13:20,041 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-09 22:13:20,052 - root - INFO - Successfully uploaded 3 files to Data
2025-07-09 22:13:23,441 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-09 22:13:23,441 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 22:13:23,441 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 22:13:38,673 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:13:39,517 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:13:44,783 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 22:13:48,715 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 22:13:53,796 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 22:13:58,709 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-09 22:13:58,715 - root - INFO - Successfully ingested documents. Chunks ingested: 249
2025-07-09 22:13:58,715 - root - INFO - Successfully removed directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-09 22:14:13,762 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:14:13,762 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:14:14,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 22:14:14,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 22:14:15,240 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:14:15,240 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:14:15,255 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:14:15,255 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:14:15,257 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:14:15,258 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:14:25,056 - root - INFO - Received question: WHAT IS M7 LEAVE POLICY
2025-07-09 22:14:25,056 - root - INFO - Chain init: 0.00s
2025-07-09 22:14:25,742 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:14:27,487 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:14:31,041 - root - INFO - Question processing: 5.98s
2025-07-09 22:14:31,041 - root - INFO - ⏱️ ask_hr took 5.98 seconds
2025-07-09 22:16:26,790 - root - INFO - Starting HR Assistant...
2025-07-09 22:16:26,970 - root - INFO - Configuration validated successfully
2025-07-09 22:16:26,971 - root - INFO - Pre-warming the model...
2025-07-09 22:16:26,972 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:16:26,972 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:16:29,379 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:16:29,943 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:16:31,155 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:16:31,332 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:16:31,334 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:16:31,335 - root - INFO - Loading google LLM...
2025-07-09 22:16:31,350 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.03995251655578613, 'init_vectorstore': 4.009467601776123, 'setup_retriever': 0.0, 'load_llm': 0.19203662872314453, 'create_chains': 0.0030028820037841797}
2025-07-09 22:16:31,352 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 4.38s
2025-07-09 22:16:31,574 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:16:33,046 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:16:36,817 - root - INFO - Model pre-warmed successfully
2025-07-09 22:16:36,818 - root - INFO - HR Assistant started successfully
2025-07-09 22:16:53,113 - root - INFO - Starting HR Assistant...
2025-07-09 22:16:53,307 - root - INFO - Configuration validated successfully
2025-07-09 22:16:53,308 - root - INFO - Pre-warming the model...
2025-07-09 22:16:53,309 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:16:53,309 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:16:56,621 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:16:57,179 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:16:58,416 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:16:58,635 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:16:58,636 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:16:58,638 - root - INFO - Loading google LLM...
2025-07-09 22:16:58,658 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.06813383102416992, 'init_vectorstore': 4.84398889541626, 'setup_retriever': 0.0, 'load_llm': 0.23844361305236816, 'create_chains': 0.0036246776580810547}
2025-07-09 22:16:58,661 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.35s
2025-07-09 22:16:58,908 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:17:00,245 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:17:04,191 - root - INFO - Model pre-warmed successfully
2025-07-09 22:17:04,192 - root - INFO - HR Assistant started successfully
2025-07-09 22:17:18,962 - root - INFO - Starting HR Assistant...
2025-07-09 22:17:19,127 - root - INFO - Configuration validated successfully
2025-07-09 22:17:19,128 - root - INFO - Pre-warming the model...
2025-07-09 22:17:19,129 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:17:19,130 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:17:21,748 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:17:22,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:17:24,195 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:17:24,414 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:17:24,414 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:17:24,415 - root - INFO - Loading google LLM...
2025-07-09 22:17:24,432 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04336380958557129, 'init_vectorstore': 4.886192321777344, 'setup_retriever': 0.0, 'load_llm': 0.2326202392578125, 'create_chains': 0.0037996768951416016}
2025-07-09 22:17:24,433 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 5.30s
2025-07-09 22:17:24,665 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:17:25,998 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:17:30,976 - root - INFO - Model pre-warmed successfully
2025-07-09 22:17:30,985 - root - INFO - HR Assistant started successfully
2025-07-09 22:17:38,257 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:17:38,257 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:17:39,329 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 22:17:39,329 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 22:17:39,511 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:17:39,529 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:17:39,541 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:17:39,545 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:17:39,559 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:17:39,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:17:54,675 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:17:54,677 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:18:10,800 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:18:10,801 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:50:13,873 - root - INFO - Starting HR Assistant...
2025-07-09 22:50:14,023 - root - INFO - Configuration validated successfully
2025-07-09 22:50:14,023 - root - INFO - Pre-warming the model...
2025-07-09 22:50:14,023 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:50:14,023 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:50:17,981 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:50:19,018 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:50:20,340 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 22:50:20,473 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:50:20,473 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:50:20,473 - root - INFO - Loading google LLM...
2025-07-09 22:50:20,536 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.1606299877166748, 'init_vectorstore': 5.975987195968628, 'setup_retriever': 0.0, 'load_llm': 0.19135761260986328, 'create_chains': 0.005236625671386719}
2025-07-09 22:50:20,536 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.51s
2025-07-09 22:50:21,079 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:50:22,984 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:50:26,695 - root - INFO - Model pre-warmed successfully
2025-07-09 22:50:26,695 - root - INFO - HR Assistant started successfully
2025-07-09 22:50:26,706 - root - INFO - Received question: If i do 4 more excess leave then how much deduction from 200k salary
2025-07-09 22:50:26,706 - root - INFO - Chain init: 0.00s
2025-07-09 22:50:26,945 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:50:28,361 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 22:50:31,505 - root - INFO - Question processing: 4.80s
2025-07-09 22:50:31,505 - root - INFO - ⏱️ ask_hr took 4.80 seconds
2025-07-09 22:51:01,349 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:51:01,349 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:51:12,303 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 22:51:12,304 - admin_interface - INFO - Successfully updated .env file
2025-07-09 22:51:12,336 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:51:12,339 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:51:12,476 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:51:12,476 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:51:12,486 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:51:12,486 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:51:12,486 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 22:51:12,486 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 22:51:40,817 - root - INFO - Starting HR Assistant...
2025-07-09 22:51:40,956 - root - INFO - Configuration validated successfully
2025-07-09 22:51:40,956 - root - INFO - Pre-warming the model...
2025-07-09 22:51:40,956 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:51:40,956 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:51:43,946 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:51:44,940 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:51:46,828 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 400 Bad Request"
2025-07-09 22:51:46,830 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: status_code: 400, body: {'id': 'ffd677ea-9125-4d8a-9fda-1bca212d7b86', 'message': 'invalid request: valid input_type must be provided with the provided model'}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 255, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 220, in embed_documents
    resp = self.client.embed(texts=texts, model=self.model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 208, in embed
    responses = [
                ^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
                       ^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\base_client.py", line 1833, in embed
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'id': 'ffd677ea-9125-4d8a-9fda-1bca212d7b86', 'message': 'invalid request: valid input_type must be provided with the provided model'}
2025-07-09 22:51:47,026 - root - ERROR - Startup failed: status_code: 400, body: {'id': 'ffd677ea-9125-4d8a-9fda-1bca212d7b86', 'message': 'invalid request: valid input_type must be provided with the provided model'}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 255, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 220, in embed_documents
    resp = self.client.embed(texts=texts, model=self.model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 208, in embed
    responses = [
                ^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
                       ^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\base_client.py", line 1833, in embed
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'id': 'ffd677ea-9125-4d8a-9fda-1bca212d7b86', 'message': 'invalid request: valid input_type must be provided with the provided model'}
2025-07-09 22:52:43,719 - root - INFO - Starting HR Assistant...
2025-07-09 22:52:43,896 - root - INFO - Configuration validated successfully
2025-07-09 22:52:43,896 - root - INFO - Pre-warming the model...
2025-07-09 22:52:43,905 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:52:43,905 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:52:48,260 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:52:48,992 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:52:49,660 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:52:49,690 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:52:49,690 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:53:18,828 - root - INFO - Starting HR Assistant...
2025-07-09 22:53:18,967 - root - INFO - Configuration validated successfully
2025-07-09 22:53:18,967 - root - INFO - Pre-warming the model...
2025-07-09 22:53:18,969 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:53:18,969 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:53:22,298 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:53:23,284 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:53:24,107 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 400 Bad Request"
2025-07-09 22:53:24,109 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: status_code: 400, body: {'id': 'f9a7ac36-29e3-4975-a7d5-14fcbfb416b1', 'message': 'invalid request: valid input_type must be provided with the provided model'}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 255, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 220, in embed_documents
    resp = self.client.embed(texts=texts, model=self.model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 208, in embed
    responses = [
                ^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
                       ^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\base_client.py", line 1833, in embed
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'id': 'f9a7ac36-29e3-4975-a7d5-14fcbfb416b1', 'message': 'invalid request: valid input_type must be provided with the provided model'}
2025-07-09 22:53:24,128 - root - ERROR - Startup failed: status_code: 400, body: {'id': 'f9a7ac36-29e3-4975-a7d5-14fcbfb416b1', 'message': 'invalid request: valid input_type must be provided with the provided model'}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 255, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 220, in embed_documents
    resp = self.client.embed(texts=texts, model=self.model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 208, in embed
    responses = [
                ^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
                       ^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\cohere\base_client.py", line 1833, in embed
    raise BadRequestError(
cohere.errors.bad_request_error.BadRequestError: status_code: 400, body: {'id': 'f9a7ac36-29e3-4975-a7d5-14fcbfb416b1', 'message': 'invalid request: valid input_type must be provided with the provided model'}
2025-07-09 22:53:55,543 - root - INFO - Starting HR Assistant...
2025-07-09 22:53:55,693 - root - INFO - Configuration validated successfully
2025-07-09 22:53:55,693 - root - INFO - Pre-warming the model...
2025-07-09 22:53:55,693 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:53:55,693 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:53:59,590 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:54:00,589 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:54:01,240 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:54:01,266 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:54:01,275 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:54:15,841 - root - INFO - Starting HR Assistant...
2025-07-09 22:54:16,012 - root - INFO - Configuration validated successfully
2025-07-09 22:54:16,012 - root - INFO - Pre-warming the model...
2025-07-09 22:54:16,012 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:54:16,017 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:54:19,368 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:54:20,188 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:54:21,024 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:54:21,197 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:54:21,200 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:54:35,876 - root - INFO - Starting HR Assistant...
2025-07-09 22:54:36,056 - root - INFO - Configuration validated successfully
2025-07-09 22:54:36,058 - root - INFO - Pre-warming the model...
2025-07-09 22:54:36,058 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:54:36,058 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:54:39,535 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:54:40,280 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:54:41,184 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:54:41,209 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:54:41,212 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:54:53,126 - root - INFO - Starting HR Assistant...
2025-07-09 22:54:53,283 - root - INFO - Configuration validated successfully
2025-07-09 22:54:53,283 - root - INFO - Pre-warming the model...
2025-07-09 22:54:53,285 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:54:53,285 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:54:56,452 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:54:57,196 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:54:58,011 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:54:58,020 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:54:58,020 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:56:02,682 - root - INFO - Starting HR Assistant...
2025-07-09 22:56:02,842 - root - INFO - Configuration validated successfully
2025-07-09 22:56:02,844 - root - INFO - Pre-warming the model...
2025-07-09 22:56:02,844 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:56:02,845 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:56:06,388 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:56:07,213 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:56:08,698 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:56:08,711 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:56:08,716 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:56:19,119 - root - INFO - Starting HR Assistant...
2025-07-09 22:56:19,278 - root - INFO - Configuration validated successfully
2025-07-09 22:56:19,278 - root - INFO - Pre-warming the model...
2025-07-09 22:56:19,278 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:56:19,278 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:56:22,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:56:24,045 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:56:25,356 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:56:25,375 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:56:25,380 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:56:36,819 - root - INFO - Starting HR Assistant...
2025-07-09 22:56:36,984 - root - INFO - Configuration validated successfully
2025-07-09 22:56:36,986 - root - INFO - Pre-warming the model...
2025-07-09 22:56:36,986 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:56:36,986 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:56:40,221 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:56:41,326 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:56:42,129 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:56:42,142 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:56:42,145 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:59:13,020 - root - INFO - Starting HR Assistant...
2025-07-09 22:59:13,156 - root - INFO - Configuration validated successfully
2025-07-09 22:59:13,156 - root - INFO - Pre-warming the model...
2025-07-09 22:59:13,156 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 22:59:13,156 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 22:59:16,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 22:59:17,464 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 22:59:19,772 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 22:59:19,783 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 22:59:19,787 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 23:00:52,560 - root - INFO - Starting HR Assistant...
2025-07-09 23:00:52,684 - root - INFO - Configuration validated successfully
2025-07-09 23:00:52,684 - root - INFO - Pre-warming the model...
2025-07-09 23:00:52,684 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:00:52,684 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:00:55,851 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:00:56,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:00:57,749 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:00:57,765 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 23:00:57,769 - root - ERROR - Startup failed: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1116, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection is configured for dense vectors with 768 dimensions. Selected embeddings are 1024-dimensional. If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-09 23:10:58,944 - root - INFO - Starting HR Assistant...
2025-07-09 23:10:59,085 - root - INFO - Configuration validated successfully
2025-07-09 23:10:59,085 - root - INFO - Pre-warming the model...
2025-07-09 23:10:59,088 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:10:59,088 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:11:02,578 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:11:03,528 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:11:05,281 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:11:05,291 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 23:11:05,430 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 23:11:05,430 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 23:11:05,430 - root - INFO - Loading google LLM...
2025-07-09 23:11:05,631 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.7865591049194336, 'init_vectorstore': 5.285318851470947, 'setup_retriever': 0.0, 'load_llm': 0.3406064510345459, 'create_chains': 0.0}
2025-07-09 23:11:05,631 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 6.54s
2025-07-09 23:11:05,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:11:06,292 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:11:07,350 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:11:07,652 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:11:12,073 - root - INFO - Model pre-warmed successfully
2025-07-09 23:11:12,073 - root - INFO - HR Assistant started successfully
2025-07-09 23:12:20,824 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 23:12:20,824 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 23:12:27,259 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 23:12:27,259 - admin_interface - INFO - Successfully updated .env file
2025-07-09 23:12:27,312 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:12:27,314 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:12:27,493 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:12:27,500 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:12:27,512 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 23:12:27,512 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 23:12:27,512 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 23:12:27,512 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 23:12:34,935 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 23:12:34,935 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 23:12:34,935 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-09 23:12:34,935 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-09 23:12:37,496 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:12:38,479 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:12:38,657 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:12:38,831 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:12:38,831 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:13:18,823 - root - INFO - Received question: If i do 4 more excess leave then how much deduction from 200k salary
2025-07-09 23:13:18,823 - root - INFO - Chain init: 0.00s
2025-07-09 23:13:19,799 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:13:20,869 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:13:21,539 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:13:21,853 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:13:24,172 - root - INFO - Question processing: 5.35s
2025-07-09 23:13:24,172 - root - INFO - ⏱️ ask_hr took 5.35 seconds
2025-07-09 23:13:56,132 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:13:56,137 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:14:28,935 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 23:14:28,935 - admin_interface - INFO - Successfully updated .env file
2025-07-09 23:14:40,871 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:14:40,871 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:15:49,736 - root - INFO - Starting HR Assistant...
2025-07-09 23:15:49,878 - root - INFO - Configuration validated successfully
2025-07-09 23:15:49,878 - root - INFO - Pre-warming the model...
2025-07-09 23:15:49,878 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:15:49,878 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:15:52,857 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:15:53,741 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:15:55,952 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Error embedding content: 404 models/embedings-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_google_genai\embeddings.py", line 229, in embed_documents
    result = self.client.batch_embed_contents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 1437, in batch_embed_contents
    response = rpc(
               ^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.NotFound: 404 models/embedings-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_google_genai\embeddings.py", line 233, in embed_documents
    raise GoogleGenerativeAIError(f"Error embedding content: {e}") from e
langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 404 models/embedings-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.
2025-07-09 23:15:56,276 - root - ERROR - Startup failed: Error embedding content: 404 models/embedings-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_google_genai\embeddings.py", line 229, in embed_documents
    result = self.client.batch_embed_contents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py", line 1437, in batch_embed_contents
    response = rpc(
               ^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\gapic_v1\method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
                 ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\retry\retry_unary.py", line 147, in retry_target
    result = target()
             ^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\google\api_core\grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.NotFound: 404 models/embedings-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_google_genai\embeddings.py", line 233, in embed_documents
    raise GoogleGenerativeAIError(f"Error embedding content: {e}") from e
langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 404 models/embedings-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.
2025-07-09 23:17:35,027 - root - INFO - Starting HR Assistant...
2025-07-09 23:17:35,202 - root - INFO - Configuration validated successfully
2025-07-09 23:17:35,202 - root - INFO - Pre-warming the model...
2025-07-09 23:17:35,202 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:17:35,205 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:17:38,450 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:17:39,592 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:17:40,994 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 23:17:41,144 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:17:41,150 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:17:41,150 - root - INFO - Loading groq LLM...
2025-07-09 23:17:42,997 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.05707716941833496, 'init_vectorstore': 5.546798467636108, 'setup_retriever': 0.0, 'load_llm': 2.000239849090576, 'create_chains': 0.0035657882690429688}
2025-07-09 23:17:42,999 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.79s
2025-07-09 23:17:43,208 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:17:45,274 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:17:47,001 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 23:17:47,078 - root - INFO - Model pre-warmed successfully
2025-07-09 23:17:47,078 - root - INFO - HR Assistant started successfully
2025-07-09 23:17:58,580 - root - INFO - Starting HR Assistant...
2025-07-09 23:17:58,715 - root - INFO - Configuration validated successfully
2025-07-09 23:17:58,715 - root - INFO - Pre-warming the model...
2025-07-09 23:17:58,715 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:17:58,715 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:18:01,372 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:18:02,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:18:04,671 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 23:18:04,819 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:18:04,819 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:18:04,819 - root - INFO - Loading groq LLM...
2025-07-09 23:18:06,417 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.041387081146240234, 'init_vectorstore': 5.76580023765564, 'setup_retriever': 0.0, 'load_llm': 1.7450199127197266, 'create_chains': 0.0003941059112548828}
2025-07-09 23:18:06,417 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.70s
2025-07-09 23:18:06,627 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:18:08,392 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:18:10,688 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 23:18:10,708 - root - INFO - Model pre-warmed successfully
2025-07-09 23:18:10,709 - root - INFO - HR Assistant started successfully
2025-07-09 23:19:23,935 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:19:23,938 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:19:24,681 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:19:25,179 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:19:25,179 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:19:25,196 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:19:25,196 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:19:25,196 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:19:25,196 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:19:25,671 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:19:26,171 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:19:26,171 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:20:07,797 - root - INFO - Received question: If i do 4 more excess leave then how much deduction from 200k salary
2025-07-09 23:20:07,797 - root - INFO - Chain init: 0.00s
2025-07-09 23:20:08,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:20:10,602 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:20:12,917 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 23:20:12,928 - root - INFO - Question processing: 5.13s
2025-07-09 23:20:12,928 - root - INFO - ⏱️ ask_hr took 5.13 seconds
2025-07-09 23:21:42,062 - root - INFO - Received question: i am an M7 grade and if i do 4 excess leave how much salary deduction my salary is 200k
2025-07-09 23:21:42,062 - root - INFO - Chain init: 0.00s
2025-07-09 23:21:46,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:21:49,059 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:21:52,628 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 23:21:52,633 - root - INFO - Question processing: 10.57s
2025-07-09 23:21:52,634 - root - INFO - ⏱️ ask_hr took 10.57 seconds
2025-07-09 23:22:23,281 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:22:24,430 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:22:24,899 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:22:24,904 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:22:24,910 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:22:24,910 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:22:24,910 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:22:25,183 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:22:25,183 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:22:34,729 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-09 23:22:34,734 - admin_interface - INFO - Successfully updated .env file
2025-07-09 23:22:34,741 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:22:34,743 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:22:34,919 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:22:34,919 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:22:34,928 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:22:34,928 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:22:34,928 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:22:34,928 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:23:25,674 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:23:26,979 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:23:27,178 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:23:27,183 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:23:27,183 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:23:27,183 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:23:27,183 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:23:51,996 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:23:51,997 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:23:51,998 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:23:51,998 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:23:54,163 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:23:54,919 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:23:55,378 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:24:40,557 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:24:41,557 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:24:41,886 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:24:41,891 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:24:41,891 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:24:41,891 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:24:41,891 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:24:55,911 - root - INFO - Starting HR Assistant...
2025-07-09 23:24:56,073 - root - INFO - Configuration validated successfully
2025-07-09 23:24:56,073 - root - INFO - Pre-warming the model...
2025-07-09 23:24:56,073 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:24:56,073 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:25:08,311 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: timed out
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 101, in handle_request
    raise exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 134, in send_inner
    response = self._client.send(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 112, in send
    response = self.middleware(request, self.send_inner)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 250, in __call__
    return call_next(request)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 136, in send_inner
    raise ResponseHandlingException(e)
qdrant_client.http.exceptions.ResponseHandlingException: timed out
2025-07-09 23:25:08,607 - root - ERROR - Startup failed: timed out
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 101, in handle_request
    raise exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 134, in send_inner
    response = self._client.send(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 112, in send
    response = self.middleware(request, self.send_inner)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 250, in __call__
    return call_next(request)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 136, in send_inner
    raise ResponseHandlingException(e)
qdrant_client.http.exceptions.ResponseHandlingException: timed out
2025-07-09 23:26:46,681 - root - INFO - Starting HR Assistant...
2025-07-09 23:26:46,811 - root - INFO - Configuration validated successfully
2025-07-09 23:26:46,811 - root - INFO - Pre-warming the model...
2025-07-09 23:26:46,811 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-09 23:26:46,811 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-09 23:26:50,096 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:26:51,150 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:26:52,545 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:26:52,555 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-09 23:26:52,712 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:26:52,712 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:26:52,713 - root - INFO - Loading groq LLM...
2025-07-09 23:26:54,264 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.7620177268981934, 'init_vectorstore': 4.8352210521698, 'setup_retriever': 0.0, 'load_llm': 1.7047131061553955, 'create_chains': 0.003563404083251953}
2025-07-09 23:26:54,264 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.45s
2025-07-09 23:26:54,591 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:26:54,955 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:26:55,581 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:26:55,936 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:26:57,636 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 23:26:57,656 - root - INFO - Model pre-warmed successfully
2025-07-09 23:26:57,656 - root - INFO - HR Assistant started successfully
2025-07-09 23:26:59,861 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:27:01,173 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:27:01,416 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:27:01,426 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:01,426 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:01,426 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:01,426 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:01,434 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:01,434 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:01,434 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:01,436 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:03,726 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:27:04,958 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:27:05,217 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:27:38,009 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:38,009 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:38,009 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:38,014 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:40,605 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:27:42,101 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:27:42,665 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:27:55,584 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:27:56,902 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:27:57,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:27:57,321 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:57,321 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:57,321 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:57,321 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:57,391 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:57,403 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:57,405 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:27:57,405 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:27:59,945 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:28:00,360 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:28:01,256 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:28:01,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:28:01,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:28:01,922 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:28:01,924 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:28:02,325 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:28:30,243 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:28:31,562 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:28:31,882 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:28:31,904 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:28:31,904 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:28:31,904 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:28:31,904 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:29:00,363 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:29:01,375 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:29:01,842 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:29:01,844 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:29:01,844 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:29:01,850 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:29:01,850 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:29:01,858 - root - INFO - Received question: i am an M7 grade and if i do 4 excess leave how much salary deduction my salary is 100k
2025-07-09 23:29:01,863 - root - INFO - Chain init: 0.00s
2025-07-09 23:29:03,079 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:29:03,850 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:29:04,294 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-09 23:29:04,717 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-09 23:29:07,211 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-09 23:29:07,213 - root - INFO - Question processing: 5.35s
2025-07-09 23:29:07,215 - root - INFO - ⏱️ ask_hr took 5.36 seconds
2025-07-09 23:29:30,397 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-09 23:29:31,389 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-09 23:29:31,636 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-09 23:29:31,654 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:29:31,654 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-09 23:29:31,660 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-09 23:29:31,661 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:25,677 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:25,692 - root - INFO - Configuration validated successfully
2025-07-10 06:45:25,695 - root - INFO - Pre-warming the model...
2025-07-10 06:45:25,697 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:25,698 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:26,283 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:26,388 - root - INFO - Configuration validated successfully
2025-07-10 06:45:26,464 - root - INFO - Pre-warming the model...
2025-07-10 06:45:26,470 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:26,471 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:27,378 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:27,469 - root - INFO - Configuration validated successfully
2025-07-10 06:45:27,471 - root - INFO - Pre-warming the model...
2025-07-10 06:45:27,564 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:27,566 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:28,273 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:28,380 - root - INFO - Configuration validated successfully
2025-07-10 06:45:28,467 - root - INFO - Pre-warming the model...
2025-07-10 06:45:28,471 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:28,482 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:28,677 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:28,679 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:28,782 - root - INFO - Configuration validated successfully
2025-07-10 06:45:28,867 - root - INFO - Pre-warming the model...
2025-07-10 06:45:28,867 - root - INFO - Configuration validated successfully
2025-07-10 06:45:28,870 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:28,880 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:28,873 - root - INFO - Pre-warming the model...
2025-07-10 06:45:28,968 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:28,975 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:29,366 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:29,479 - root - INFO - Configuration validated successfully
2025-07-10 06:45:29,565 - root - INFO - Pre-warming the model...
2025-07-10 06:45:29,567 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:29,572 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:29,667 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:29,774 - root - INFO - Configuration validated successfully
2025-07-10 06:45:29,779 - root - INFO - Pre-warming the model...
2025-07-10 06:45:29,864 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:29,867 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:29,865 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:29,978 - root - INFO - Configuration validated successfully
2025-07-10 06:45:30,067 - root - INFO - Pre-warming the model...
2025-07-10 06:45:30,078 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:30,164 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:31,275 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:31,573 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:31,577 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:31,574 - root - INFO - Configuration validated successfully
2025-07-10 06:45:31,667 - root - INFO - Pre-warming the model...
2025-07-10 06:45:31,676 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:31,764 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:31,773 - root - INFO - Configuration validated successfully
2025-07-10 06:45:31,865 - root - INFO - Pre-warming the model...
2025-07-10 06:45:31,867 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:31,868 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:31,870 - root - INFO - Configuration validated successfully
2025-07-10 06:45:31,964 - root - INFO - Pre-warming the model...
2025-07-10 06:45:31,974 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:32,064 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:32,670 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:32,774 - root - INFO - Configuration validated successfully
2025-07-10 06:45:32,776 - root - INFO - Pre-warming the model...
2025-07-10 06:45:32,778 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:32,868 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:32,881 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:32,972 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:33,769 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:33,877 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:33,970 - root - INFO - Configuration validated successfully
2025-07-10 06:45:33,970 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:33,975 - root - INFO - Pre-warming the model...
2025-07-10 06:45:34,065 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:34,068 - root - INFO - Configuration validated successfully
2025-07-10 06:45:34,075 - root - INFO - Pre-warming the model...
2025-07-10 06:45:34,077 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:34,069 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:34,079 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:34,365 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:34,366 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:34,369 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:34,473 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:34,566 - root - INFO - Configuration validated successfully
2025-07-10 06:45:34,582 - root - INFO - Pre-warming the model...
2025-07-10 06:45:34,666 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:34,675 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:34,767 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:34,875 - root - INFO - Starting HR Assistant...
2025-07-10 06:45:34,966 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:34,976 - root - INFO - Configuration validated successfully
2025-07-10 06:45:34,983 - root - INFO - Pre-warming the model...
2025-07-10 06:45:35,067 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:45:35,073 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:45:35,252 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:35,398 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:35,458 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:35,659 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:36,353 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:36,633 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:37,043 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:37,044 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:37,048 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:37,048 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:37,055 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:37,057 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:37,065 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:37,067 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:37,165 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:37,171 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:37,172 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:37,176 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:37,181 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:37,183 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:37,185 - root - INFO - Loading groq LLM...
2025-07-10 06:45:37,186 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:37,189 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:37,191 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:37,191 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:37,192 - root - INFO - Loading groq LLM...
2025-07-10 06:45:37,193 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:37,194 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:37,196 - root - INFO - Loading groq LLM...
2025-07-10 06:45:37,199 - root - INFO - Loading groq LLM...
2025-07-10 06:45:37,602 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:37,653 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:37,660 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:37,776 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:37,784 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:37,785 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:37,787 - root - INFO - Loading groq LLM...
2025-07-10 06:45:37,986 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.96718430519104, 'init_vectorstore': 9.492472887039185, 'setup_retriever': 0.0005097389221191406, 'load_llm': 0.8127539157867432, 'create_chains': 0.008440732955932617}
2025-07-10 06:45:38,057 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 12.36s
2025-07-10 06:45:38,087 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.5862114429473877, 'init_vectorstore': 6.007895469665527, 'setup_retriever': 0.0014836788177490234, 'load_llm': 0.9033119678497314, 'create_chains': 0.007996559143066406}
2025-07-10 06:45:38,089 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.52s
2025-07-10 06:45:38,161 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.9051563739776611, 'init_vectorstore': 6.295333385467529, 'setup_retriever': 0.0007092952728271484, 'load_llm': 0.9224226474761963, 'create_chains': 0.06655383110046387}
2025-07-10 06:45:38,163 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.20s
2025-07-10 06:45:38,166 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.4024858474731445, 'init_vectorstore': 7.194761037826538, 'setup_retriever': 0.0003662109375, 'load_llm': 0.9826614856719971, 'create_chains': 0.012497186660766602}
2025-07-10 06:45:38,168 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.60s
2025-07-10 06:45:38,252 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:38,253 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:38,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,256 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:38,258 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:38,354 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,613 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.5961272716522217, 'init_vectorstore': 6.196248292922974, 'setup_retriever': 0.0007717609405517578, 'load_llm': 0.8274796009063721, 'create_chains': 0.009527921676635742}
2025-07-10 06:45:38,615 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.64s
2025-07-10 06:45:38,865 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,866 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,866 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,866 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,867 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:38,867 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:38,871 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:39,338 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:39,337 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:39,337 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:39,338 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:39,340 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:39,343 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:39,344 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:39,349 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:39,457 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:39,467 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:39,476 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:39,477 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:39,479 - root - INFO - Loading groq LLM...
2025-07-10 06:45:39,481 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:39,483 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:39,485 - root - INFO - Loading groq LLM...
2025-07-10 06:45:39,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:39,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:39,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:39,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:39,920 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.3918070793151855, 'init_vectorstore': 5.286234140396118, 'setup_retriever': 0.007036924362182617, 'load_llm': 0.4470381736755371, 'create_chains': 0.0062007904052734375}
2025-07-10 06:45:39,922 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.15s
2025-07-10 06:45:39,931 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.3137168884277344, 'init_vectorstore': 5.075160503387451, 'setup_retriever': 0.000652313232421875, 'load_llm': 0.4673902988433838, 'create_chains': 0.006624698638916016}
2025-07-10 06:45:39,933 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.06s
2025-07-10 06:45:40,122 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:40,123 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,124 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,124 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,127 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,154 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:40,168 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:40,170 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:40,171 - root - INFO - Loading groq LLM...
2025-07-10 06:45:40,501 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.3919904232025146, 'init_vectorstore': 7.887583255767822, 'setup_retriever': 0.0003695487976074219, 'load_llm': 0.33852696418762207, 'create_chains': 0.008066654205322266}
2025-07-10 06:45:40,502 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.63s
2025-07-10 06:45:40,571 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:40,571 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:40,572 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:40,572 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:40,577 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:40,579 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,616 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:40,653 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:40,656 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:40,657 - root - INFO - Loading groq LLM...
2025-07-10 06:45:40,954 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,953 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:40,960 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,962 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,962 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,963 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,977 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:40,983 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:41,058 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:41,059 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:41,073 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:41,154 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:41,155 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:41,157 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:41,158 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:41,164 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:41,164 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:41,169 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:41,177 - root - INFO - Loading groq LLM...
2025-07-10 06:45:41,180 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:41,184 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:41,185 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:41,187 - root - INFO - Loading groq LLM...
2025-07-10 06:45:41,199 - root - INFO - Loading groq LLM...
2025-07-10 06:45:41,206 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:41,213 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:41,235 - root - INFO - Loading groq LLM...
2025-07-10 06:45:41,232 - root - INFO - Loading groq LLM...
2025-07-10 06:45:41,386 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.7153525352478027, 'init_vectorstore': 5.733831882476807, 'setup_retriever': 0.0008401870727539062, 'load_llm': 0.7567455768585205, 'create_chains': 0.013242006301879883}
2025-07-10 06:45:41,454 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.39s
2025-07-10 06:45:41,674 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:41,677 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:42,052 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:42,054 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:42,055 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:42,055 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:42,061 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:42,353 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:42,354 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:42,364 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:42,366 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:42,367 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:42,368 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:42,460 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:42,376 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:42,467 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:42,526 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:42,554 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:42,554 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:42,556 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:42,569 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:42,571 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:42,928 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:42,954 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:43,069 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.6879127025604248, 'init_vectorstore': 5.20802116394043, 'setup_retriever': 0.0009534358978271484, 'load_llm': 1.9839982986450195, 'create_chains': 0.011645317077636719}
2025-07-10 06:45:43,071 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.90s
2025-07-10 06:45:43,178 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.9130918979644775, 'init_vectorstore': 6.174929141998291, 'setup_retriever': 0.0006396770477294922, 'load_llm': 2.108048677444458, 'create_chains': 0.01154947280883789}
2025-07-10 06:45:43,253 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.38s
2025-07-10 06:45:43,282 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.5005080699920654, 'init_vectorstore': 12.175001859664917, 'setup_retriever': 0.00046133995056152344, 'load_llm': 2.1193175315856934, 'create_chains': 0.007801532745361328}
2025-07-10 06:45:43,282 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.6015441417694092, 'init_vectorstore': 5.189738512039185, 'setup_retriever': 0.00030231475830078125, 'load_llm': 2.2167556285858154, 'create_chains': 0.0062007904052734375}
2025-07-10 06:45:43,352 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 16.88s
2025-07-10 06:45:43,356 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.18s
2025-07-10 06:45:43,393 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.1742308139801025, 'init_vectorstore': 4.904096364974976, 'setup_retriever': 0.0029816627502441406, 'load_llm': 2.218320608139038, 'create_chains': 0.010908126831054688}
2025-07-10 06:45:43,397 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.32s
2025-07-10 06:45:43,502 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:43,502 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:44,035 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:44,035 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:44,036 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:44,038 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:44,069 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:44,070 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:44,414 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:44,414 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:44,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:45:44,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:44,735 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:44,736 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:44,749 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:44,778 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:44,779 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:44,781 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:44,783 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:45,153 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:45,153 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:45:45,158 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '608948de8c4c0c5b8934373502621589', 'date': 'Thu, 10 Jul 2025 06:45:44 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'df5ba266-51cb-4978-aad1-0fa95b143273', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '608948de8c4c0c5b8934373502621589', 'date': 'Thu, 10 Jul 2025 06:45:44 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'df5ba266-51cb-4978-aad1-0fa95b143273', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:45:45,726 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 06:45:45,910 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:46,137 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:46,173 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:46,174 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:48,275 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:48,275 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:48,278 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:48,303 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:48,313 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:48,315 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:48,316 - root - INFO - Loading groq LLM...
2025-07-10 06:45:48,690 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:45:48,690 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:45:48,694 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '89f47a7c66022052a97630175a1931cf', 'date': 'Thu, 10 Jul 2025 06:45:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'addd93b2-2c9b-464c-9262-e9e365170f5f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '89f47a7c66022052a97630175a1931cf', 'date': 'Thu, 10 Jul 2025 06:45:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'addd93b2-2c9b-464c-9262-e9e365170f5f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:45:48,695 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '346790f3313e893bd3eb6de140d6b694', 'date': 'Thu, 10 Jul 2025 06:45:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '20', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '391b2f14-3a23-4ec5-acf8-83d1376dcfb4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '346790f3313e893bd3eb6de140d6b694', 'date': 'Thu, 10 Jul 2025 06:45:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '20', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '391b2f14-3a23-4ec5-acf8-83d1376dcfb4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:45:48,810 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2904956340789795, 'init_vectorstore': 12.329605102539062, 'setup_retriever': 0.0008873939514160156, 'load_llm': 0.49944567680358887, 'create_chains': 0.006917715072631836}
2025-07-10 06:45:48,811 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 14.13s
2025-07-10 06:45:49,024 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:49,025 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:49,025 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:49,055 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:49,059 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:45:49,062 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:49,063 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:49,064 - root - INFO - Loading groq LLM...
2025-07-10 06:45:49,065 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:45:49,067 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:45:49,074 - root - INFO - Loading groq LLM...
2025-07-10 06:45:49,497 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 4.193889379501343, 'init_vectorstore': 15.19318699836731, 'setup_retriever': 0.0007076263427734375, 'load_llm': 0.4315927028656006, 'create_chains': 0.006645917892456055}
2025-07-10 06:45:49,498 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.92s
2025-07-10 06:45:49,514 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:49,516 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:45:49,536 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.7986092567443848, 'init_vectorstore': 14.983923435211182, 'setup_retriever': 0.0006582736968994141, 'load_llm': 0.4720449447631836, 'create_chains': 0.008497476577758789}
2025-07-10 06:45:49,552 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.38s
2025-07-10 06:45:49,919 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:49,924 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:45:49,929 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dc5c3c6d9b7c04a0175b75e552b114ea', 'date': 'Thu, 10 Jul 2025 06:45:49 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '17', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'ed628ad1-5627-47f7-872b-f927c92a520a', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dc5c3c6d9b7c04a0175b75e552b114ea', 'date': 'Thu, 10 Jul 2025 06:45:49 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '17', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'ed628ad1-5627-47f7-872b-f927c92a520a', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:45:50,352 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:50,352 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:45:50,357 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8f5f9012fded3724c32bf5b28cd05f9', 'date': 'Thu, 10 Jul 2025 06:45:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'dbcdf767-7cb2-4ed5-95ca-8d19e514ad48', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8f5f9012fded3724c32bf5b28cd05f9', 'date': 'Thu, 10 Jul 2025 06:45:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'dbcdf767-7cb2-4ed5-95ca-8d19e514ad48', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:45:50,662 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:45:50,662 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:50,662 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:45:50,988 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:45:50,988 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:45:50,993 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '946f3d5fd7f1abea6fc66a4ee216fffe', 'date': 'Thu, 10 Jul 2025 06:45:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2b53e4f4-1e07-4964-968d-cd6189a750a6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '946f3d5fd7f1abea6fc66a4ee216fffe', 'date': 'Thu, 10 Jul 2025 06:45:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2b53e4f4-1e07-4964-968d-cd6189a750a6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:45:50,994 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dee2c6b96dc3ae06b9d4615528ee6607', 'date': 'Thu, 10 Jul 2025 06:45:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6eeb46ac-8cf4-4d4f-8f3c-cdf976a982aa', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dee2c6b96dc3ae06b9d4615528ee6607', 'date': 'Thu, 10 Jul 2025 06:45:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6eeb46ac-8cf4-4d4f-8f3c-cdf976a982aa', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:45:51,648 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:51,676 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:51,678 - root - INFO - HR Assistant started successfully
2025-07-10 06:45:51,977 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:45:52,007 - root - INFO - Model pre-warmed successfully
2025-07-10 06:45:52,009 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:05,897 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:05,914 - root - INFO - Configuration validated successfully
2025-07-10 06:46:05,915 - root - INFO - Pre-warming the model...
2025-07-10 06:46:05,918 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:05,923 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:06,021 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:06,099 - root - INFO - Configuration validated successfully
2025-07-10 06:46:06,101 - root - INFO - Pre-warming the model...
2025-07-10 06:46:06,104 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:06,106 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:06,596 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:06,693 - root - INFO - Configuration validated successfully
2025-07-10 06:46:06,694 - root - INFO - Pre-warming the model...
2025-07-10 06:46:06,696 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:06,698 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:07,088 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:07,104 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:07,196 - root - INFO - Configuration validated successfully
2025-07-10 06:46:07,198 - root - INFO - Pre-warming the model...
2025-07-10 06:46:07,207 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:07,289 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:07,291 - root - INFO - Configuration validated successfully
2025-07-10 06:46:07,296 - root - INFO - Pre-warming the model...
2025-07-10 06:46:07,298 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:07,300 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:07,604 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:07,698 - root - INFO - Configuration validated successfully
2025-07-10 06:46:07,703 - root - INFO - Pre-warming the model...
2025-07-10 06:46:07,705 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:07,789 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:07,802 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:08,090 - root - INFO - Configuration validated successfully
2025-07-10 06:46:08,095 - root - INFO - Pre-warming the model...
2025-07-10 06:46:08,097 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:08,102 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:08,106 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:08,491 - root - INFO - Configuration validated successfully
2025-07-10 06:46:08,503 - root - INFO - Pre-warming the model...
2025-07-10 06:46:08,591 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:08,689 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:08,792 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:08,803 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:08,899 - root - INFO - Configuration validated successfully
2025-07-10 06:46:08,903 - root - INFO - Pre-warming the model...
2025-07-10 06:46:08,989 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:08,996 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:09,090 - root - INFO - Configuration validated successfully
2025-07-10 06:46:09,099 - root - INFO - Pre-warming the model...
2025-07-10 06:46:09,101 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:09,105 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:09,099 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:09,301 - root - INFO - Configuration validated successfully
2025-07-10 06:46:09,389 - root - INFO - Pre-warming the model...
2025-07-10 06:46:09,394 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:09,400 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:10,290 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:10,310 - root - INFO - Configuration validated successfully
2025-07-10 06:46:10,395 - root - INFO - Pre-warming the model...
2025-07-10 06:46:10,401 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:10,488 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:10,497 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:10,890 - root - INFO - Configuration validated successfully
2025-07-10 06:46:10,895 - root - INFO - Pre-warming the model...
2025-07-10 06:46:10,903 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:10,905 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:11,102 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:12,198 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:12,299 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:12,302 - root - INFO - Configuration validated successfully
2025-07-10 06:46:12,390 - root - INFO - Pre-warming the model...
2025-07-10 06:46:12,393 - root - INFO - Configuration validated successfully
2025-07-10 06:46:12,490 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:12,494 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:12,498 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:12,589 - root - INFO - Pre-warming the model...
2025-07-10 06:46:12,596 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:12,598 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:12,601 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:12,604 - root - INFO - Configuration validated successfully
2025-07-10 06:46:12,603 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:12,611 - root - INFO - Pre-warming the model...
2025-07-10 06:46:12,697 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:12,695 - root - INFO - Configuration validated successfully
2025-07-10 06:46:12,702 - root - INFO - Pre-warming the model...
2025-07-10 06:46:12,700 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:12,705 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:12,790 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:12,813 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:12,892 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:12,899 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:13,007 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:13,290 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:13,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:13,908 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:13,919 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:14,026 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:14,023 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:14,089 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:14,090 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:14,214 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:14,215 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:14,217 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:14,217 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:14,217 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:14,294 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:14,219 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0b53310c42d60b5f58603a71dcfa8a3e', 'date': 'Thu, 10 Jul 2025 06:46:13 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '17', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '10b5a8f1-66a0-4275-963f-e664ad398b45', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0b53310c42d60b5f58603a71dcfa8a3e', 'date': 'Thu, 10 Jul 2025 06:46:13 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '17', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '10b5a8f1-66a0-4275-963f-e664ad398b45', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:14,305 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0b53310c42d60b5f58603a71dcfa8a3e', 'date': 'Thu, 10 Jul 2025 06:46:13 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '17', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '10b5a8f1-66a0-4275-963f-e664ad398b45', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0b53310c42d60b5f58603a71dcfa8a3e', 'date': 'Thu, 10 Jul 2025 06:46:13 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '17', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '10b5a8f1-66a0-4275-963f-e664ad398b45', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:14,300 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9011a97a735d544c117644862ce4f940', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '552c2a16-8b03-4cb0-8ce9-25536522150d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9011a97a735d544c117644862ce4f940', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '552c2a16-8b03-4cb0-8ce9-25536522150d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:14,394 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9011a97a735d544c117644862ce4f940', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '552c2a16-8b03-4cb0-8ce9-25536522150d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9011a97a735d544c117644862ce4f940', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '552c2a16-8b03-4cb0-8ce9-25536522150d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:14,717 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:14,717 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:14,721 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0a2858f572357692b04ba054fce4d436', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8f264d3a-b5ab-45b5-b689-7968779122a4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0a2858f572357692b04ba054fce4d436', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8f264d3a-b5ab-45b5-b689-7968779122a4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:14,739 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0a2858f572357692b04ba054fce4d436', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8f264d3a-b5ab-45b5-b689-7968779122a4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0a2858f572357692b04ba054fce4d436', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8f264d3a-b5ab-45b5-b689-7968779122a4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:15,206 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:15,225 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:15,289 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:15,296 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:15,291 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '66344c31947a9c93dc2809d0be9b98e0', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '431a2ec9-929e-4f2d-92da-613c4348355e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '66344c31947a9c93dc2809d0be9b98e0', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '431a2ec9-929e-4f2d-92da-613c4348355e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:15,408 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '66344c31947a9c93dc2809d0be9b98e0', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '431a2ec9-929e-4f2d-92da-613c4348355e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '66344c31947a9c93dc2809d0be9b98e0', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '431a2ec9-929e-4f2d-92da-613c4348355e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:15,308 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5ce9db47659ff541c900d2c211c50045', 'date': 'Thu, 10 Jul 2025 06:46:15 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7a58e8a-295e-4fd7-904f-980bdecf3edf', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5ce9db47659ff541c900d2c211c50045', 'date': 'Thu, 10 Jul 2025 06:46:15 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7a58e8a-295e-4fd7-904f-980bdecf3edf', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:15,420 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5ce9db47659ff541c900d2c211c50045', 'date': 'Thu, 10 Jul 2025 06:46:15 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7a58e8a-295e-4fd7-904f-980bdecf3edf', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5ce9db47659ff541c900d2c211c50045', 'date': 'Thu, 10 Jul 2025 06:46:15 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7a58e8a-295e-4fd7-904f-980bdecf3edf', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:15,303 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '09f7807c74e3857c162e8be22d4e6d0d', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'f12f3b06-c61f-4e06-a6af-1c4334d9361b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '09f7807c74e3857c162e8be22d4e6d0d', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'f12f3b06-c61f-4e06-a6af-1c4334d9361b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:15,498 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '09f7807c74e3857c162e8be22d4e6d0d', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'f12f3b06-c61f-4e06-a6af-1c4334d9361b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '09f7807c74e3857c162e8be22d4e6d0d', 'date': 'Thu, 10 Jul 2025 06:46:14 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'f12f3b06-c61f-4e06-a6af-1c4334d9361b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:15,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:15,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:15,712 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:15,707 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:15,707 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:15,707 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:15,719 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:15,714 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:15,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:16,098 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:16,592 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:16,603 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8dc783d6b876439fe5bf92420a108f2b', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '85a59388-7180-4a26-9201-c5583757e688', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8dc783d6b876439fe5bf92420a108f2b', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '85a59388-7180-4a26-9201-c5583757e688', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:16,711 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8dc783d6b876439fe5bf92420a108f2b', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '85a59388-7180-4a26-9201-c5583757e688', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8dc783d6b876439fe5bf92420a108f2b', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '85a59388-7180-4a26-9201-c5583757e688', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:16,989 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:16,991 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:16,995 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:16,992 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:16,998 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:16,999 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:17,006 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:17,090 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:17,089 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'acafbc3d34b051403383adc1c94eb138', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53546cea-2d14-4750-95ce-bf33b9074395', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'acafbc3d34b051403383adc1c94eb138', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53546cea-2d14-4750-95ce-bf33b9074395', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,306 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'acafbc3d34b051403383adc1c94eb138', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53546cea-2d14-4750-95ce-bf33b9074395', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'acafbc3d34b051403383adc1c94eb138', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53546cea-2d14-4750-95ce-bf33b9074395', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,400 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:17,397 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:17,193 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '27db23a82b1da0e54441d20c7242f946', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '1cb9f4d8-34df-403c-81e6-35fbdca1e9e6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '27db23a82b1da0e54441d20c7242f946', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '1cb9f4d8-34df-403c-81e6-35fbdca1e9e6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,696 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '27db23a82b1da0e54441d20c7242f946', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '1cb9f4d8-34df-403c-81e6-35fbdca1e9e6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '27db23a82b1da0e54441d20c7242f946', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '1cb9f4d8-34df-403c-81e6-35fbdca1e9e6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,189 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '88be424a50452b45ffd2fb7b50d508b8', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd834a515-ad4c-4668-97cd-8f5efd226669', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '88be424a50452b45ffd2fb7b50d508b8', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd834a515-ad4c-4668-97cd-8f5efd226669', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,399 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a8cd19026416f0cda914a4db90981a85', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0737ecbc-5c27-4e13-a6be-ddeeb451a130', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a8cd19026416f0cda914a4db90981a85', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0737ecbc-5c27-4e13-a6be-ddeeb451a130', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,706 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '88be424a50452b45ffd2fb7b50d508b8', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd834a515-ad4c-4668-97cd-8f5efd226669', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '88be424a50452b45ffd2fb7b50d508b8', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd834a515-ad4c-4668-97cd-8f5efd226669', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,799 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a8cd19026416f0cda914a4db90981a85', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0737ecbc-5c27-4e13-a6be-ddeeb451a130', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a8cd19026416f0cda914a4db90981a85', 'date': 'Thu, 10 Jul 2025 06:46:16 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0737ecbc-5c27-4e13-a6be-ddeeb451a130', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,503 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1898fd90ac89b8dc4e2b7a0aa844565', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'dc059633-b0b5-40c7-99f5-11d477ee1c3f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1898fd90ac89b8dc4e2b7a0aa844565', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'dc059633-b0b5-40c7-99f5-11d477ee1c3f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,911 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1898fd90ac89b8dc4e2b7a0aa844565', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'dc059633-b0b5-40c7-99f5-11d477ee1c3f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1898fd90ac89b8dc4e2b7a0aa844565', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'dc059633-b0b5-40c7-99f5-11d477ee1c3f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:17,501 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8f78b5c0775b2b46ca86fb658bdff48f', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44bccfbe-7202-46e9-bca0-394d69522d88', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8f78b5c0775b2b46ca86fb658bdff48f', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44bccfbe-7202-46e9-bca0-394d69522d88', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:18,006 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8f78b5c0775b2b46ca86fb658bdff48f', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44bccfbe-7202-46e9-bca0-394d69522d88', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8f78b5c0775b2b46ca86fb658bdff48f', 'date': 'Thu, 10 Jul 2025 06:46:17 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44bccfbe-7202-46e9-bca0-394d69522d88', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:18,591 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:18,592 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:18,592 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:18,592 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:18,696 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '32ce1e6c853864fb201747c520c11c62', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '64ea6af1-f1bd-457b-8a9f-8ada95f2be31', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '32ce1e6c853864fb201747c520c11c62', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '64ea6af1-f1bd-457b-8a9f-8ada95f2be31', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:18,702 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7fb30a6476a18ee9790cb920aa15b4d5', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '3ef1d99e-2827-418e-8f93-47ab7e60a666', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7fb30a6476a18ee9790cb920aa15b4d5', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '3ef1d99e-2827-418e-8f93-47ab7e60a666', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:19,289 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7fb30a6476a18ee9790cb920aa15b4d5', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '3ef1d99e-2827-418e-8f93-47ab7e60a666', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7fb30a6476a18ee9790cb920aa15b4d5', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '3ef1d99e-2827-418e-8f93-47ab7e60a666', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:18,797 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '852b786025460a02cbe285047ff153ce', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45697197-5500-41e4-afe2-35c2e18ddae4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '852b786025460a02cbe285047ff153ce', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45697197-5500-41e4-afe2-35c2e18ddae4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:19,496 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '852b786025460a02cbe285047ff153ce', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45697197-5500-41e4-afe2-35c2e18ddae4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '852b786025460a02cbe285047ff153ce', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45697197-5500-41e4-afe2-35c2e18ddae4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:19,196 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '32ce1e6c853864fb201747c520c11c62', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '64ea6af1-f1bd-457b-8a9f-8ada95f2be31', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '32ce1e6c853864fb201747c520c11c62', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '64ea6af1-f1bd-457b-8a9f-8ada95f2be31', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:18,692 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fd1027eb8900ce1ffe7bcdc4041b7a44', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd6dfd376-e38c-49ac-8b3c-fe620d497221', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fd1027eb8900ce1ffe7bcdc4041b7a44', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd6dfd376-e38c-49ac-8b3c-fe620d497221', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:19,592 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fd1027eb8900ce1ffe7bcdc4041b7a44', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd6dfd376-e38c-49ac-8b3c-fe620d497221', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fd1027eb8900ce1ffe7bcdc4041b7a44', 'date': 'Thu, 10 Jul 2025 06:46:18 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd6dfd376-e38c-49ac-8b3c-fe620d497221', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:30,032 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:30,041 - root - INFO - Configuration validated successfully
2025-07-10 06:46:30,042 - root - INFO - Pre-warming the model...
2025-07-10 06:46:30,043 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:30,044 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:30,046 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:30,056 - root - INFO - Configuration validated successfully
2025-07-10 06:46:30,057 - root - INFO - Pre-warming the model...
2025-07-10 06:46:30,059 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:30,060 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:30,241 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:30,253 - root - INFO - Configuration validated successfully
2025-07-10 06:46:30,254 - root - INFO - Pre-warming the model...
2025-07-10 06:46:30,256 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:30,258 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:30,535 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:30,548 - root - INFO - Configuration validated successfully
2025-07-10 06:46:30,550 - root - INFO - Pre-warming the model...
2025-07-10 06:46:30,552 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:30,554 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:30,925 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:30,941 - root - INFO - Configuration validated successfully
2025-07-10 06:46:31,024 - root - INFO - Pre-warming the model...
2025-07-10 06:46:31,026 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:31,031 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:31,129 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:31,142 - root - INFO - Configuration validated successfully
2025-07-10 06:46:31,227 - root - INFO - Pre-warming the model...
2025-07-10 06:46:31,229 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:31,236 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:31,331 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:31,426 - root - INFO - Configuration validated successfully
2025-07-10 06:46:31,429 - root - INFO - Pre-warming the model...
2025-07-10 06:46:31,432 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:31,435 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:31,625 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:31,821 - root - INFO - Configuration validated successfully
2025-07-10 06:46:31,830 - root - INFO - Pre-warming the model...
2025-07-10 06:46:31,834 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:31,837 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:31,921 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:31,924 - root - INFO - Configuration validated successfully
2025-07-10 06:46:31,931 - root - INFO - Pre-warming the model...
2025-07-10 06:46:32,024 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:32,031 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:32,125 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:32,141 - root - INFO - Configuration validated successfully
2025-07-10 06:46:32,221 - root - INFO - Pre-warming the model...
2025-07-10 06:46:32,229 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:32,232 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:32,234 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:32,424 - root - INFO - Configuration validated successfully
2025-07-10 06:46:32,431 - root - INFO - Pre-warming the model...
2025-07-10 06:46:32,434 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:32,436 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:33,038 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:33,126 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:33,332 - root - INFO - Configuration validated successfully
2025-07-10 06:46:33,421 - root - INFO - Pre-warming the model...
2025-07-10 06:46:33,433 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:33,441 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:33,436 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:33,522 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:33,723 - root - INFO - Configuration validated successfully
2025-07-10 06:46:33,731 - root - INFO - Pre-warming the model...
2025-07-10 06:46:33,730 - root - INFO - Configuration validated successfully
2025-07-10 06:46:33,821 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:33,834 - root - INFO - Pre-warming the model...
2025-07-10 06:46:33,921 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:33,836 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:33,836 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:33,927 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:34,224 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:34,230 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:34,325 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:34,422 - root - INFO - Configuration validated successfully
2025-07-10 06:46:34,424 - root - INFO - Configuration validated successfully
2025-07-10 06:46:34,426 - root - INFO - Pre-warming the model...
2025-07-10 06:46:34,430 - root - INFO - Pre-warming the model...
2025-07-10 06:46:34,434 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:34,436 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:34,444 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:34,438 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:34,622 - root - INFO - Starting HR Assistant...
2025-07-10 06:46:34,625 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:34,625 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:34,637 - root - INFO - Configuration validated successfully
2025-07-10 06:46:34,723 - root - INFO - Pre-warming the model...
2025-07-10 06:46:34,725 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:46:34,726 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:46:35,520 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:35,521 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:35,522 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:35,524 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:35,917 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:35,918 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:35,924 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:35,935 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:35,923 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '569916f0e27ca0d6e24b74d6e119b42a', 'date': 'Thu, 10 Jul 2025 06:46:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a786d140-23f9-4a6d-92d2-b11a82757918', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '569916f0e27ca0d6e24b74d6e119b42a', 'date': 'Thu, 10 Jul 2025 06:46:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a786d140-23f9-4a6d-92d2-b11a82757918', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:36,035 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '569916f0e27ca0d6e24b74d6e119b42a', 'date': 'Thu, 10 Jul 2025 06:46:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a786d140-23f9-4a6d-92d2-b11a82757918', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '569916f0e27ca0d6e24b74d6e119b42a', 'date': 'Thu, 10 Jul 2025 06:46:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a786d140-23f9-4a6d-92d2-b11a82757918', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:36,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:36,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:36,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:36,633 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:37,000 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:37,002 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:37,004 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:37,005 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:37,005 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:37,006 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:37,030 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:37,037 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:37,051 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:37,052 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:37,054 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:37,056 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:37,057 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:37,062 - root - INFO - Loading groq LLM...
2025-07-10 06:46:37,063 - root - INFO - Loading groq LLM...
2025-07-10 06:46:37,117 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:37,119 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:37,120 - root - INFO - Loading groq LLM...
2025-07-10 06:46:37,433 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:37,520 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:37,521 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:37,616 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:37,622 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:37,624 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:37,625 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:37,626 - root - INFO - Loading groq LLM...
2025-07-10 06:46:37,636 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:37,638 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:37,640 - root - INFO - Loading groq LLM...
2025-07-10 06:46:37,740 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:37,817 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:37,819 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:37,826 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:37,819 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:37,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:46:38,332 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:38,332 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:38,337 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:38,340 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:38,423 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:38,521 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.002591609954834, 'init_vectorstore': 5.726988315582275, 'setup_retriever': 0.0007836818695068359, 'load_llm': 1.3655707836151123, 'create_chains': 0.09942746162414551}
2025-07-10 06:46:38,524 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.4663197994232178, 'init_vectorstore': 6.496852159500122, 'setup_retriever': 0.0018413066864013672, 'load_llm': 1.3983075618743896, 'create_chains': 0.09500861167907715}
2025-07-10 06:46:38,530 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.21s
2025-07-10 06:46:38,531 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.47s
2025-07-10 06:46:38,621 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:38,628 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:38,723 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2062208652496338, 'init_vectorstore': 5.2089080810546875, 'setup_retriever': 0.0003681182861328125, 'load_llm': 1.582059383392334, 'create_chains': 0.1033172607421875}
2025-07-10 06:46:38,724 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:38,726 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:38,729 - root - INFO - Loading groq LLM...
2025-07-10 06:46:38,729 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:38,724 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.17s
2025-07-10 06:46:38,733 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:38,735 - root - INFO - Loading groq LLM...
2025-07-10 06:46:39,381 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,382 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,382 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,382 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,417 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,420 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:39,420 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,420 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:39,421 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:39,427 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2995812892913818, 'init_vectorstore': 4.99375581741333, 'setup_retriever': 0.0006258487701416016, 'load_llm': 1.7951512336730957, 'create_chains': 0.010188817977905273}
2025-07-10 06:46:39,532 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.21s
2025-07-10 06:46:39,519 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.3010072708129883, 'init_vectorstore': 5.121973276138306, 'setup_retriever': 0.07074880599975586, 'load_llm': 1.8023383617401123, 'create_chains': 0.10017561912536621}
2025-07-10 06:46:39,536 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.50s
2025-07-10 06:46:39,632 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:39,632 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:39,640 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:39,641 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:39,643 - root - INFO - Loading groq LLM...
2025-07-10 06:46:39,644 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:39,648 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:39,650 - root - INFO - Loading groq LLM...
2025-07-10 06:46:39,804 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:39,819 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:39,825 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:39,830 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:39,844 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:39,925 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:39,934 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:40,019 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:40,021 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:40,022 - root - INFO - Loading groq LLM...
2025-07-10 06:46:40,026 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:40,027 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:40,031 - root - INFO - Loading groq LLM...
2025-07-10 06:46:40,045 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:40,124 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:40,232 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.5041131973266602, 'init_vectorstore': 4.994518756866455, 'setup_retriever': 0.0006139278411865234, 'load_llm': 1.5992169380187988, 'create_chains': 0.010808944702148438}
2025-07-10 06:46:40,236 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.20s
2025-07-10 06:46:40,337 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.4994871616363525, 'init_vectorstore': 5.600442409515381, 'setup_retriever': 0.0012202262878417969, 'load_llm': 1.694835901260376, 'create_chains': 0.013360261917114258}
2025-07-10 06:46:40,339 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.90s
2025-07-10 06:46:40,505 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,517 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,517 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,517 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,517 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,518 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,519 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,519 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,519 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,522 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:40,725 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:40,727 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:40,786 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:40,820 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:40,821 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:40,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:40,824 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:40,826 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:40,826 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:40,831 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:40,838 - root - INFO - Loading groq LLM...
2025-07-10 06:46:40,839 - root - INFO - Loading groq LLM...
2025-07-10 06:46:40,841 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:46:40,917 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:40,918 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:40,925 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:40,926 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:40,927 - root - INFO - Loading groq LLM...
2025-07-10 06:46:40,929 - root - INFO - Loading groq LLM...
2025-07-10 06:46:40,937 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:46:40,939 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:46:41,023 - root - INFO - Loading groq LLM...
2025-07-10 06:46:41,031 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:41,031 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:41,031 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:41,117 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:41,118 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:41,224 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:41,226 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:42,018 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:42,028 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:42,329 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:42,330 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:42,521 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.1017258167266846, 'init_vectorstore': 4.998150587081909, 'setup_retriever': 0.0004112720489501953, 'load_llm': 2.5989832878112793, 'create_chains': 0.2898852825164795}
2025-07-10 06:46:42,528 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.00s
2025-07-10 06:46:42,722 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2956790924072266, 'init_vectorstore': 6.405391693115234, 'setup_retriever': 0.0008361339569091797, 'load_llm': 2.990861177444458, 'create_chains': 0.09884428977966309}
2025-07-10 06:46:42,735 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.81s
2025-07-10 06:46:43,030 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.0112805366516113, 'init_vectorstore': 5.588433504104614, 'setup_retriever': 0.0005645751953125, 'load_llm': 2.99503231048584, 'create_chains': 0.1007692813873291}
2025-07-10 06:46:43,032 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.71s
2025-07-10 06:46:43,127 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:43,221 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:43,424 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.70808744430542, 'init_vectorstore': 4.6869056224823, 'setup_retriever': 0.00046181678771972656, 'load_llm': 3.410428524017334, 'create_chains': 0.08876562118530273}
2025-07-10 06:46:43,429 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.91s
2025-07-10 06:46:43,424 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:43,432 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:43,431 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:43,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:43,734 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:43,921 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:44,018 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:44,026 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:44,124 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:44,128 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:44,160 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:44,219 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:44,226 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:44,231 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:44,230 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:44,230 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:44,443 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.6939685344696045, 'init_vectorstore': 4.998850107192993, 'setup_retriever': 0.0009081363677978516, 'load_llm': 3.6079132556915283, 'create_chains': 0.011258840560913086}
2025-07-10 06:46:44,517 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:44,517 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.48s
2025-07-10 06:46:44,517 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2069518566131592, 'init_vectorstore': 4.991938591003418, 'setup_retriever': 0.0007312297821044922, 'load_llm': 3.705329179763794, 'create_chains': 0.009531021118164062}
2025-07-10 06:46:44,520 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.0776629447937012, 'init_vectorstore': 5.006540298461914, 'setup_retriever': 0.0008854866027832031, 'load_llm': 3.607478141784668, 'create_chains': 0.08653402328491211}
2025-07-10 06:46:44,524 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.80s
2025-07-10 06:46:44,521 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.00s
2025-07-10 06:46:44,529 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:44,530 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:44,621 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:44,623 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:44,641 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.5906651020050049, 'init_vectorstore': 5.222309827804565, 'setup_retriever': 0.001184701919555664, 'load_llm': 3.7825965881347656, 'create_chains': 0.017667055130004883}
2025-07-10 06:46:44,716 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.70s
2025-07-10 06:46:44,750 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2118144035339355, 'init_vectorstore': 4.9828877449035645, 'setup_retriever': 0.0004203319549560547, 'load_llm': 4.0020904541015625, 'create_chains': 0.022794485092163086}
2025-07-10 06:46:44,753 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.23s
2025-07-10 06:46:44,827 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:44,828 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:44,831 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:44,908 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:44,918 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:44,942 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:44,944 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:44,945 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:44,946 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:45,346 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:45,347 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:45,349 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:45,608 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:45,609 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:45,609 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:45,610 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:46:45,611 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:45,657 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:45,659 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:46,076 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:46,082 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:46:46,084 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:46,085 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:46,089 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:46,091 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:46:46,402 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:46,403 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:46,403 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:46,403 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:46,404 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:46,404 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:46:46,409 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'eec8ab8f983d2b815b202ff9ebbd4aee', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '03b80155-2671-45df-8086-683c6f1e2d92', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'eec8ab8f983d2b815b202ff9ebbd4aee', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '03b80155-2671-45df-8086-683c6f1e2d92', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:46,409 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8a29be8bf9de1e6f0eb542014ae3e2c4', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2b6b7f3c-e5b1-4fea-a978-2b32b1c26a11', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8a29be8bf9de1e6f0eb542014ae3e2c4', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2b6b7f3c-e5b1-4fea-a978-2b32b1c26a11', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:46,407 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4cae13982e403096c01df5f6461e87b6', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'ffcbbfce-964b-4fc3-92f2-534c0f448217', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4cae13982e403096c01df5f6461e87b6', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'ffcbbfce-964b-4fc3-92f2-534c0f448217', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:46,410 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ca4cf60a5cbc8ed6b78c6e6a18b592f1', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '089cc737-7a2e-412b-84dc-737ea509e1e4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ca4cf60a5cbc8ed6b78c6e6a18b592f1', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '089cc737-7a2e-412b-84dc-737ea509e1e4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:46,518 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:46,520 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:46,536 - root - INFO - Model pre-warmed successfully
2025-07-10 06:46:46,539 - root - INFO - HR Assistant started successfully
2025-07-10 06:46:47,348 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:47,348 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:46:47,353 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f88cb5a964b517ffbaea0c1129b2fbc5', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0cf57c58-1713-413d-a1a7-8c4ebdba66c9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f88cb5a964b517ffbaea0c1129b2fbc5', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0cf57c58-1713-413d-a1a7-8c4ebdba66c9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:46:47,353 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '13e0e09b1700c521ab3ecbf3fef64bd6', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'f6616fe5-abc4-4810-997c-6886636fcc98', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '13e0e09b1700c521ab3ecbf3fef64bd6', 'date': 'Thu, 10 Jul 2025 06:46:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'f6616fe5-abc4-4810-997c-6886636fcc98', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:38,671 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:38,712 - root - INFO - Configuration validated successfully
2025-07-10 06:55:38,713 - root - INFO - Pre-warming the model...
2025-07-10 06:55:38,714 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:38,715 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:38,717 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:38,725 - root - INFO - Configuration validated successfully
2025-07-10 06:55:38,727 - root - INFO - Pre-warming the model...
2025-07-10 06:55:38,728 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:38,729 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:38,836 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:38,846 - root - INFO - Configuration validated successfully
2025-07-10 06:55:38,909 - root - INFO - Pre-warming the model...
2025-07-10 06:55:38,911 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:38,912 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:39,117 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:39,128 - root - INFO - Configuration validated successfully
2025-07-10 06:55:39,130 - root - INFO - Pre-warming the model...
2025-07-10 06:55:39,131 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:39,133 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:39,323 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:39,416 - root - INFO - Configuration validated successfully
2025-07-10 06:55:39,418 - root - INFO - Pre-warming the model...
2025-07-10 06:55:39,420 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:39,421 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:39,615 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:39,715 - root - INFO - Configuration validated successfully
2025-07-10 06:55:39,720 - root - INFO - Pre-warming the model...
2025-07-10 06:55:39,722 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:39,724 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:40,018 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:40,123 - root - INFO - Configuration validated successfully
2025-07-10 06:55:40,124 - root - INFO - Pre-warming the model...
2025-07-10 06:55:40,210 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:40,213 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:40,520 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:40,713 - root - INFO - Configuration validated successfully
2025-07-10 06:55:40,812 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:40,812 - root - INFO - Pre-warming the model...
2025-07-10 06:55:40,816 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:40,818 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:40,913 - root - INFO - Configuration validated successfully
2025-07-10 06:55:40,925 - root - INFO - Pre-warming the model...
2025-07-10 06:55:40,927 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:40,929 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:41,015 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:41,116 - root - INFO - Configuration validated successfully
2025-07-10 06:55:41,120 - root - INFO - Pre-warming the model...
2025-07-10 06:55:41,124 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:41,209 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:41,610 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:41,810 - root - INFO - Configuration validated successfully
2025-07-10 06:55:41,820 - root - INFO - Pre-warming the model...
2025-07-10 06:55:41,819 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:41,826 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:42,009 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:41,921 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:42,011 - root - INFO - Configuration validated successfully
2025-07-10 06:55:42,109 - root - INFO - Pre-warming the model...
2025-07-10 06:55:42,117 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:42,120 - root - INFO - Configuration validated successfully
2025-07-10 06:55:42,209 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:42,217 - root - INFO - Pre-warming the model...
2025-07-10 06:55:42,221 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:42,310 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:42,316 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:42,416 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:42,615 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:42,826 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:42,918 - root - INFO - Configuration validated successfully
2025-07-10 06:55:42,920 - root - INFO - Pre-warming the model...
2025-07-10 06:55:42,921 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:42,924 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:43,121 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:43,214 - root - INFO - Configuration validated successfully
2025-07-10 06:55:43,220 - root - INFO - Pre-warming the model...
2025-07-10 06:55:43,227 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:43,320 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:43,416 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:43,430 - root - INFO - Configuration validated successfully
2025-07-10 06:55:43,509 - root - INFO - Pre-warming the model...
2025-07-10 06:55:43,517 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:43,519 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:43,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:43,726 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:43,730 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:43,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:43,928 - root - INFO - Starting HR Assistant...
2025-07-10 06:55:44,116 - root - INFO - Configuration validated successfully
2025-07-10 06:55:44,117 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:44,121 - root - INFO - Pre-warming the model...
2025-07-10 06:55:44,219 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:55:44,221 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:55:44,416 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:44,631 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:44,949 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:45,023 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:45,897 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:45,897 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:45,897 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:45,899 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:45,899 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,208 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,208 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,211 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,214 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,514 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,515 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,525 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:55:46,844 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:46,845 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:46,846 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,098 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,098 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,098 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,098 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,628 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,630 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:47,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:47,630 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:47,631 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:47,633 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:47,633 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:47,633 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:47,633 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:47,723 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:47,726 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:47,729 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:47,809 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:47,733 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:47,811 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:47,811 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:47,811 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:47,812 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:47,815 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:47,816 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:47,816 - root - INFO - Loading groq LLM...
2025-07-10 06:55:47,817 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:47,818 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:47,821 - root - INFO - Loading groq LLM...
2025-07-10 06:55:47,821 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:47,821 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:47,821 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:47,822 - root - INFO - Loading groq LLM...
2025-07-10 06:55:47,823 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:47,824 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:47,826 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:47,827 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:47,828 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:47,830 - root - INFO - Loading groq LLM...
2025-07-10 06:55:47,834 - root - INFO - Loading groq LLM...
2025-07-10 06:55:47,831 - root - INFO - Loading groq LLM...
2025-07-10 06:55:47,833 - root - INFO - Loading groq LLM...
2025-07-10 06:55:48,338 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,339 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,345 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,410 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,411 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,412 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,414 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,611 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:48,615 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:48,712 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:48,713 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:48,713 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:48,720 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:48,724 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:48,813 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:48,814 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:48,818 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:48,815 - root - INFO - Loading groq LLM...
2025-07-10 06:55:48,820 - root - INFO - Loading groq LLM...
2025-07-10 06:55:48,817 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:48,824 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:48,828 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:48,817 - root - INFO - Loading groq LLM...
2025-07-10 06:55:48,827 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:48,923 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:48,835 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:48,909 - root - INFO - Loading groq LLM...
2025-07-10 06:55:48,910 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,911 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:48,913 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:49,013 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:49,115 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:49,013 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:49,016 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:49,121 - root - INFO - Loading groq LLM...
2025-07-10 06:55:49,117 - root - INFO - Loading groq LLM...
2025-07-10 06:55:49,118 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:49,209 - root - INFO - Loading groq LLM...
2025-07-10 06:55:49,229 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:49,212 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:49,321 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:55:49,413 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:49,512 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:49,414 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:49,415 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:55:49,521 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:49,516 - root - INFO - Loading groq LLM...
2025-07-10 06:55:49,609 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:55:49,718 - root - INFO - Loading groq LLM...
2025-07-10 06:55:49,617 - root - INFO - Loading groq LLM...
2025-07-10 06:55:51,516 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.001816749572754, 'init_vectorstore': 5.5951173305511475, 'setup_retriever': 0.00038886070251464844, 'load_llm': 2.7970259189605713, 'create_chains': 0.10398149490356445}
2025-07-10 06:55:51,519 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.0014567375183105, 'init_vectorstore': 5.693459510803223, 'setup_retriever': 0.0008630752563476562, 'load_llm': 2.900871515274048, 'create_chains': 0.0073506832122802734}
2025-07-10 06:55:51,616 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.80s
2025-07-10 06:55:51,612 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.60s
2025-07-10 06:55:51,724 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.2966587543487549, 'init_vectorstore': 7.084120988845825, 'setup_retriever': 0.0003094673156738281, 'load_llm': 3.902174234390259, 'create_chains': 0.010569572448730469}
2025-07-10 06:55:51,626 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.606415033340454, 'init_vectorstore': 5.693691730499268, 'setup_retriever': 0.00029158592224121094, 'load_llm': 2.8974854946136475, 'create_chains': 0.014990806579589844}
2025-07-10 06:55:51,816 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 12.39s
2025-07-10 06:55:51,817 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.60s
2025-07-10 06:55:51,819 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.5819599628448486, 'init_vectorstore': 6.411731958389282, 'setup_retriever': 0.0007450580596923828, 'load_llm': 3.997770071029663, 'create_chains': 0.0947422981262207}
2025-07-10 06:55:51,821 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 12.10s
2025-07-10 06:55:51,914 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.3981664180755615, 'init_vectorstore': 7.1988866329193115, 'setup_retriever': 0.0006337165832519531, 'load_llm': 3.9118385314941406, 'create_chains': 0.1906437873840332}
2025-07-10 06:55:52,009 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 12.87s
2025-07-10 06:55:52,019 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.8047716617584229, 'init_vectorstore': 4.991925239562988, 'setup_retriever': 0.0004401206970214844, 'load_llm': 3.1930298805236816, 'create_chains': 0.009430885314941406}
2025-07-10 06:55:52,011 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.3956151008605957, 'init_vectorstore': 4.402724266052246, 'setup_retriever': 0.0004839897155761719, 'load_llm': 3.0045385360717773, 'create_chains': 0.19414615631103516}
2025-07-10 06:55:52,110 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.10s
2025-07-10 06:55:52,123 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.20s
2025-07-10 06:55:52,126 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.6912181377410889, 'init_vectorstore': 8.393242120742798, 'setup_retriever': 0.0004820823669433594, 'load_llm': 4.213154077529907, 'create_chains': 0.10174298286437988}
2025-07-10 06:55:52,210 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 13.49s
2025-07-10 06:55:52,420 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.7008247375488281, 'init_vectorstore': 4.809580087661743, 'setup_retriever': 0.0009179115295410156, 'load_llm': 3.3879644870758057, 'create_chains': 0.10878920555114746}
2025-07-10 06:55:52,426 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.10s
2025-07-10 06:55:52,514 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.383788824081421, 'init_vectorstore': 4.315099477767944, 'setup_retriever': 0.0005249977111816406, 'load_llm': 3.1800289154052734, 'create_chains': 0.1048135757446289}
2025-07-10 06:55:52,519 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.00s
2025-07-10 06:55:52,617 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.7959482669830322, 'init_vectorstore': 8.014975309371948, 'setup_retriever': 0.0008101463317871094, 'load_llm': 4.785760879516602, 'create_chains': 0.10240650177001953}
2025-07-10 06:55:52,617 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.6011183261871338, 'init_vectorstore': 4.998858213424683, 'setup_retriever': 0.0007033348083496094, 'load_llm': 3.6917312145233154, 'create_chains': 0.09806942939758301}
2025-07-10 06:55:52,622 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 13.71s
2025-07-10 06:55:52,628 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.41s
2025-07-10 06:55:52,712 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.6045827865600586, 'init_vectorstore': 5.8132805824279785, 'setup_retriever': 0.07857155799865723, 'load_llm': 4.804448366165161, 'create_chains': 0.09817957878112793}
2025-07-10 06:55:52,713 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.7789402008056641, 'init_vectorstore': 8.204625606536865, 'setup_retriever': 0.0007777214050292969, 'load_llm': 4.893720865249634, 'create_chains': 0.09636116027832031}
2025-07-10 06:55:52,717 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 12.50s
2025-07-10 06:55:52,720 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 13.99s
2025-07-10 06:55:52,948 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.5028650760650635, 'init_vectorstore': 4.383082389831543, 'setup_retriever': 0.010245561599731445, 'load_llm': 3.5981333255767822, 'create_chains': 0.028523921966552734}
2025-07-10 06:55:52,950 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.63s
2025-07-10 06:55:53,121 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.1067345142364502, 'init_vectorstore': 3.789128541946411, 'setup_retriever': 0.00037550926208496094, 'load_llm': 3.901705026626587, 'create_chains': 0.0076024532318115234}
2025-07-10 06:55:53,123 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.81s
2025-07-10 06:55:53,684 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,687 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,685 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,685 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,686 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,933 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,934 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,935 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:53,936 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,181 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,181 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,181 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,182 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,182 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,182 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,182 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,182 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:55:54,530 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:54,530 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:54,534 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:54,535 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:54,536 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,177 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,181 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,182 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,185 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,189 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,189 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,194 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,194 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,196 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,197 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,212 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,213 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,220 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,221 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,222 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,310 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,225 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,744 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:55:55,744 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,744 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,744 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,744 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,744 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:55:55,745 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:55:55,744 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,745 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,745 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,745 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,745 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:55:55,746 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:55:55,746 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,746 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,746 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:55:55,747 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:55:55,813 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ec75aa787bde8a4bc14c541fb1d852b0', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b55fc3b5-8edb-46f7-b6d0-89ae360c24ea', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ec75aa787bde8a4bc14c541fb1d852b0', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b55fc3b5-8edb-46f7-b6d0-89ae360c24ea', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:56,013 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1ca2de88c1c49c708857b24a585c6e9f', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e35d087b-c649-4c92-8812-c6ba197db8ac', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1ca2de88c1c49c708857b24a585c6e9f', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e35d087b-c649-4c92-8812-c6ba197db8ac', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,760 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b35cf79724ae7e275bffc80828565e46', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '646913a4-590e-426c-bdd1-bb65d87dc643', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b35cf79724ae7e275bffc80828565e46', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '646913a4-590e-426c-bdd1-bb65d87dc643', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,911 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a4128345026768c62c5aa478acff23d6', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '7c0b4cdf-130d-47b5-894c-bf932e3d120f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a4128345026768c62c5aa478acff23d6', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '7c0b4cdf-130d-47b5-894c-bf932e3d120f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,750 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '403c6396c1c8173a6350969965efa32c', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c5b0521d-4311-4f96-9e8c-17326b156365', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '403c6396c1c8173a6350969965efa32c', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c5b0521d-4311-4f96-9e8c-17326b156365', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,910 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0472674d4dec4b6f8511d678582071a4', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '18', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '371501f4-7f46-4547-bc16-b9a3cdb65838', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0472674d4dec4b6f8511d678582071a4', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '18', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '371501f4-7f46-4547-bc16-b9a3cdb65838', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,758 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1e0bd3441ab16f1684dbb99c0e83f420', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8ac4b7b0-aaf9-4488-9295-5af61ca59542', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1e0bd3441ab16f1684dbb99c0e83f420', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8ac4b7b0-aaf9-4488-9295-5af61ca59542', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,752 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'cd99142599d177cf55b7f5a5d81083d9', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '82c22643-98ab-44c4-ac99-e00654fab08b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'cd99142599d177cf55b7f5a5d81083d9', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '82c22643-98ab-44c4-ac99-e00654fab08b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:56,311 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:55:55,751 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '03252a16bdd27f8ed39e784e47c53408', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5d4da4a6-18af-4082-8b77-7ad1726e2949', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '03252a16bdd27f8ed39e784e47c53408', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5d4da4a6-18af-4082-8b77-7ad1726e2949', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,754 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '314a36b81ececcecb430b97f52cf2772', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '1dc9f4e9-1e63-4ffe-9901-dd15f4c621a2', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '314a36b81ececcecb430b97f52cf2772', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '1dc9f4e9-1e63-4ffe-9901-dd15f4c621a2', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:55,756 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8895c01ea1664428b3c7287174078125', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '81ce8cda-8a1f-4198-86de-bc6ab0215429', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '8895c01ea1664428b3c7287174078125', 'date': 'Thu, 10 Jul 2025 06:55:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '81ce8cda-8a1f-4198-86de-bc6ab0215429', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:55:58,611 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:55:58,615 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:55:58,717 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:55:58,726 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:55:58,817 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:55:58,816 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:55:59,412 - root - INFO - Model pre-warmed successfully
2025-07-10 06:55:59,414 - root - INFO - HR Assistant started successfully
2025-07-10 06:55:59,510 - root - INFO - Model pre-warmed successfully
2025-07-10 06:55:59,513 - root - INFO - HR Assistant started successfully
2025-07-10 06:56:00,212 - root - INFO - Model pre-warmed successfully
2025-07-10 06:56:00,213 - root - INFO - HR Assistant started successfully
2025-07-10 06:56:00,321 - root - INFO - Model pre-warmed successfully
2025-07-10 06:56:00,422 - root - INFO - HR Assistant started successfully
2025-07-10 06:56:00,510 - root - INFO - Model pre-warmed successfully
2025-07-10 06:56:00,512 - root - INFO - HR Assistant started successfully
2025-07-10 06:56:00,616 - root - INFO - Model pre-warmed successfully
2025-07-10 06:56:00,625 - root - INFO - HR Assistant started successfully
2025-07-10 06:56:34,629 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:34,635 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:34,638 - root - INFO - Configuration validated successfully
2025-07-10 06:56:34,640 - root - INFO - Pre-warming the model...
2025-07-10 06:56:34,641 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:34,642 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:34,645 - root - INFO - Configuration validated successfully
2025-07-10 06:56:34,646 - root - INFO - Pre-warming the model...
2025-07-10 06:56:34,652 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:34,654 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:34,717 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:34,728 - root - INFO - Configuration validated successfully
2025-07-10 06:56:34,730 - root - INFO - Pre-warming the model...
2025-07-10 06:56:34,795 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:34,797 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:35,313 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:35,415 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:35,492 - root - INFO - Configuration validated successfully
2025-07-10 06:56:35,506 - root - INFO - Pre-warming the model...
2025-07-10 06:56:35,507 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:35,509 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:35,497 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:35,589 - root - INFO - Configuration validated successfully
2025-07-10 06:56:35,690 - root - INFO - Configuration validated successfully
2025-07-10 06:56:35,788 - root - INFO - Pre-warming the model...
2025-07-10 06:56:35,805 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:35,806 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:35,792 - root - INFO - Pre-warming the model...
2025-07-10 06:56:35,896 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:35,900 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:35,904 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:36,004 - root - INFO - Configuration validated successfully
2025-07-10 06:56:36,088 - root - INFO - Pre-warming the model...
2025-07-10 06:56:36,098 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:36,102 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:36,190 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:36,206 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:36,389 - root - INFO - Configuration validated successfully
2025-07-10 06:56:36,489 - root - INFO - Configuration validated successfully
2025-07-10 06:56:36,494 - root - INFO - Pre-warming the model...
2025-07-10 06:56:36,588 - root - INFO - Pre-warming the model...
2025-07-10 06:56:36,608 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:36,589 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:36,604 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:36,596 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:36,701 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:36,993 - root - INFO - Configuration validated successfully
2025-07-10 06:56:36,789 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:37,094 - root - INFO - Pre-warming the model...
2025-07-10 06:56:37,101 - root - INFO - Configuration validated successfully
2025-07-10 06:56:37,292 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:37,200 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:37,488 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:37,298 - root - INFO - Pre-warming the model...
2025-07-10 06:56:37,600 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:37,390 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:37,399 - root - INFO - Configuration validated successfully
2025-07-10 06:56:37,788 - root - INFO - Pre-warming the model...
2025-07-10 06:56:37,798 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:37,605 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:37,799 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:37,801 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:37,802 - root - INFO - Configuration validated successfully
2025-07-10 06:56:37,902 - root - INFO - Pre-warming the model...
2025-07-10 06:56:37,994 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:37,994 - root - INFO - Configuration validated successfully
2025-07-10 06:56:38,088 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:38,094 - root - INFO - Pre-warming the model...
2025-07-10 06:56:38,107 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:38,194 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:38,609 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:38,697 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:38,697 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:38,791 - root - INFO - Configuration validated successfully
2025-07-10 06:56:38,901 - root - INFO - Pre-warming the model...
2025-07-10 06:56:38,988 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:38,999 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:38,996 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:39,104 - root - INFO - Configuration validated successfully
2025-07-10 06:56:39,295 - root - INFO - Pre-warming the model...
2025-07-10 06:56:39,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:39,304 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:39,399 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:39,899 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:56:40,203 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:56:40,206 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:40,493 - root - INFO - Configuration validated successfully
2025-07-10 06:56:40,497 - root - INFO - Pre-warming the model...
2025-07-10 06:56:40,503 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:40,506 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:40,690 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:41,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:41,108 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:56:41,590 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:41,592 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:41,925 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:56:41,926 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:56:41,926 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:56:41,989 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0f520c88698a9ad77c1b8336140cba64', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53107a4f-6f51-4017-826f-8d4ce1f24bff', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0f520c88698a9ad77c1b8336140cba64', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53107a4f-6f51-4017-826f-8d4ce1f24bff', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:56:41,991 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '497752b59ab371810f8bb89415a4013e', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '31a6aa24-106c-4494-9e43-44a107beb937', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '497752b59ab371810f8bb89415a4013e', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '31a6aa24-106c-4494-9e43-44a107beb937', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:56:41,990 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dccefe87754fd880fa29eb875c3740b5', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0a53b1a8-7e53-4f50-b9be-d71bcf26dbec', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dccefe87754fd880fa29eb875c3740b5', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0a53b1a8-7e53-4f50-b9be-d71bcf26dbec', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:56:42,013 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0f520c88698a9ad77c1b8336140cba64', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53107a4f-6f51-4017-826f-8d4ce1f24bff', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0f520c88698a9ad77c1b8336140cba64', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '53107a4f-6f51-4017-826f-8d4ce1f24bff', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:56:42,018 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '497752b59ab371810f8bb89415a4013e', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '31a6aa24-106c-4494-9e43-44a107beb937', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '497752b59ab371810f8bb89415a4013e', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '31a6aa24-106c-4494-9e43-44a107beb937', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:56:42,019 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dccefe87754fd880fa29eb875c3740b5', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0a53b1a8-7e53-4f50-b9be-d71bcf26dbec', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dccefe87754fd880fa29eb875c3740b5', 'date': 'Thu, 10 Jul 2025 06:56:41 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0a53b1a8-7e53-4f50-b9be-d71bcf26dbec', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:56:56,377 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:56,384 - root - INFO - Configuration validated successfully
2025-07-10 06:56:56,385 - root - INFO - Pre-warming the model...
2025-07-10 06:56:56,386 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:56,387 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:56,453 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:56,461 - root - INFO - Configuration validated successfully
2025-07-10 06:56:56,462 - root - INFO - Pre-warming the model...
2025-07-10 06:56:56,463 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:56,464 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:56,639 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:56,649 - root - INFO - Configuration validated successfully
2025-07-10 06:56:56,650 - root - INFO - Pre-warming the model...
2025-07-10 06:56:56,652 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:56,654 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:57,036 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:57,127 - root - INFO - Configuration validated successfully
2025-07-10 06:56:57,129 - root - INFO - Pre-warming the model...
2025-07-10 06:56:57,131 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:57,133 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:57,327 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:57,340 - root - INFO - Configuration validated successfully
2025-07-10 06:56:57,419 - root - INFO - Pre-warming the model...
2025-07-10 06:56:57,421 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:57,422 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:57,636 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:57,825 - root - INFO - Configuration validated successfully
2025-07-10 06:56:57,827 - root - INFO - Pre-warming the model...
2025-07-10 06:56:57,832 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:57,932 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:57,933 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:58,041 - root - INFO - Configuration validated successfully
2025-07-10 06:56:58,120 - root - INFO - Pre-warming the model...
2025-07-10 06:56:58,134 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:58,136 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:58,433 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:58,627 - root - INFO - Configuration validated successfully
2025-07-10 06:56:58,634 - root - INFO - Pre-warming the model...
2025-07-10 06:56:58,635 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:58,628 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:58,719 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:58,922 - root - INFO - Configuration validated successfully
2025-07-10 06:56:58,926 - root - INFO - Pre-warming the model...
2025-07-10 06:56:58,938 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:59,020 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:59,027 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:59,128 - root - INFO - Configuration validated successfully
2025-07-10 06:56:59,220 - root - INFO - Pre-warming the model...
2025-07-10 06:56:59,225 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:59,227 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:59,426 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:56:59,427 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:59,633 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:59,630 - root - INFO - Starting HR Assistant...
2025-07-10 06:56:59,721 - root - INFO - Configuration validated successfully
2025-07-10 06:56:59,829 - root - INFO - Pre-warming the model...
2025-07-10 06:56:59,737 - root - INFO - Configuration validated successfully
2025-07-10 06:56:59,919 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:59,926 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:56:59,924 - root - INFO - Pre-warming the model...
2025-07-10 06:56:59,933 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:56:59,935 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:00,021 - root - INFO - Configuration validated successfully
2025-07-10 06:57:00,028 - root - INFO - Pre-warming the model...
2025-07-10 06:57:00,135 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:00,222 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:00,235 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:00,421 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:00,529 - root - INFO - Configuration validated successfully
2025-07-10 06:57:00,541 - root - INFO - Pre-warming the model...
2025-07-10 06:57:00,626 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:00,627 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:00,727 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:01,525 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:01,534 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:01,620 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:01,634 - root - INFO - Configuration validated successfully
2025-07-10 06:57:01,639 - root - INFO - Pre-warming the model...
2025-07-10 06:57:01,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:01,723 - root - INFO - Configuration validated successfully
2025-07-10 06:57:01,725 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:01,736 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:01,734 - root - INFO - Pre-warming the model...
2025-07-10 06:57:01,828 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:01,829 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:01,924 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:02,033 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:02,128 - root - INFO - Configuration validated successfully
2025-07-10 06:57:02,132 - root - INFO - Pre-warming the model...
2025-07-10 06:57:02,135 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:02,136 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:02,921 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:02,932 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:03,035 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:03,044 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:03,120 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:03,121 - root - INFO - Loading groq LLM...
2025-07-10 06:57:03,269 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:03,323 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:03,327 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:03,327 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:03,527 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:03,540 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:03,542 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:03,543 - root - INFO - Loading groq LLM...
2025-07-10 06:57:03,922 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:03,925 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:03,930 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:03,931 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,175 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,220 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,414 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.5633645057678223, 'init_vectorstore': 6.495021104812622, 'setup_retriever': 0.0005209445953369141, 'load_llm': 0.8800129890441895, 'create_chains': 0.006609439849853516}
2025-07-10 06:57:04,415 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.95s
2025-07-10 06:57:04,417 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.25320887565612793, 'init_vectorstore': 6.388540983200073, 'setup_retriever': 0.00048041343688964844, 'load_llm': 1.373955249786377, 'create_chains': 0.007376194000244141}
2025-07-10 06:57:04,418 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.03s
2025-07-10 06:57:04,513 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,514 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,517 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,517 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,542 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:04,543 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:04,577 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:04,621 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:04,622 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:04,624 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:04,624 - root - INFO - Loading groq LLM...
2025-07-10 06:57:04,627 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,628 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,637 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:04,638 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:04,639 - root - INFO - Loading groq LLM...
2025-07-10 06:57:04,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,721 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,721 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,843 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:04,865 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:04,870 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:04,871 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:04,872 - root - INFO - Loading groq LLM...
2025-07-10 06:57:04,883 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,883 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,900 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:04,920 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:04,940 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:04,945 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:04,945 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:05,037 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:05,045 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:05,046 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:05,047 - root - INFO - Loading groq LLM...
2025-07-10 06:57:05,357 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.593111276626587, 'init_vectorstore': 5.885282278060913, 'setup_retriever': 0.0012290477752685547, 'load_llm': 0.7272205352783203, 'create_chains': 0.006063699722290039}
2025-07-10 06:57:05,358 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.22s
2025-07-10 06:57:05,363 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.3072845935821533, 'init_vectorstore': 6.547029733657837, 'setup_retriever': 0.0006303787231445312, 'load_llm': 0.7783567905426025, 'create_chains': 0.007775306701660156}
2025-07-10 06:57:05,365 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.71s
2025-07-10 06:57:05,539 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.89975905418396, 'init_vectorstore': 5.533031225204468, 'setup_retriever': 0.00042366981506347656, 'load_llm': 0.6668550968170166, 'create_chains': 0.007788181304931641}
2025-07-10 06:57:05,541 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.12s
2025-07-10 06:57:05,610 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.8090763092041016, 'init_vectorstore': 5.205572605133057, 'setup_retriever': 0.0007660388946533203, 'load_llm': 0.5654251575469971, 'create_chains': 0.007793426513671875}
2025-07-10 06:57:05,621 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.68s
2025-07-10 06:57:05,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:05,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:05,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:05,652 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:05,658 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:05,658 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:05,661 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:05,661 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:05,661 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:05,663 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:05,665 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:05,812 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:05,815 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:05,825 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:05,826 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:05,829 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:05,829 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:05,841 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:05,842 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:05,844 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:05,846 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:05,851 - root - INFO - Loading groq LLM...
2025-07-10 06:57:05,853 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:05,848 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:05,850 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:05,854 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:05,856 - root - INFO - Loading groq LLM...
2025-07-10 06:57:05,856 - root - INFO - Loading groq LLM...
2025-07-10 06:57:05,857 - root - INFO - Loading groq LLM...
2025-07-10 06:57:06,014 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,027 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,027 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:06,032 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:06,044 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:06,092 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,112 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,114 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:06,115 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:06,124 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,213 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,215 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,403 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,423 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:06,427 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,512 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,513 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:06,524 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:06,622 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:06,613 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:06,613 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:06,626 - root - INFO - Loading groq LLM...
2025-07-10 06:57:06,627 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:06,628 - root - INFO - Loading groq LLM...
2025-07-10 06:57:06,632 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:06,712 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:06,715 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:06,716 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:06,720 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:06,721 - root - INFO - Loading groq LLM...
2025-07-10 06:57:06,721 - root - INFO - Loading groq LLM...
2025-07-10 06:57:06,812 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,812 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:06,823 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:06,918 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:07,125 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:07,125 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:07,128 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:07,227 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:07,317 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:07,319 - root - INFO - Loading groq LLM...
2025-07-10 06:57:07,412 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:07,414 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:07,419 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:07,528 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:07,611 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:07,623 - root - INFO - Loading groq LLM...
2025-07-10 06:57:07,627 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:57:07,716 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:57:07,718 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:57:07,722 - root - INFO - Loading groq LLM...
2025-07-10 06:57:07,825 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:07,826 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:08,213 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:08,220 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.201411008834839, 'init_vectorstore': 5.387517929077148, 'setup_retriever': 0.011263132095336914, 'load_llm': 2.3047566413879395, 'create_chains': 0.08997631072998047}
2025-07-10 06:57:08,225 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.01s
2025-07-10 06:57:08,520 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.1938109397888184, 'init_vectorstore': 4.491790771484375, 'setup_retriever': 0.0032958984375, 'load_llm': 2.688171148300171, 'create_chains': 0.005223989486694336}
2025-07-10 06:57:08,524 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.49s
2025-07-10 06:57:08,620 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:08,621 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:08,721 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:08,724 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:09,113 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:09,114 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:09,125 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:09,121 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:09,191 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:09,219 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:09,422 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:09,424 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:09,618 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.0017075538635254, 'init_vectorstore': 4.487106084823608, 'setup_retriever': 0.002508401870727539, 'load_llm': 3.703082799911499, 'create_chains': 0.10044097900390625}
2025-07-10 06:57:09,623 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.39s
2025-07-10 06:57:09,719 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.00191330909729, 'init_vectorstore': 4.9901442527771, 'setup_retriever': 0.0014185905456542969, 'load_llm': 3.78773832321167, 'create_chains': 0.10224127769470215}
2025-07-10 06:57:09,727 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.91s
2025-07-10 06:57:09,825 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.705388069152832, 'init_vectorstore': 4.2042741775512695, 'setup_retriever': 0.00031638145446777344, 'load_llm': 3.081787109375, 'create_chains': 0.1108388900756836}
2025-07-10 06:57:09,911 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.28s
2025-07-10 06:57:09,917 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.9020085334777832, 'init_vectorstore': 4.1972551345825195, 'setup_retriever': 0.0002486705780029297, 'load_llm': 3.294571876525879, 'create_chains': 0.19834375381469727}
2025-07-10 06:57:09,924 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 9.69s
2025-07-10 06:57:10,017 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:10,023 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:10,029 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:10,122 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.011439561843872, 'init_vectorstore': 4.479884624481201, 'setup_retriever': 0.0005774497985839844, 'load_llm': 3.419992446899414, 'create_chains': 0.08887577056884766}
2025-07-10 06:57:10,130 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.20s
2025-07-10 06:57:10,227 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.9057931900024414, 'init_vectorstore': 4.481369972229004, 'setup_retriever': 0.0006356239318847656, 'load_llm': 3.7033531665802, 'create_chains': 0.011392593383789062}
2025-07-10 06:57:10,232 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.30s
2025-07-10 06:57:10,314 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:10,320 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:10,335 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.296363353729248, 'init_vectorstore': 3.9865024089813232, 'setup_retriever': 0.011339426040649414, 'load_llm': 3.1931848526000977, 'create_chains': 0.017189979553222656}
2025-07-10 06:57:10,338 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.52s
2025-07-10 06:57:10,433 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:10,438 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:10,508 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:10,497 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:10,513 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:10,623 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:10,625 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:10,716 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.305297613143921, 'init_vectorstore': 4.183864116668701, 'setup_retriever': 0.0004417896270751953, 'load_llm': 3.262460708618164, 'create_chains': 0.040994882583618164}
2025-07-10 06:57:10,717 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.89s
2025-07-10 06:57:10,782 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.1038203239440918, 'init_vectorstore': 4.295603275299072, 'setup_retriever': 0.0009140968322753906, 'load_llm': 3.147226572036743, 'create_chains': 0.007906913757324219}
2025-07-10 06:57:10,783 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.56s
2025-07-10 06:57:11,002 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:11,006 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:11,003 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:11,003 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:11,003 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:11,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:11,005 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:11,014 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:11,496 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:11,497 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:11,496 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:11,501 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:11,502 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:11,504 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:11,505 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:11,507 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:57:11,507 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f47ed7dfb0ecbfa9827d133b2fc23bd2', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e82e86ad-851c-48dd-be79-18d72917dfde', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f47ed7dfb0ecbfa9827d133b2fc23bd2', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e82e86ad-851c-48dd-be79-18d72917dfde', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:11,834 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:57:11,835 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:11,834 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:11,835 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:11,835 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:11,835 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:11,839 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7be616c1010bfb7fd5d76255cc90d0c9', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '28', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'de5de5c8-8d82-4191-8bf0-eb8ecf62d422', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7be616c1010bfb7fd5d76255cc90d0c9', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '28', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'de5de5c8-8d82-4191-8bf0-eb8ecf62d422', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:11,850 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'bda2c0a8f4fe9e9df811c4052395f567', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '147a658a-d80b-40cd-8452-3e0977dd1368', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'bda2c0a8f4fe9e9df811c4052395f567', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '147a658a-d80b-40cd-8452-3e0977dd1368', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:11,848 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ed7a631af065386cba98aa37605b665f', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '72c3a4ad-4e61-452b-b157-9b1b6a166024', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ed7a631af065386cba98aa37605b665f', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '72c3a4ad-4e61-452b-b157-9b1b6a166024', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:11,847 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c35c62a2c24a14f3ecab9e3330880536', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '19', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c34bd519-ef48-45b8-8d22-710428d0f1b7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c35c62a2c24a14f3ecab9e3330880536', 'date': 'Thu, 10 Jul 2025 06:57:11 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '19', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c34bd519-ef48-45b8-8d22-710428d0f1b7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:12,322 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:12,323 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:12,324 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:12,337 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '54fcc69a35fd07ced6121cf3dfc69d4b', 'date': 'Thu, 10 Jul 2025 06:57:12 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '41', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '594ab989-a7d2-4300-bb8b-0aca04d9e469', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '54fcc69a35fd07ced6121cf3dfc69d4b', 'date': 'Thu, 10 Jul 2025 06:57:12 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '41', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '594ab989-a7d2-4300-bb8b-0aca04d9e469', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:12,330 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c8c8b83056798cfd36742d793509f8a6', 'date': 'Thu, 10 Jul 2025 06:57:12 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '31', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '333ba24c-4cf5-4874-983a-715fe1a56d7c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c8c8b83056798cfd36742d793509f8a6', 'date': 'Thu, 10 Jul 2025 06:57:12 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '31', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '333ba24c-4cf5-4874-983a-715fe1a56d7c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:12,330 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f0dedacccd53bbaf531935784f9746f4', 'date': 'Thu, 10 Jul 2025 06:57:12 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '76a17ae4-e433-439d-959d-e98ade517704', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f0dedacccd53bbaf531935784f9746f4', 'date': 'Thu, 10 Jul 2025 06:57:12 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '76a17ae4-e433-439d-959d-e98ade517704', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:12,815 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:12,822 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:13,129 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:13,134 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:13,211 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:13,218 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:14,020 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:57:14,315 - root - INFO - Model pre-warmed successfully
2025-07-10 06:57:14,319 - root - INFO - HR Assistant started successfully
2025-07-10 06:57:27,911 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:27,920 - root - INFO - Configuration validated successfully
2025-07-10 06:57:27,921 - root - INFO - Pre-warming the model...
2025-07-10 06:57:27,922 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:27,924 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:27,926 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:27,937 - root - INFO - Configuration validated successfully
2025-07-10 06:57:27,952 - root - INFO - Pre-warming the model...
2025-07-10 06:57:27,958 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:27,967 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:28,068 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:28,077 - root - INFO - Configuration validated successfully
2025-07-10 06:57:28,079 - root - INFO - Pre-warming the model...
2025-07-10 06:57:28,080 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:28,082 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:28,263 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:28,272 - root - INFO - Configuration validated successfully
2025-07-10 06:57:28,274 - root - INFO - Pre-warming the model...
2025-07-10 06:57:28,276 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:28,354 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:28,460 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:28,553 - root - INFO - Configuration validated successfully
2025-07-10 06:57:28,555 - root - INFO - Pre-warming the model...
2025-07-10 06:57:28,557 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:28,560 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:28,662 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:28,756 - root - INFO - Configuration validated successfully
2025-07-10 06:57:28,759 - root - INFO - Pre-warming the model...
2025-07-10 06:57:28,761 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:28,767 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:29,052 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:29,152 - root - INFO - Configuration validated successfully
2025-07-10 06:57:29,156 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:29,156 - root - INFO - Pre-warming the model...
2025-07-10 06:57:29,162 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:29,164 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:29,253 - root - INFO - Configuration validated successfully
2025-07-10 06:57:29,258 - root - INFO - Pre-warming the model...
2025-07-10 06:57:29,265 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:29,353 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:29,553 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:29,664 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:29,752 - root - INFO - Configuration validated successfully
2025-07-10 06:57:29,759 - root - INFO - Configuration validated successfully
2025-07-10 06:57:29,760 - root - INFO - Pre-warming the model...
2025-07-10 06:57:29,858 - root - INFO - Pre-warming the model...
2025-07-10 06:57:29,862 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:29,860 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:29,951 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:29,959 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:29,959 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:29,957 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:30,164 - root - INFO - Configuration validated successfully
2025-07-10 06:57:30,252 - root - INFO - Pre-warming the model...
2025-07-10 06:57:30,252 - root - INFO - Configuration validated successfully
2025-07-10 06:57:30,259 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:30,351 - root - INFO - Pre-warming the model...
2025-07-10 06:57:30,451 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:30,365 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:30,459 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:30,651 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:30,669 - root - INFO - Configuration validated successfully
2025-07-10 06:57:30,751 - root - INFO - Pre-warming the model...
2025-07-10 06:57:30,763 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:30,851 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:30,852 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:31,052 - root - INFO - Configuration validated successfully
2025-07-10 06:57:31,063 - root - INFO - Pre-warming the model...
2025-07-10 06:57:31,065 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:31,151 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:31,253 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:31,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:31,459 - root - INFO - Configuration validated successfully
2025-07-10 06:57:31,562 - root - INFO - Pre-warming the model...
2025-07-10 06:57:31,651 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:31,666 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:31,754 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:32,056 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:32,065 - root - INFO - Configuration validated successfully
2025-07-10 06:57:32,066 - root - INFO - Pre-warming the model...
2025-07-10 06:57:32,068 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:32,151 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:32,164 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:32,263 - root - INFO - Configuration validated successfully
2025-07-10 06:57:32,264 - root - INFO - Pre-warming the model...
2025-07-10 06:57:32,266 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:32,351 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:32,558 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:32,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:32,855 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:33,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:33,156 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:33,455 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:33,465 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:33,712 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:33,752 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,123 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:34,127 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:34,128 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:34,128 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,151 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5fe734ae93c4f50ed1768b6d77dcb3ba', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5f000149-e3f9-4a29-9473-7818de78e606', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5fe734ae93c4f50ed1768b6d77dcb3ba', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5f000149-e3f9-4a29-9473-7818de78e606', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:34,152 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4ad946c152e5929238585b9692a2e358', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d928cc1-dd59-4313-a7df-13b0aae36e41', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4ad946c152e5929238585b9692a2e358', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d928cc1-dd59-4313-a7df-13b0aae36e41', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:34,170 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5fe734ae93c4f50ed1768b6d77dcb3ba', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5f000149-e3f9-4a29-9473-7818de78e606', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5fe734ae93c4f50ed1768b6d77dcb3ba', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5f000149-e3f9-4a29-9473-7818de78e606', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:34,171 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4ad946c152e5929238585b9692a2e358', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d928cc1-dd59-4313-a7df-13b0aae36e41', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4ad946c152e5929238585b9692a2e358', 'date': 'Thu, 10 Jul 2025 06:57:33 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d928cc1-dd59-4313-a7df-13b0aae36e41', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:34,453 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:34,457 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:34,701 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:34,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,711 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:34,710 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,946 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:34,947 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:34,948 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,950 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,951 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,951 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:34,956 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0afb7d09e7295a074ab6f7ad90988f4a', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '32', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8922700d-c4ab-4177-afe1-62ad0daed47f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0afb7d09e7295a074ab6f7ad90988f4a', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '32', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8922700d-c4ab-4177-afe1-62ad0daed47f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:34,980 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0afb7d09e7295a074ab6f7ad90988f4a', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '32', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8922700d-c4ab-4177-afe1-62ad0daed47f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0afb7d09e7295a074ab6f7ad90988f4a', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '32', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8922700d-c4ab-4177-afe1-62ad0daed47f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,198 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:35,255 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,253 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7511883773c9b73f2c5d7717644394d3', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '04d2c178-ffba-4162-93d4-29b755e17e51', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7511883773c9b73f2c5d7717644394d3', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '04d2c178-ffba-4162-93d4-29b755e17e51', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,286 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7511883773c9b73f2c5d7717644394d3', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '04d2c178-ffba-4162-93d4-29b755e17e51', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7511883773c9b73f2c5d7717644394d3', 'date': 'Thu, 10 Jul 2025 06:57:34 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '04d2c178-ffba-4162-93d4-29b755e17e51', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,431 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,444 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:35,450 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,454 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '681c05363011bbc5bea40a68c5c8f1f5', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e0c31820-8fd6-489c-824d-fa954bc074b9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '681c05363011bbc5bea40a68c5c8f1f5', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e0c31820-8fd6-489c-824d-fa954bc074b9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,563 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '681c05363011bbc5bea40a68c5c8f1f5', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e0c31820-8fd6-489c-824d-fa954bc074b9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '681c05363011bbc5bea40a68c5c8f1f5', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e0c31820-8fd6-489c-824d-fa954bc074b9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,762 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:35,763 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:35,773 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:35,765 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:35,765 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,766 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,769 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,769 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,841 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:35,846 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1299137e47699c6cdeae033aeed654e2', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '74447a46-e71b-41b7-94a2-b82781b4962c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1299137e47699c6cdeae033aeed654e2', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '74447a46-e71b-41b7-94a2-b82781b4962c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,842 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '2384e1601167826286feefe5c516f378', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c3b0dba9-577b-4c07-8e8b-12251c31762d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '2384e1601167826286feefe5c516f378', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c3b0dba9-577b-4c07-8e8b-12251c31762d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,955 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '2384e1601167826286feefe5c516f378', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c3b0dba9-577b-4c07-8e8b-12251c31762d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '2384e1601167826286feefe5c516f378', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c3b0dba9-577b-4c07-8e8b-12251c31762d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,848 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c71da67e739867f694ea2a6f57f5ff38', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '60c24504-6ac4-4b8e-8a62-06cbf1d856dd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c71da67e739867f694ea2a6f57f5ff38', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '60c24504-6ac4-4b8e-8a62-06cbf1d856dd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,949 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1299137e47699c6cdeae033aeed654e2', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '74447a46-e71b-41b7-94a2-b82781b4962c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1299137e47699c6cdeae033aeed654e2', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '74447a46-e71b-41b7-94a2-b82781b4962c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:35,842 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f8712ba326ebdbab790e0a14f4095ddf', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '9bf92af7-09ed-4bf3-bd7b-aceada528375', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f8712ba326ebdbab790e0a14f4095ddf', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '9bf92af7-09ed-4bf3-bd7b-aceada528375', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:36,052 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c71da67e739867f694ea2a6f57f5ff38', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '60c24504-6ac4-4b8e-8a62-06cbf1d856dd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c71da67e739867f694ea2a6f57f5ff38', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '60c24504-6ac4-4b8e-8a62-06cbf1d856dd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:36,141 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f8712ba326ebdbab790e0a14f4095ddf', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '9bf92af7-09ed-4bf3-bd7b-aceada528375', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f8712ba326ebdbab790e0a14f4095ddf', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '9bf92af7-09ed-4bf3-bd7b-aceada528375', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:36,249 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:36,344 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0d0f56c4771d4d3348e5b8feaa3f1e6a', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '75a6e8d7-efe1-4354-b07a-ceac2c6668b7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0d0f56c4771d4d3348e5b8feaa3f1e6a', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '75a6e8d7-efe1-4354-b07a-ceac2c6668b7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:36,449 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0d0f56c4771d4d3348e5b8feaa3f1e6a', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '75a6e8d7-efe1-4354-b07a-ceac2c6668b7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0d0f56c4771d4d3348e5b8feaa3f1e6a', 'date': 'Thu, 10 Jul 2025 06:57:35 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '75a6e8d7-efe1-4354-b07a-ceac2c6668b7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:36,746 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:36,750 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:36,842 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1322e50200f31a78f55172e3300fea00', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '822e5ce3-0354-4997-94cf-e648e8e36b96', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1322e50200f31a78f55172e3300fea00', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '822e5ce3-0354-4997-94cf-e648e8e36b96', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:36,950 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1322e50200f31a78f55172e3300fea00', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '822e5ce3-0354-4997-94cf-e648e8e36b96', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1322e50200f31a78f55172e3300fea00', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '822e5ce3-0354-4997-94cf-e648e8e36b96', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:36,841 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9434a4bbf3219a844984ebe7b96cecaf', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8e13da2e-d3fb-4228-afc9-91723c20220f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9434a4bbf3219a844984ebe7b96cecaf', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8e13da2e-d3fb-4228-afc9-91723c20220f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,141 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9434a4bbf3219a844984ebe7b96cecaf', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8e13da2e-d3fb-4228-afc9-91723c20220f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9434a4bbf3219a844984ebe7b96cecaf', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8e13da2e-d3fb-4228-afc9-91723c20220f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,148 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:37,150 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:37,150 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:37,147 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:37,157 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:37,343 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '28b1aa028e7174042fbcc7c72d49983d', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2f0b815c-e826-4bd7-812a-b64e6b10b3e0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '28b1aa028e7174042fbcc7c72d49983d', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2f0b815c-e826-4bd7-812a-b64e6b10b3e0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,248 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e7c627a1420380813677d79e3926ca1f', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8a7d13b4-dd0b-4513-a868-c56a535c71a6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e7c627a1420380813677d79e3926ca1f', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8a7d13b4-dd0b-4513-a868-c56a535c71a6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,341 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '194698b8d00d992a2d3e800c2124613e', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c7f40cca-365f-4626-8c32-aa1d38921783', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '194698b8d00d992a2d3e800c2124613e', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c7f40cca-365f-4626-8c32-aa1d38921783', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,454 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e7c627a1420380813677d79e3926ca1f', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8a7d13b4-dd0b-4513-a868-c56a535c71a6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e7c627a1420380813677d79e3926ca1f', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '8a7d13b4-dd0b-4513-a868-c56a535c71a6', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,544 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '194698b8d00d992a2d3e800c2124613e', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c7f40cca-365f-4626-8c32-aa1d38921783', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '194698b8d00d992a2d3e800c2124613e', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'c7f40cca-365f-4626-8c32-aa1d38921783', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,544 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '28b1aa028e7174042fbcc7c72d49983d', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2f0b815c-e826-4bd7-812a-b64e6b10b3e0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '28b1aa028e7174042fbcc7c72d49983d', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '23', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2f0b815c-e826-4bd7-812a-b64e6b10b3e0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,352 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7571d026228661ce476572fd13a0e78b', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b21d475d-d4e0-4df1-b2a2-ddc4af2c2e4e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7571d026228661ce476572fd13a0e78b', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b21d475d-d4e0-4df1-b2a2-ddc4af2c2e4e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,349 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '05196dcc693be26de23751eaea71c769', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b319848f-5386-406b-84c6-dd4e55541d1f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '05196dcc693be26de23751eaea71c769', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b319848f-5386-406b-84c6-dd4e55541d1f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,661 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '05196dcc693be26de23751eaea71c769', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b319848f-5386-406b-84c6-dd4e55541d1f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '05196dcc693be26de23751eaea71c769', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b319848f-5386-406b-84c6-dd4e55541d1f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:37,643 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7571d026228661ce476572fd13a0e78b', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b21d475d-d4e0-4df1-b2a2-ddc4af2c2e4e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7571d026228661ce476572fd13a0e78b', 'date': 'Thu, 10 Jul 2025 06:57:36 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '29', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b21d475d-d4e0-4df1-b2a2-ddc4af2c2e4e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:49,145 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:49,145 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:49,152 - root - INFO - Configuration validated successfully
2025-07-10 06:57:49,153 - root - INFO - Configuration validated successfully
2025-07-10 06:57:49,153 - root - INFO - Pre-warming the model...
2025-07-10 06:57:49,154 - root - INFO - Pre-warming the model...
2025-07-10 06:57:49,155 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:49,156 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:49,156 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:49,157 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:49,248 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:49,308 - root - INFO - Configuration validated successfully
2025-07-10 06:57:49,309 - root - INFO - Pre-warming the model...
2025-07-10 06:57:49,311 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:49,312 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:49,610 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:49,630 - root - INFO - Configuration validated successfully
2025-07-10 06:57:49,707 - root - INFO - Pre-warming the model...
2025-07-10 06:57:49,708 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:49,712 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:49,923 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:50,014 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:50,107 - root - INFO - Configuration validated successfully
2025-07-10 06:57:50,112 - root - INFO - Pre-warming the model...
2025-07-10 06:57:50,112 - root - INFO - Configuration validated successfully
2025-07-10 06:57:50,117 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:50,119 - root - INFO - Pre-warming the model...
2025-07-10 06:57:50,120 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:50,210 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:50,212 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:50,321 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:50,414 - root - INFO - Configuration validated successfully
2025-07-10 06:57:50,420 - root - INFO - Pre-warming the model...
2025-07-10 06:57:50,422 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:50,425 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:50,514 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:50,713 - root - INFO - Configuration validated successfully
2025-07-10 06:57:50,718 - root - INFO - Pre-warming the model...
2025-07-10 06:57:50,724 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:50,808 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:51,214 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:51,409 - root - INFO - Configuration validated successfully
2025-07-10 06:57:51,411 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:51,416 - root - INFO - Pre-warming the model...
2025-07-10 06:57:51,421 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:51,507 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:51,511 - root - INFO - Configuration validated successfully
2025-07-10 06:57:51,513 - root - INFO - Pre-warming the model...
2025-07-10 06:57:51,520 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:51,607 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:51,822 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:51,915 - root - INFO - Configuration validated successfully
2025-07-10 06:57:52,008 - root - INFO - Pre-warming the model...
2025-07-10 06:57:52,013 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:52,022 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:52,417 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:52,514 - root - INFO - Configuration validated successfully
2025-07-10 06:57:52,521 - root - INFO - Pre-warming the model...
2025-07-10 06:57:52,612 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:52,620 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:52,713 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:52,817 - root - INFO - Configuration validated successfully
2025-07-10 06:57:52,820 - root - INFO - Pre-warming the model...
2025-07-10 06:57:52,822 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:52,908 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:53,117 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:53,222 - root - INFO - Configuration validated successfully
2025-07-10 06:57:53,312 - root - INFO - Pre-warming the model...
2025-07-10 06:57:53,317 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:53,419 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:53,413 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:53,425 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:53,616 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:53,720 - root - INFO - Configuration validated successfully
2025-07-10 06:57:53,722 - root - INFO - Pre-warming the model...
2025-07-10 06:57:53,723 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:53,812 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:53,813 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:53,820 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:54,018 - root - INFO - Starting HR Assistant...
2025-07-10 06:57:54,207 - root - INFO - Configuration validated successfully
2025-07-10 06:57:54,216 - root - INFO - Pre-warming the model...
2025-07-10 06:57:54,220 - root - INFO - Configuration validated successfully
2025-07-10 06:57:54,224 - root - INFO - Pre-warming the model...
2025-07-10 06:57:54,219 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:54,319 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:54,308 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:57:54,410 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:57:54,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:54,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:54,723 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:55,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:55,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:55,411 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:55,412 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:55,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:56,368 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:56,368 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:56,368 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:56,369 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:56,370 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:56,371 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:56,371 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:56,374 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f13f8a20a21923f830f1162399fd910c', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0e699931-0439-43a2-b7a2-d2d7a7592387', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f13f8a20a21923f830f1162399fd910c', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0e699931-0439-43a2-b7a2-d2d7a7592387', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:56,393 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f13f8a20a21923f830f1162399fd910c', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0e699931-0439-43a2-b7a2-d2d7a7592387', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f13f8a20a21923f830f1162399fd910c', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0e699931-0439-43a2-b7a2-d2d7a7592387', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:56,383 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b997a2af31e40a3cfb323ed9df7ca834', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a4c61eeb-abf3-48cd-9c81-970ad4d00b3d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b997a2af31e40a3cfb323ed9df7ca834', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a4c61eeb-abf3-48cd-9c81-970ad4d00b3d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:56,405 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b997a2af31e40a3cfb323ed9df7ca834', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a4c61eeb-abf3-48cd-9c81-970ad4d00b3d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b997a2af31e40a3cfb323ed9df7ca834', 'date': 'Thu, 10 Jul 2025 06:57:55 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a4c61eeb-abf3-48cd-9c81-970ad4d00b3d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:56,385 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ed021c2d655552395b9c8990c4714e48', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '4f1ade73-c4a1-42ec-8cd2-348a42488b8a', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ed021c2d655552395b9c8990c4714e48', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '4f1ade73-c4a1-42ec-8cd2-348a42488b8a', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:56,411 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ed021c2d655552395b9c8990c4714e48', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '4f1ade73-c4a1-42ec-8cd2-348a42488b8a', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ed021c2d655552395b9c8990c4714e48', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '4f1ade73-c4a1-42ec-8cd2-348a42488b8a', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:56,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:56,615 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:56,615 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:57,107 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:57,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:57,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:57,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:57,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:57,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:57,114 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:57:57,115 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e9be19ffd506f209961a11fe96586658', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'efa08016-3a57-4563-9e14-e80bac7daa7c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e9be19ffd506f209961a11fe96586658', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'efa08016-3a57-4563-9e14-e80bac7daa7c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:57,136 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e9be19ffd506f209961a11fe96586658', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'efa08016-3a57-4563-9e14-e80bac7daa7c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'e9be19ffd506f209961a11fe96586658', 'date': 'Thu, 10 Jul 2025 06:57:56 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'efa08016-3a57-4563-9e14-e80bac7daa7c', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:57,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:57,521 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:57,519 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:57,519 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:58,025 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:58,026 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:58,024 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:58,032 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b744b2be206dfae82dc848f130df3410', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '20', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd03ebf84-b209-4285-a260-42737a0876fe', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b744b2be206dfae82dc848f130df3410', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '20', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd03ebf84-b209-4285-a260-42737a0876fe', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,035 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '6ef836b2bbfefe86e631e29ae8c8f2b5', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6a234f0d-e3de-4b97-a616-0d5bfac5d4c4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '6ef836b2bbfefe86e631e29ae8c8f2b5', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6a234f0d-e3de-4b97-a616-0d5bfac5d4c4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,109 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '31be1d33d8ef1fa6a4054ab3628e8b4c', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5d456933-914d-49b5-89f4-5806051010ae', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '31be1d33d8ef1fa6a4054ab3628e8b4c', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5d456933-914d-49b5-89f4-5806051010ae', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,131 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '6ef836b2bbfefe86e631e29ae8c8f2b5', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6a234f0d-e3de-4b97-a616-0d5bfac5d4c4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '6ef836b2bbfefe86e631e29ae8c8f2b5', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6a234f0d-e3de-4b97-a616-0d5bfac5d4c4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,131 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b744b2be206dfae82dc848f130df3410', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '20', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd03ebf84-b209-4285-a260-42737a0876fe', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b744b2be206dfae82dc848f130df3410', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '20', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd03ebf84-b209-4285-a260-42737a0876fe', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,208 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '31be1d33d8ef1fa6a4054ab3628e8b4c', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5d456933-914d-49b5-89f4-5806051010ae', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '31be1d33d8ef1fa6a4054ab3628e8b4c', 'date': 'Thu, 10 Jul 2025 06:57:57 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5d456933-914d-49b5-89f4-5806051010ae', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,370 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:58,369 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:58,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:58,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:58,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:58,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:57:58,591 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:58,617 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:58,614 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:58,710 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:58,609 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1e760c7795981d611dcd6e869cdb71d', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d96f23b-b4db-49e3-a715-bcaf0c57eb13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1e760c7795981d611dcd6e869cdb71d', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d96f23b-b4db-49e3-a715-bcaf0c57eb13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,810 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1e760c7795981d611dcd6e869cdb71d', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d96f23b-b4db-49e3-a715-bcaf0c57eb13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a1e760c7795981d611dcd6e869cdb71d', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6d96f23b-b4db-49e3-a715-bcaf0c57eb13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,716 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '39ddfb0f8d91c376d14a2bfdb361613b', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '63c86f4e-f454-4b98-9b0d-366018c761d4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '39ddfb0f8d91c376d14a2bfdb361613b', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '63c86f4e-f454-4b98-9b0d-366018c761d4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,812 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'bc4e4053740a0a3ca668e33c40a69df5', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '41b492f6-49d4-4686-a116-07b0070662b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'bc4e4053740a0a3ca668e33c40a69df5', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '41b492f6-49d4-4686-a116-07b0070662b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,915 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '39ddfb0f8d91c376d14a2bfdb361613b', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '63c86f4e-f454-4b98-9b0d-366018c761d4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '39ddfb0f8d91c376d14a2bfdb361613b', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '63c86f4e-f454-4b98-9b0d-366018c761d4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,917 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'bc4e4053740a0a3ca668e33c40a69df5', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '41b492f6-49d4-4686-a116-07b0070662b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'bc4e4053740a0a3ca668e33c40a69df5', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '25', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '41b492f6-49d4-4686-a116-07b0070662b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:58,808 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a9a62cf62ef2edcbaa77ce6b7aedb57f', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '832eee02-c37e-4c1e-9c51-ad95e08a405e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a9a62cf62ef2edcbaa77ce6b7aedb57f', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '832eee02-c37e-4c1e-9c51-ad95e08a405e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:59,012 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a9a62cf62ef2edcbaa77ce6b7aedb57f', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '832eee02-c37e-4c1e-9c51-ad95e08a405e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a9a62cf62ef2edcbaa77ce6b7aedb57f', 'date': 'Thu, 10 Jul 2025 06:57:58 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '832eee02-c37e-4c1e-9c51-ad95e08a405e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:59,514 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:57:59,610 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '79eb9f10e4a1ca0b0ae35690468cafdb', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bfba3dc3-15e9-42d0-b686-3f2770f228db', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '79eb9f10e4a1ca0b0ae35690468cafdb', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bfba3dc3-15e9-42d0-b686-3f2770f228db', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:57:59,820 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '79eb9f10e4a1ca0b0ae35690468cafdb', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bfba3dc3-15e9-42d0-b686-3f2770f228db', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '79eb9f10e4a1ca0b0ae35690468cafdb', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '14', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bfba3dc3-15e9-42d0-b686-3f2770f228db', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,108 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:00,109 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:00,111 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:00,123 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:00,111 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:00,224 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '508ce484a6ac36561810b821aa1e3ef6', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '69cb7922-5a79-4efb-9633-6219591d432b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '508ce484a6ac36561810b821aa1e3ef6', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '69cb7922-5a79-4efb-9633-6219591d432b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,312 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '17c9e67c5987dcb93320c95f62f34f40', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'fa3506ac-1d0f-46a6-8bf7-cda4d7798d69', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '17c9e67c5987dcb93320c95f62f34f40', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'fa3506ac-1d0f-46a6-8bf7-cda4d7798d69', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,310 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c100bd9cad789f4abaff5704db1399a5', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44173e20-b80e-4952-84df-5233df429239', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c100bd9cad789f4abaff5704db1399a5', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44173e20-b80e-4952-84df-5233df429239', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,617 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '508ce484a6ac36561810b821aa1e3ef6', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '69cb7922-5a79-4efb-9633-6219591d432b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '508ce484a6ac36561810b821aa1e3ef6', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '69cb7922-5a79-4efb-9633-6219591d432b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,619 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '17c9e67c5987dcb93320c95f62f34f40', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'fa3506ac-1d0f-46a6-8bf7-cda4d7798d69', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '17c9e67c5987dcb93320c95f62f34f40', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '15', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'fa3506ac-1d0f-46a6-8bf7-cda4d7798d69', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,220 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8ebcadbd33fa1449607ca2440d9744c', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '36e9e2e1-c1ff-4701-a813-9bb3058c6401', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8ebcadbd33fa1449607ca2440d9744c', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '36e9e2e1-c1ff-4701-a813-9bb3058c6401', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,712 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c100bd9cad789f4abaff5704db1399a5', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44173e20-b80e-4952-84df-5233df429239', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c100bd9cad789f4abaff5704db1399a5', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '44173e20-b80e-4952-84df-5233df429239', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,216 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '265bef8b254cdc8d029988b2253cf2c4', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e8b588c4-25e0-4159-9425-ebeae0209d2d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '265bef8b254cdc8d029988b2253cf2c4', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e8b588c4-25e0-4159-9425-ebeae0209d2d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,822 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '265bef8b254cdc8d029988b2253cf2c4', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e8b588c4-25e0-4159-9425-ebeae0209d2d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '265bef8b254cdc8d029988b2253cf2c4', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'e8b588c4-25e0-4159-9425-ebeae0209d2d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:00,817 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8ebcadbd33fa1449607ca2440d9744c', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '36e9e2e1-c1ff-4701-a813-9bb3058c6401', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 177, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 264, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8ebcadbd33fa1449607ca2440d9744c', 'date': 'Thu, 10 Jul 2025 06:57:59 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '6', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '36e9e2e1-c1ff-4701-a813-9bb3058c6401', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:18,652 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:18,718 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:18,722 - root - INFO - Configuration validated successfully
2025-07-10 06:58:18,723 - root - INFO - Pre-warming the model...
2025-07-10 06:58:18,724 - root - INFO - Configuration validated successfully
2025-07-10 06:58:18,725 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:18,725 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:18,728 - root - INFO - Pre-warming the model...
2025-07-10 06:58:18,731 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:18,732 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:18,734 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:18,739 - root - INFO - Configuration validated successfully
2025-07-10 06:58:18,739 - root - INFO - Pre-warming the model...
2025-07-10 06:58:18,742 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:18,743 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:19,422 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:19,523 - root - INFO - Configuration validated successfully
2025-07-10 06:58:19,526 - root - INFO - Pre-warming the model...
2025-07-10 06:58:19,530 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:19,615 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:19,636 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:19,824 - root - INFO - Configuration validated successfully
2025-07-10 06:58:19,915 - root - INFO - Pre-warming the model...
2025-07-10 06:58:20,015 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:20,020 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:20,024 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:20,127 - root - INFO - Configuration validated successfully
2025-07-10 06:58:20,132 - root - INFO - Pre-warming the model...
2025-07-10 06:58:20,217 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:20,223 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:20,520 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:20,722 - root - INFO - Configuration validated successfully
2025-07-10 06:58:20,822 - root - INFO - Pre-warming the model...
2025-07-10 06:58:20,825 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:20,826 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:21,018 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:21,320 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:21,422 - root - INFO - Configuration validated successfully
2025-07-10 06:58:21,517 - root - INFO - Pre-warming the model...
2025-07-10 06:58:21,519 - root - INFO - Configuration validated successfully
2025-07-10 06:58:21,716 - root - INFO - Pre-warming the model...
2025-07-10 06:58:21,527 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:21,615 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:21,825 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:21,729 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:21,723 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:21,917 - root - INFO - Configuration validated successfully
2025-07-10 06:58:22,116 - root - INFO - Pre-warming the model...
2025-07-10 06:58:21,921 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:22,226 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:22,025 - root - INFO - Configuration validated successfully
2025-07-10 06:58:22,438 - root - INFO - Pre-warming the model...
2025-07-10 06:58:22,439 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:22,026 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:22,215 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:22,716 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:22,321 - root - INFO - Configuration validated successfully
2025-07-10 06:58:22,917 - root - INFO - Pre-warming the model...
2025-07-10 06:58:22,423 - root - INFO - Configuration validated successfully
2025-07-10 06:58:22,522 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:22,724 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:23,016 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:23,417 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:23,116 - root - INFO - Pre-warming the model...
2025-07-10 06:58:23,431 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:23,516 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:23,531 - root - INFO - Configuration validated successfully
2025-07-10 06:58:23,615 - root - INFO - Pre-warming the model...
2025-07-10 06:58:23,718 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:23,818 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:24,126 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:24,221 - root - INFO - Configuration validated successfully
2025-07-10 06:58:24,319 - root - INFO - Pre-warming the model...
2025-07-10 06:58:24,427 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:24,446 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:24,827 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:24,930 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:25,032 - root - INFO - Configuration validated successfully
2025-07-10 06:58:25,118 - root - INFO - Pre-warming the model...
2025-07-10 06:58:25,218 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:25,228 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:25,627 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:25,621 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:25,724 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:25,931 - root - INFO - Starting HR Assistant...
2025-07-10 06:58:26,029 - root - INFO - Configuration validated successfully
2025-07-10 06:58:26,030 - root - INFO - Pre-warming the model...
2025-07-10 06:58:26,036 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 06:58:26,116 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 06:58:26,133 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:26,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:26,318 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:26,733 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:27,019 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:27,219 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:27,224 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:27,622 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:27,717 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:28,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:28,427 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:28,423 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:28,937 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:29,025 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:29,033 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:29,030 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:29,116 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:29,216 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:29,222 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:29,223 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:29,234 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:29,318 - root - INFO - Loading groq LLM...
2025-07-10 06:58:29,422 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:29,434 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:29,429 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:29,440 - root - INFO - Loading groq LLM...
2025-07-10 06:58:29,525 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:29,632 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:29,722 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:29,636 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:29,632 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:29,723 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:29,727 - root - INFO - Loading groq LLM...
2025-07-10 06:58:29,818 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:29,921 - root - INFO - Loading groq LLM...
2025-07-10 06:58:30,123 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:30,222 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:30,334 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:30,416 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:30,423 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:30,424 - root - INFO - Loading groq LLM...
2025-07-10 06:58:30,526 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:30,618 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:30,624 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:30,619 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:30,626 - root - INFO - Loading groq LLM...
2025-07-10 06:58:30,925 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:31,325 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:31,328 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:31,777 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:31,826 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:31,825 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:32,126 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:32,227 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:32,321 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:32,326 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:32,333 - root - INFO - Loading groq LLM...
2025-07-10 06:58:32,426 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:32,420 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:32,434 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:32,675 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:32,718 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:32,922 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:33,017 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:33,033 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:33,036 - root - INFO - Loading groq LLM...
2025-07-10 06:58:33,116 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:33,117 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:33,229 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:33,318 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:33,326 - root - INFO - Loading groq LLM...
2025-07-10 06:58:33,426 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:33,434 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:33,428 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:33,622 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:33,725 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:33,918 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:33,919 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:33,929 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:34,030 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:34,023 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:34,124 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:34,127 - root - INFO - Loading groq LLM...
2025-07-10 06:58:34,035 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:34,117 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:34,229 - root - INFO - Loading groq LLM...
2025-07-10 06:58:34,323 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:34,330 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:34,432 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:34,435 - root - INFO - Loading groq LLM...
2025-07-10 06:58:34,922 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:34,924 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:35,021 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:35,513 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:35,522 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:35,705 - root - INFO - Loading groq LLM...
2025-07-10 06:58:35,905 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:35,906 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:35,916 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:36,121 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.572443962097168, 'init_vectorstore': 6.900078058242798, 'setup_retriever': 0.0030601024627685547, 'load_llm': 6.7856385707855225, 'create_chains': 0.11291313171386719}
2025-07-10 06:58:36,207 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.097881555557251, 'init_vectorstore': 7.398946046829224, 'setup_retriever': 0.0013377666473388672, 'load_llm': 5.797881364822388, 'create_chains': 0.18696856498718262}
2025-07-10 06:58:36,308 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 16.68s
2025-07-10 06:58:36,216 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 17.48s
2025-07-10 06:58:36,310 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:36,411 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:36,424 - root - INFO - Loading groq LLM...
2025-07-10 06:58:36,415 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.894585609436035, 'init_vectorstore': 7.301348686218262, 'setup_retriever': 0.0041196346282958984, 'load_llm': 6.980360507965088, 'create_chains': 0.40312719345092773}
2025-07-10 06:58:36,418 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:36,611 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 17.86s
2025-07-10 06:58:36,820 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.4974894523620605, 'init_vectorstore': 6.804190635681152, 'setup_retriever': 0.0007960796356201172, 'load_llm': 7.184720516204834, 'create_chains': 0.1093442440032959}
2025-07-10 06:58:36,906 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:36,909 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:37,111 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:37,006 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 16.88s
2025-07-10 06:58:37,118 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:37,114 - root - INFO - Loading groq LLM...
2025-07-10 06:58:37,207 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:37,305 - root - INFO - Loading groq LLM...
2025-07-10 06:58:37,514 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.4836220741271973, 'init_vectorstore': 7.203634262084961, 'setup_retriever': 0.0009219646453857422, 'load_llm': 7.881615161895752, 'create_chains': 0.2031114101409912}
2025-07-10 06:58:37,523 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.79s
2025-07-10 06:58:38,517 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.2881314754486084, 'init_vectorstore': 7.801705837249756, 'setup_retriever': 0.0010573863983154297, 'load_llm': 7.797266960144043, 'create_chains': 0.29596710205078125}
2025-07-10 06:58:38,711 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.39s
2025-07-10 06:58:38,619 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:38,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:39,009 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.7044870853424072, 'init_vectorstore': 7.502258777618408, 'setup_retriever': 0.0012755393981933594, 'load_llm': 6.597256183624268, 'create_chains': 0.18449831008911133}
2025-07-10 06:58:39,016 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 17.09s
2025-07-10 06:58:39,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:39,214 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:39,810 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:40,312 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:40,312 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:40,445 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:40,515 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:40,615 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.2962307929992676, 'init_vectorstore': 8.11658000946045, 'setup_retriever': 0.08456707000732422, 'load_llm': 7.290727853775024, 'create_chains': 0.2071983814239502}
2025-07-10 06:58:40,620 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:40,706 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.09s
2025-07-10 06:58:40,909 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:40,911 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.207289934158325, 'init_vectorstore': 7.890625715255737, 'setup_retriever': 0.0022726058959960938, 'load_llm': 7.787795305252075, 'create_chains': 0.20078110694885254}
2025-07-10 06:58:40,917 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.10s
2025-07-10 06:58:41,277 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:41,310 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:41,408 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:41,316 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:41,409 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:41,414 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.6940410137176514, 'init_vectorstore': 6.511246681213379, 'setup_retriever': 0.08907008171081543, 'load_llm': 7.294550180435181, 'create_chains': 0.2016916275024414}
2025-07-10 06:58:41,607 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.18s
2025-07-10 06:58:41,809 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.9080708026885986, 'init_vectorstore': 7.906235694885254, 'setup_retriever': 0.0008597373962402344, 'load_llm': 7.482170343399048, 'create_chains': 0.2922194004058838}
2025-07-10 06:58:41,818 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.60s
2025-07-10 06:58:42,117 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 1.7951507568359375, 'init_vectorstore': 10.899641752243042, 'setup_retriever': 0.0014009475708007812, 'load_llm': 8.305647373199463, 'create_chains': 0.0856485366821289}
2025-07-10 06:58:42,127 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 21.30s
2025-07-10 06:58:42,165 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:42,215 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:42,208 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:42,208 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:42,208 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:42,209 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:42,214 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:42,523 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:42,525 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:42,521 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:43,111 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:43,111 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 06:58:43,115 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:43,209 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:43,210 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:43,314 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:43,326 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:43,406 - root - INFO - Loading groq LLM...
2025-07-10 06:58:43,515 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.1147758960723877, 'init_vectorstore': 9.27750539779663, 'setup_retriever': 0.004336118698120117, 'load_llm': 6.988906383514404, 'create_chains': 0.10816407203674316}
2025-07-10 06:58:43,517 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.50s
2025-07-10 06:58:43,608 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:43,609 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.7135262489318848, 'init_vectorstore': 7.6001012325286865, 'setup_retriever': 0.08670449256896973, 'load_llm': 8.49558424949646, 'create_chains': 0.09248733520507812}
2025-07-10 06:58:43,610 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:43,611 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:43,618 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.8975162506103516, 'init_vectorstore': 8.582233190536499, 'setup_retriever': 0.09385442733764648, 'load_llm': 6.617861270904541, 'create_chains': 0.09355783462524414}
2025-07-10 06:58:43,619 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 2.9066855907440186, 'init_vectorstore': 8.995203733444214, 'setup_retriever': 0.0015552043914794922, 'load_llm': 7.702331066131592, 'create_chains': 0.09459996223449707}
2025-07-10 06:58:43,627 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.18s
2025-07-10 06:58:43,713 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 18.48s
2025-07-10 06:58:43,715 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 20.19s
2025-07-10 06:58:44,038 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:44,051 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:44,096 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:44,118 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:44,128 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:44,313 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:44,329 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:44,331 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:44,406 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:44,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:44,506 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:44,508 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:44,510 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:44,613 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:44,617 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:44,956 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:44,961 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:44,961 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:44,962 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:44,963 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:44,970 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:45,007 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 06:58:45,113 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:45,216 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:45,216 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:45,217 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:45,218 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:45,239 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:45,240 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:45,358 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:45,365 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:45,416 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 3.6018993854522705, 'init_vectorstore': 13.284060955047607, 'setup_retriever': 0.0025229454040527344, 'load_llm': 2.288299798965454, 'create_chains': 0.01652836799621582}
2025-07-10 06:58:45,418 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 19.29s
2025-07-10 06:58:45,598 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:45,601 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:45,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:45,603 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:45,605 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:45,608 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:45,609 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 06:58:45,609 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:45,611 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:45,626 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '25a56061c982a3a030ae57d547fd9e3f', 'date': 'Thu, 10 Jul 2025 06:58:45 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '33a2b963-5f51-4409-9359-a6f535b55f12', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 227, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '25a56061c982a3a030ae57d547fd9e3f', 'date': 'Thu, 10 Jul 2025 06:58:45 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '16', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '33a2b963-5f51-4409-9359-a6f535b55f12', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:45,831 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:45,833 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:45,926 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:45,927 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:45,928 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:45,929 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:46,013 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:46,031 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7a43a04a99a0127717d3ec8964493325', 'date': 'Thu, 10 Jul 2025 06:58:45 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '319da2d9-53e7-45e2-8aa4-283143a720b2', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7a43a04a99a0127717d3ec8964493325', 'date': 'Thu, 10 Jul 2025 06:58:45 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '319da2d9-53e7-45e2-8aa4-283143a720b2', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:46,177 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 06:58:46,416 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:46,416 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:46,416 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:46,416 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:46,422 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '6d69ce083f9043398ee3abe6828f3eb2', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'fc7ef630-b2ac-460c-be9f-b6581125b723', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '6d69ce083f9043398ee3abe6828f3eb2', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'fc7ef630-b2ac-460c-be9f-b6581125b723', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:46,422 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '99a6e1732cb1927e181b7e20d61325f4', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '45', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '770f05a1-e7eb-457d-8e06-23dbc1da68fd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '99a6e1732cb1927e181b7e20d61325f4', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '45', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '770f05a1-e7eb-457d-8e06-23dbc1da68fd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:46,420 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a21f205eda5eb52279450bed6a11760c', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6cbe8a6e-ffa4-4380-8677-52aedf970c19', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'a21f205eda5eb52279450bed6a11760c', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6cbe8a6e-ffa4-4380-8677-52aedf970c19', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:46,814 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:46,814 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 06:58:46,823 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 06:58:47,007 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 06:58:46,913 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '81c00e2cd7e8b7ebbb45e7feaf4a6848', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'ae11504e-ec4a-4ea6-a9cb-0843ee3a18d4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 220, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '81c00e2cd7e8b7ebbb45e7feaf4a6848', 'date': 'Thu, 10 Jul 2025 06:58:46 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'ae11504e-ec4a-4ea6-a9cb-0843ee3a18d4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 06:58:47,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 06:58:47,513 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:47,516 - root - INFO - HR Assistant started successfully
2025-07-10 06:58:47,530 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:47,606 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:47,614 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 06:58:47,620 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 06:58:47,718 - root - INFO - Model pre-warmed successfully
2025-07-10 06:58:47,721 - root - INFO - HR Assistant started successfully
2025-07-10 00:00:06,867 - root - INFO - Starting HR Assistant...
2025-07-10 00:00:07,002 - root - INFO - Configuration validated successfully
2025-07-10 00:00:07,002 - root - INFO - Pre-warming the model...
2025-07-10 00:00:07,012 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:00:07,013 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:00:07,147 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 00:00:13,199 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:00:14,209 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:00:15,458 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 00:00:15,471 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:00:15,614 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:00:15,614 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:00:15,621 - root - INFO - Loading groq LLM...
2025-07-10 00:00:17,295 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.8449862003326416, 'init_vectorstore': 7.478994369506836, 'setup_retriever': 0.0, 'load_llm': 1.8155567646026611, 'create_chains': 0.006685733795166016}
2025-07-10 00:00:17,295 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 10.28s
2025-07-10 00:00:17,599 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:00:18,100 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 00:00:18,510 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 00:00:18,917 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:00:21,159 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:00:21,182 - root - INFO - Model pre-warmed successfully
2025-07-10 00:00:21,182 - root - INFO - HR Assistant started successfully
2025-07-10 00:00:36,021 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:00:37,087 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:00:37,499 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:00:37,515 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:00:37,516 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:00:37,516 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:00:37,516 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:00:37,691 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:00:37,700 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:00:37,712 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:00:37,717 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:00:40,383 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:00:40,383 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:00:41,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:00:41,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:00:42,058 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:00:42,058 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:00:49,526 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:00:49,526 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:01:48,780 - root - INFO - Starting HR Assistant...
2025-07-10 00:01:48,924 - root - INFO - Configuration validated successfully
2025-07-10 00:01:48,927 - root - INFO - Pre-warming the model...
2025-07-10 00:01:48,927 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:01:48,927 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:01:49,065 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:01:51,627 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:01:52,356 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:01:54,662 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:01:54,809 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:01:54,809 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:01:54,809 - root - INFO - Loading groq LLM...
2025-07-10 00:01:56,416 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.15505266189575195, 'init_vectorstore': 5.4413769245147705, 'setup_retriever': 0.0, 'load_llm': 1.752107858657837, 'create_chains': 0.0017077922821044922}
2025-07-10 00:01:56,419 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 7.49s
2025-07-10 00:01:56,716 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:01:58,529 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:02:01,412 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:02:01,445 - root - INFO - Model pre-warmed successfully
2025-07-10 00:02:01,445 - root - INFO - HR Assistant started successfully
2025-07-10 00:02:06,748 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:02:06,748 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:02:07,539 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:02:07,539 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:02:07,743 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:02:07,743 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:02:07,756 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:02:07,756 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:02:07,756 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:02:07,756 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:02:09,922 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:02:09,922 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:02:37,175 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:02:38,159 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:02:38,488 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:02:38,488 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:02:38,488 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:02:38,488 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:02:38,495 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:02:42,185 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-10 00:02:42,185 - root - INFO - Chain init: 0.00s
2025-07-10 00:02:42,881 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:02:44,599 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:02:46,655 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:02:46,674 - root - INFO - Question processing: 4.49s
2025-07-10 00:02:46,676 - root - INFO - ⏱️ ask_hr took 4.49 seconds
2025-07-10 00:03:07,279 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:03:08,346 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:03:08,539 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:03:08,550 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:03:08,550 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:03:08,550 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:03:08,552 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:03:37,328 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:03:38,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:03:38,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:03:38,738 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:03:38,740 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:03:38,742 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:03:38,742 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:03:57,790 - root - INFO - Received question: M7 LEAVE POLICY ALLOWED TO AN EMPLOY
2025-07-10 00:03:57,790 - root - INFO - Chain init: 0.00s
2025-07-10 00:03:58,577 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:04:00,210 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:04:01,713 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:04:01,720 - root - INFO - Question processing: 3.93s
2025-07-10 00:04:01,720 - root - INFO - ⏱️ ask_hr took 3.93 seconds
2025-07-10 00:04:07,143 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:04:08,384 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:04:08,562 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:04:08,616 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:04:08,621 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:04:08,621 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:04:08,621 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:04:37,256 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:04:38,071 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:04:38,321 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:04:38,328 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:04:38,328 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:04:38,333 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:04:38,333 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:05:08,162 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:05:09,319 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:05:09,812 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:05:09,820 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:05:09,820 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:05:09,820 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:05:09,823 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:05:37,424 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:05:38,840 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:05:39,338 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:05:39,342 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:05:39,342 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:05:39,342 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:05:39,342 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:06:17,093 - root - INFO - Starting HR Assistant...
2025-07-10 00:06:17,269 - root - INFO - Configuration validated successfully
2025-07-10 00:06:17,269 - root - INFO - Pre-warming the model...
2025-07-10 00:06:17,269 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:06:17,269 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:06:17,409 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:06:20,520 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:06:21,833 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:06:24,223 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:06:24,406 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:06:24,409 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:06:24,409 - root - INFO - Loading groq LLM...
2025-07-10 00:06:26,203 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.08391308784484863, 'init_vectorstore': 6.729706525802612, 'setup_retriever': 0.0, 'load_llm': 1.9784045219421387, 'create_chains': 0.0021712779998779297}
2025-07-10 00:06:26,203 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.93s
2025-07-10 00:06:26,448 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:06:28,586 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:06:31,224 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:06:31,251 - root - INFO - Model pre-warmed successfully
2025-07-10 00:06:31,251 - root - INFO - HR Assistant started successfully
2025-07-10 00:06:43,542 - root - INFO - Starting HR Assistant...
2025-07-10 00:06:43,692 - root - INFO - Configuration validated successfully
2025-07-10 00:06:43,692 - root - INFO - Pre-warming the model...
2025-07-10 00:06:43,692 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:06:43,692 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:06:43,848 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:06:47,152 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:06:48,556 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:06:50,454 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:06:50,611 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:06:50,611 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:06:50,611 - root - INFO - Loading groq LLM...
2025-07-10 00:06:52,265 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.04658675193786621, 'init_vectorstore': 6.55929708480835, 'setup_retriever': 0.0, 'load_llm': 1.8051319122314453, 'create_chains': 0.006108283996582031}
2025-07-10 00:06:52,267 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.58s
2025-07-10 00:06:52,520 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:06:54,644 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:06:58,171 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:06:58,190 - root - INFO - Model pre-warmed successfully
2025-07-10 00:06:58,190 - root - INFO - HR Assistant started successfully
2025-07-10 00:07:00,714 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:07:02,210 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:07:02,952 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:07:02,957 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:07:02,957 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:07:02,957 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:07:02,957 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:07:09,005 - root - INFO - Received question: M7 LEAVE POLICY ALLOWED TO AN EMPLOY
2025-07-10 00:07:09,007 - root - INFO - Chain init: 0.00s
2025-07-10 00:07:10,271 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:07:12,660 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:07:14,555 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:07:14,561 - root - INFO - Question processing: 5.55s
2025-07-10 00:07:14,563 - root - INFO - ⏱️ ask_hr took 5.56 seconds
2025-07-10 00:07:39,650 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:07:41,081 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:07:41,499 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:07:41,507 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:07:41,507 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:07:41,507 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:07:41,507 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:07:48,020 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-10 00:07:48,020 - root - INFO - Chain init: 0.00s
2025-07-10 00:07:48,888 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:07:51,114 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:07:53,842 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:07:53,848 - root - INFO - Question processing: 5.83s
2025-07-10 00:07:53,848 - root - INFO - ⏱️ ask_hr took 5.83 seconds
2025-07-10 00:08:40,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:08:41,480 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:08:41,660 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:08:41,672 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:08:41,672 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:08:41,672 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:08:41,674 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:09:07,224 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:09:09,084 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:09:09,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:09:09,635 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:09:09,635 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:09:09,635 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:09:09,637 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:09:34,997 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:09:34,997 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:09:34,997 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:09:34,997 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:09:38,077 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:09:39,729 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:09:40,230 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:10:05,003 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:10:05,005 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:10:05,005 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:10:05,007 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:10:07,887 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:10:09,121 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:10:09,523 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:10:38,187 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:10:39,582 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:10:40,079 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:10:40,087 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:10:40,089 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:10:40,089 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:10:40,089 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:11:07,980 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:11:08,982 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:11:09,377 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:11:09,385 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:11:09,385 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:11:09,385 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:11:09,391 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:11:40,309 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:11:41,381 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:11:42,040 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:11:42,081 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:11:42,085 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:11:42,086 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:11:42,088 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:12:30,838 - root - INFO - Starting HR Assistant...
2025-07-10 00:12:30,982 - root - INFO - Configuration validated successfully
2025-07-10 00:12:30,986 - root - INFO - Pre-warming the model...
2025-07-10 00:12:30,987 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:12:30,987 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:12:31,129 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:12:34,667 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:12:35,714 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:12:37,703 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:12:37,848 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:12:37,849 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:12:37,850 - root - INFO - Loading groq LLM...
2025-07-10 00:12:39,598 - ingestion_retrieval.retrieval - INFO - TIMING - Chain initialization: {'init_embeddings': 0.1072237491607666, 'init_vectorstore': 6.466787576675415, 'setup_retriever': 0.0, 'load_llm': 1.8568265438079834, 'create_chains': 0.038945674896240234}
2025-07-10 00:12:39,600 - ingestion_retrieval.retrieval - INFO - TIMING - Total initialization time: 8.61s
2025-07-10 00:12:39,923 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:12:42,076 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:12:44,793 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:12:44,854 - root - INFO - Model pre-warmed successfully
2025-07-10 00:12:44,854 - root - INFO - HR Assistant started successfully
2025-07-10 00:12:47,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:12:48,559 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:12:49,145 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:12:49,149 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:12:49,149 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:12:49,152 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:12:49,152 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:13:40,345 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:13:41,307 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:13:41,717 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:13:41,722 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:13:41,722 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:13:41,722 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:13:41,722 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:14:09,519 - root - INFO - Received question: WHAT ARE ANNUAL LEAVE FOR M7 GRADE
2025-07-10 00:14:09,519 - root - INFO - Chain init: 0.00s
2025-07-10 00:14:10,611 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:14:12,434 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:14:15,011 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:14:15,027 - root - INFO - Question processing: 5.51s
2025-07-10 00:14:15,029 - root - INFO - ⏱️ ask_hr took 5.51 seconds
2025-07-10 00:14:40,516 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:14:42,405 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:14:42,646 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:14:42,659 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:14:42,694 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:14:42,694 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:14:42,694 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:15:06,512 - root - INFO - Received question: CAN YOU REANSWER MY LAST QUESTION
2025-07-10 00:15:06,512 - root - INFO - Chain init: 0.00s
2025-07-10 00:15:07,175 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:15:09,083 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:15:11,224 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:15:11,228 - root - INFO - Question processing: 4.72s
2025-07-10 00:15:11,228 - root - INFO - ⏱️ ask_hr took 4.72 seconds
2025-07-10 00:15:40,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:15:41,770 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:15:42,103 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:15:42,113 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:15:42,115 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:15:42,117 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:15:42,118 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:16:06,893 - root - INFO - Starting HR Assistant...
2025-07-10 00:16:07,035 - root - INFO - Configuration validated successfully
2025-07-10 00:16:07,035 - root - INFO - Pre-warming the model...
2025-07-10 00:16:07,035 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:16:07,035 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:16:07,176 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:16:09,887 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:16:10,990 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:16:12,973 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:16:13,177 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:16:13,177 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:16:13,179 - root - INFO - Loading groq LLM...
2025-07-10 00:16:14,740 - root - ERROR - Startup failed: Got multiple output keys: dict_keys([]), cannot determine which to store in memory. Please set the 'output_key' explicitly.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 301, in chain_with_memory
    memory.save_context({"input": inputs["input"]}, {})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\memory\chat_memory.py", line 72, in save_context
    input_str, output_str = self._get_input_output(inputs, outputs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\memory\chat_memory.py", line 61, in _get_input_output
    raise ValueError(
ValueError: Got multiple output keys: dict_keys([]), cannot determine which to store in memory. Please set the 'output_key' explicitly.
2025-07-10 00:16:27,016 - root - INFO - Starting HR Assistant...
2025-07-10 00:16:27,171 - root - INFO - Configuration validated successfully
2025-07-10 00:16:27,171 - root - INFO - Pre-warming the model...
2025-07-10 00:16:27,171 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:16:27,171 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:16:27,313 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:16:29,659 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:16:30,749 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:16:32,424 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:16:32,586 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:16:32,586 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:16:32,586 - root - INFO - Loading groq LLM...
2025-07-10 00:16:34,195 - root - ERROR - Startup failed: Got multiple output keys: dict_keys([]), cannot determine which to store in memory. Please set the 'output_key' explicitly.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 301, in chain_with_memory
    memory.save_context({"input": inputs["input"]}, {})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\memory\chat_memory.py", line 72, in save_context
    input_str, output_str = self._get_input_output(inputs, outputs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\memory\chat_memory.py", line 61, in _get_input_output
    raise ValueError(
ValueError: Got multiple output keys: dict_keys([]), cannot determine which to store in memory. Please set the 'output_key' explicitly.
2025-07-10 00:17:13,162 - root - INFO - Starting HR Assistant...
2025-07-10 00:17:13,303 - root - INFO - Configuration validated successfully
2025-07-10 00:17:13,315 - root - INFO - Pre-warming the model...
2025-07-10 00:17:13,315 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:17:13,315 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:17:13,467 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:17:16,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:17:17,572 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:17:21,617 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:17:21,766 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:17:21,766 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:17:21,768 - root - INFO - Loading groq LLM...
2025-07-10 00:17:24,771 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:17:26,722 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:17:29,359 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:17:29,390 - root - INFO - Model pre-warmed successfully
2025-07-10 00:17:29,392 - root - INFO - HR Assistant started successfully
2025-07-10 00:17:31,664 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:17:32,499 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:17:32,742 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:17:32,745 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:17:32,745 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:17:32,745 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:17:32,745 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:17:40,576 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:17:41,451 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:17:41,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:17:41,751 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:17:41,751 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:17:41,751 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:17:41,751 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:18:09,572 - root - INFO - Received question: WHAT ARE ANNUAL LEAVE FOR N1 GRADE
2025-07-10 00:18:09,572 - root - INFO - Chain init: 0.00s
2025-07-10 00:18:10,937 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:18:13,413 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:18:15,346 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:18:15,351 - root - INFO - Question processing: 5.78s
2025-07-10 00:18:15,351 - root - INFO - ⏱️ ask_hr took 5.78 seconds
2025-07-10 00:18:41,163 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:18:41,921 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:18:42,236 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:18:42,244 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:18:42,244 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:18:42,244 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:18:42,244 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:18:58,490 - root - INFO - Received question: CAN YOU REANSWER MY PREVIOUS QUESTION DETAIL
2025-07-10 00:18:58,490 - root - INFO - Chain init: 0.00s
2025-07-10 00:19:00,149 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:19:02,661 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:19:06,032 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:19:06,053 - root - INFO - Question processing: 7.56s
2025-07-10 00:19:06,054 - root - INFO - ⏱️ ask_hr took 7.56 seconds
2025-07-10 00:19:40,845 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:19:42,655 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:19:43,144 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:19:43,155 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:19:43,155 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:19:43,157 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:19:43,157 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:19:58,021 - root - INFO - Starting HR Assistant...
2025-07-10 00:19:58,180 - root - INFO - Configuration validated successfully
2025-07-10 00:19:58,180 - root - INFO - Pre-warming the model...
2025-07-10 00:19:58,180 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:19:58,180 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:19:58,321 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:20:01,107 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:20:02,670 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:20:04,564 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:20:04,725 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:20:04,728 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:20:04,728 - root - INFO - Loading groq LLM...
2025-07-10 00:20:06,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:20:09,162 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:20:12,876 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:20:12,900 - root - INFO - Model pre-warmed successfully
2025-07-10 00:20:12,901 - root - INFO - HR Assistant started successfully
2025-07-10 00:20:40,608 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:20:42,664 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:20:43,324 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:20:43,324 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:20:43,334 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:20:43,334 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:20:43,334 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:20:44,115 - root - INFO - Received question: WHAT ARE ANNUAL LEAVE FOR M7 GRADE
2025-07-10 00:20:44,115 - root - INFO - Chain init: 0.00s
2025-07-10 00:20:45,065 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:20:47,111 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:20:48,748 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:20:48,758 - root - INFO - Question processing: 4.64s
2025-07-10 00:20:48,760 - root - INFO - ⏱️ ask_hr took 4.64 seconds
2025-07-10 00:21:46,816 - root - INFO - Starting HR Assistant...
2025-07-10 00:21:46,954 - root - INFO - Configuration validated successfully
2025-07-10 00:21:46,954 - root - INFO - Pre-warming the model...
2025-07-10 00:21:46,954 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:21:46,954 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:21:47,089 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:21:49,406 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:21:50,306 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:21:52,440 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:21:52,589 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:21:52,589 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:21:52,589 - root - INFO - Loading groq LLM...
2025-07-10 00:21:54,558 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:21:56,473 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:21:58,744 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:21:58,762 - root - INFO - Model pre-warmed successfully
2025-07-10 00:21:58,762 - root - INFO - HR Assistant started successfully
2025-07-10 00:22:00,870 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:22:01,670 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:22:01,859 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:22:01,865 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:22:01,865 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:22:01,865 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:22:01,865 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:22:40,405 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:22:41,428 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:22:41,640 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:22:41,645 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:22:41,647 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:22:41,647 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:22:41,647 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:23:40,638 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:23:41,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:23:41,957 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:23:41,964 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:23:41,964 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:23:41,967 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:23:41,968 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:24:40,387 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:24:41,822 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:24:42,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:24:42,116 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:24:42,116 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:24:42,116 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:24:42,116 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:24:42,720 - root - INFO - Received question: excess leave salary deduction formula
2025-07-10 00:24:42,726 - root - INFO - Chain init: 0.00s
2025-07-10 00:24:43,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:24:46,056 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:24:48,451 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:24:48,455 - root - INFO - Question processing: 5.73s
2025-07-10 00:24:48,457 - root - INFO - ⏱️ ask_hr took 5.74 seconds
2025-07-10 00:25:13,837 - root - INFO - Received question: what if my salary is 200k  and i have 4 excess leave
2025-07-10 00:25:13,837 - root - INFO - Chain init: 0.00s
2025-07-10 00:25:14,830 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:25:17,135 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:25:19,280 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:25:19,282 - root - INFO - Question processing: 5.45s
2025-07-10 00:25:19,282 - root - INFO - ⏱️ ask_hr took 5.45 seconds
2025-07-10 00:25:40,014 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:25:41,283 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 00:25:41,664 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:25:41,672 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:25:41,672 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:25:41,672 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:25:41,675 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:37,023 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:37,083 - root - INFO - Configuration validated successfully
2025-07-10 07:33:37,085 - root - INFO - Pre-warming the model...
2025-07-10 07:33:37,087 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:37,089 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:37,103 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:37,303 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:37,381 - root - INFO - Configuration validated successfully
2025-07-10 07:33:37,382 - root - INFO - Pre-warming the model...
2025-07-10 07:33:37,384 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:37,385 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:37,389 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:37,395 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:37,490 - root - INFO - Configuration validated successfully
2025-07-10 07:33:37,500 - root - INFO - Pre-warming the model...
2025-07-10 07:33:37,503 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:37,511 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:37,600 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:37,899 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:38,081 - root - INFO - Configuration validated successfully
2025-07-10 07:33:38,082 - root - INFO - Pre-warming the model...
2025-07-10 07:33:38,083 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:38,084 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:38,093 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:38,393 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:38,583 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:38,588 - root - INFO - Configuration validated successfully
2025-07-10 07:33:38,681 - root - INFO - Pre-warming the model...
2025-07-10 07:33:38,685 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:38,699 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:38,785 - root - INFO - Configuration validated successfully
2025-07-10 07:33:38,791 - root - INFO - Pre-warming the model...
2025-07-10 07:33:38,880 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:38,881 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:38,884 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:38,988 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:39,287 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:39,384 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:39,586 - root - INFO - Configuration validated successfully
2025-07-10 07:33:39,589 - root - INFO - Pre-warming the model...
2025-07-10 07:33:39,586 - root - INFO - Configuration validated successfully
2025-07-10 07:33:39,592 - root - INFO - Pre-warming the model...
2025-07-10 07:33:39,595 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:39,592 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:39,597 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:39,596 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:39,687 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:39,688 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:39,885 - root - INFO - Configuration validated successfully
2025-07-10 07:33:39,985 - root - INFO - Pre-warming the model...
2025-07-10 07:33:39,885 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:40,080 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:40,095 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:40,095 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:40,098 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:40,188 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:40,195 - root - INFO - Configuration validated successfully
2025-07-10 07:33:40,293 - root - INFO - Pre-warming the model...
2025-07-10 07:33:40,294 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:40,295 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:40,383 - root - INFO - Configuration validated successfully
2025-07-10 07:33:40,389 - root - INFO - Pre-warming the model...
2025-07-10 07:33:40,301 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:40,395 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:40,486 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:40,484 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:40,593 - root - INFO - Configuration validated successfully
2025-07-10 07:33:40,595 - root - INFO - Pre-warming the model...
2025-07-10 07:33:40,596 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:40,597 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:40,593 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:40,794 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:41,895 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:42,083 - root - INFO - Configuration validated successfully
2025-07-10 07:33:42,090 - root - INFO - Pre-warming the model...
2025-07-10 07:33:42,182 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:42,185 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:42,297 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:42,300 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:42,297 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:42,580 - root - INFO - Configuration validated successfully
2025-07-10 07:33:42,587 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:42,592 - root - INFO - Configuration validated successfully
2025-07-10 07:33:42,589 - root - INFO - Pre-warming the model...
2025-07-10 07:33:42,593 - root - INFO - Pre-warming the model...
2025-07-10 07:33:42,596 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:42,594 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:42,599 - root - INFO - Configuration validated successfully
2025-07-10 07:33:42,596 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:42,599 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:42,681 - root - INFO - Pre-warming the model...
2025-07-10 07:33:42,688 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:42,694 - root - INFO - Starting HR Assistant...
2025-07-10 07:33:42,780 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:42,781 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:42,782 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:42,885 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:42,890 - root - INFO - Configuration validated successfully
2025-07-10 07:33:42,985 - root - INFO - Pre-warming the model...
2025-07-10 07:33:43,080 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:33:43,093 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:33:43,083 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:43,085 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:43,089 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:43,183 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:43,281 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:33:43,983 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:43,984 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:44,724 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:44,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:44,784 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:44,792 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:45,084 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:45,087 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:45,385 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:45,388 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:45,389 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:45,394 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:45,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:45,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:45,958 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:46,025 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:46,032 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:46,037 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:46,039 - root - INFO - Loading groq LLM...
2025-07-10 07:33:46,465 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:46,468 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:46,475 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:46,475 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:46,489 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:46,491 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:46,494 - root - INFO - Loading groq LLM...
2025-07-10 07:33:46,719 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:46,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:46,722 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:46,727 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:46,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:46,729 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:46,729 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:46,735 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:46,740 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:46,745 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:46,747 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:46,749 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:46,794 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:46,783 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:46,796 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:46,790 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:46,795 - root - INFO - Loading groq LLM...
2025-07-10 07:33:46,798 - root - INFO - Loading groq LLM...
2025-07-10 07:33:46,882 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:46,887 - root - INFO - Loading groq LLM...
2025-07-10 07:33:47,285 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:47,296 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:47,298 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:47,299 - root - INFO - Loading groq LLM...
2025-07-10 07:33:47,696 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:47,694 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:47,782 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:48,081 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:48,087 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:48,083 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:48,089 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:48,191 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:48,194 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:48,196 - root - INFO - Loading groq LLM...
2025-07-10 07:33:48,602 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:48,687 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:48,690 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:48,789 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:48,793 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:48,795 - root - INFO - Loading groq LLM...
2025-07-10 07:33:48,801 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:48,809 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:48,885 - root - INFO - Loading groq LLM...
2025-07-10 07:33:49,198 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,289 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,481 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,481 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:49,482 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:49,490 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:49,496 - root - INFO - Loading groq LLM...
2025-07-10 07:33:49,493 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:49,499 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:49,586 - root - INFO - Loading groq LLM...
2025-07-10 07:33:49,590 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:49,597 - root - INFO - Loading groq LLM...
2025-07-10 07:33:49,784 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,790 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,786 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,795 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,885 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:49,900 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:49,887 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:49,980 - root - INFO - Loading groq LLM...
2025-07-10 07:33:49,983 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:49,986 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:49,988 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:49,994 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:33:49,996 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:50,180 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:50,389 - root - INFO - Loading groq LLM...
2025-07-10 07:33:50,197 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:50,392 - root - INFO - Loading groq LLM...
2025-07-10 07:33:50,289 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:50,490 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:50,381 - root - INFO - Loading groq LLM...
2025-07-10 07:33:50,492 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:50,686 - root - INFO - Loading groq LLM...
2025-07-10 07:33:51,184 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:51,186 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:52,082 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:52,482 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:52,785 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:52,889 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:53,187 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:53,792 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:53,795 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:54,086 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:54,271 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:54,272 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:54,281 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:54,272 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:54,286 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:54,484 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:33:54,686 - root - INFO - Model pre-warmed successfully
2025-07-10 07:33:54,687 - root - INFO - HR Assistant started successfully
2025-07-10 07:33:54,883 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:33:54,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:55,162 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:55,187 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:55,389 - root - INFO - Model pre-warmed successfully
2025-07-10 07:33:55,391 - root - INFO - HR Assistant started successfully
2025-07-10 07:33:55,588 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:55,591 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:55,593 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:33:55,595 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:33:56,011 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:56,015 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:56,086 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:56,085 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:56,581 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:56,609 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:33:56,683 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:56,697 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:57,187 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:57,582 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:33:57,587 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:58,686 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:33:58,882 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:33:59,081 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:34:00,187 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:00,286 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:00,782 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:00,784 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:00,892 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:00,897 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:00,983 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:01,090 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:01,283 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:01,586 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:01,589 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:01,682 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:01,691 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:01,991 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:01,993 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:02,769 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:02,800 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:02,801 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:02,943 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:02,970 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:02,971 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,468 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,468 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,510 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,510 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,511 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,512 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,727 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:34:03,802 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,806 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,884 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,884 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,887 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,889 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,893 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,893 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,898 - root - INFO - Model pre-warmed successfully
2025-07-10 07:34:03,898 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,905 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:03,907 - root - INFO - HR Assistant started successfully
2025-07-10 07:34:38,027 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:34:38,029 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:34:38,031 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:34:38,033 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:34:39,286 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:34:39,982 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:34:40,287 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:35:38,050 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:35:38,052 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:35:38,054 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:35:38,055 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:35:39,669 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:35:40,894 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:35:41,305 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:36:38,089 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:36:38,098 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:36:38,099 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:36:38,100 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:36:39,263 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:36:40,749 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:36:41,075 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:37:38,029 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:37:38,032 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:37:38,033 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:37:38,034 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:37:39,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:37:40,017 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:37:40,214 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:38:38,048 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:38:38,050 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:38:38,051 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:38:38,052 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:38:39,158 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:38:39,891 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:38:40,112 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:39:38,005 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:39:38,007 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:39:38,008 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:39:38,009 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:39:39,099 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:39:40,027 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:39:40,661 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:40:38,028 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:40:38,029 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:40:38,030 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:40:38,032 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:40:39,530 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:40:40,851 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:40:41,181 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:41:38,010 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:41:38,012 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:41:38,014 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:41:38,015 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:41:39,300 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:41:40,271 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:41:40,576 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:42:38,002 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:42:38,003 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:42:38,005 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:42:38,006 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:42:38,869 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:42:39,764 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:42:40,093 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:43:38,003 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:43:38,008 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:43:38,009 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:43:38,011 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:43:39,635 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:43:40,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:43:40,989 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:44:38,010 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:44:38,011 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:44:38,013 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:44:38,014 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:44:39,697 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:44:40,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:44:40,773 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:45:38,002 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:45:38,003 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:45:38,004 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:45:38,006 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:45:39,144 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:45:40,477 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:45:40,730 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:46:38,013 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:46:38,015 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:46:38,016 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:46:38,018 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:46:39,075 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:46:40,393 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:46:40,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:47:38,038 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:47:38,039 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:47:38,040 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:47:38,041 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:47:39,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:47:40,469 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:47:40,653 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:48:30,194 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:30,199 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:30,201 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:30,202 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:31,587 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:48:32,597 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:48:32,796 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:48:33,952 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:33,955 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:33,957 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:33,962 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:35,053 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:48:35,253 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:35,259 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:35,262 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:35,268 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:35,671 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:48:35,887 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:48:36,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:48:36,578 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:48:36,578 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:48:36,900 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:48:36,945 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:48:37,460 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:48:38,270 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:48:38,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:48:39,085 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:48:47,167 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:47,169 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:51,917 - admin_interface - INFO - Successfully updated .env file
2025-07-10 07:48:51,928 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:48:51,930 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:48:51,991 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:48:51,993 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:48:52,019 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:52,021 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:52,022 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:48:52,025 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:48:57,848 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:48:57,850 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:30,483 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:30,490 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:30,775 - root - INFO - Configuration validated successfully
2025-07-10 07:49:30,776 - root - INFO - Pre-warming the model...
2025-07-10 07:49:30,779 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:30,780 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:30,795 - root - INFO - Configuration validated successfully
2025-07-10 07:49:30,796 - root - INFO - Pre-warming the model...
2025-07-10 07:49:30,799 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:30,866 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:30,985 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:31,167 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:31,171 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:31,391 - root - INFO - Configuration validated successfully
2025-07-10 07:49:31,470 - root - INFO - Pre-warming the model...
2025-07-10 07:49:31,482 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:31,487 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:31,987 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:32,066 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:32,474 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:32,677 - root - INFO - Configuration validated successfully
2025-07-10 07:49:32,679 - root - INFO - Pre-warming the model...
2025-07-10 07:49:32,682 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:32,684 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:33,068 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:33,367 - root - INFO - Configuration validated successfully
2025-07-10 07:49:33,374 - root - INFO - Pre-warming the model...
2025-07-10 07:49:33,381 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:33,466 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:33,666 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:34,084 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:34,168 - root - INFO - Configuration validated successfully
2025-07-10 07:49:34,184 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:34,266 - root - INFO - Pre-warming the model...
2025-07-10 07:49:34,378 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:34,468 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:34,475 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:34,873 - root - INFO - Configuration validated successfully
2025-07-10 07:49:34,884 - root - INFO - Pre-warming the model...
2025-07-10 07:49:34,968 - root - INFO - Configuration validated successfully
2025-07-10 07:49:34,970 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:35,066 - root - INFO - Pre-warming the model...
2025-07-10 07:49:35,166 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:35,073 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:35,173 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:35,467 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:35,273 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:35,873 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:36,169 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:36,565 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:36,666 - root - INFO - Configuration validated successfully
2025-07-10 07:49:36,763 - root - INFO - Pre-warming the model...
2025-07-10 07:49:36,764 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:36,862 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:36,966 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:36,873 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:36,965 - root - INFO - Configuration validated successfully
2025-07-10 07:49:37,163 - root - INFO - Pre-warming the model...
2025-07-10 07:49:37,075 - root - INFO - Configuration validated successfully
2025-07-10 07:49:37,271 - root - INFO - Pre-warming the model...
2025-07-10 07:49:37,263 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:37,376 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:37,361 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:37,560 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:37,968 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:37,966 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:38,168 - root - INFO - Configuration validated successfully
2025-07-10 07:49:38,175 - root - INFO - Pre-warming the model...
2025-07-10 07:49:38,360 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:38,367 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:38,365 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:38,665 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:38,762 - root - INFO - Configuration validated successfully
2025-07-10 07:49:38,860 - root - INFO - Pre-warming the model...
2025-07-10 07:49:38,963 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:38,962 - root - INFO - Configuration validated successfully
2025-07-10 07:49:39,064 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:38,969 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:39,065 - root - INFO - Pre-warming the model...
2025-07-10 07:49:39,160 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:39,062 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:39,260 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:39,263 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:39,264 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:39,569 - root - INFO - Configuration validated successfully
2025-07-10 07:49:39,663 - root - INFO - Pre-warming the model...
2025-07-10 07:49:39,670 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:39,760 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:39,778 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:39,766 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:39,962 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:40,368 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:40,570 - root - INFO - Starting HR Assistant...
2025-07-10 07:49:40,678 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:41,160 - root - INFO - Configuration validated successfully
2025-07-10 07:49:41,167 - root - INFO - Pre-warming the model...
2025-07-10 07:49:41,180 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:41,266 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:41,762 - root - INFO - Configuration validated successfully
2025-07-10 07:49:41,860 - root - INFO - Pre-warming the model...
2025-07-10 07:49:41,867 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:49:41,870 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:49:41,962 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:42,167 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:42,364 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:49:43,278 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:43,566 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:43,600 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:43,964 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,020 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,184 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,187 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,199 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,468 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,508 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:44,511 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,528 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:44,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:44,761 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:45,032 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:45,033 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:45,034 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:45,035 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:49:45,046 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,045 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,056 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:45,061 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:45,201 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:45,210 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:45,291 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:45,301 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:45,303 - root - INFO - Loading google LLM...
2025-07-10 07:49:45,312 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:45,313 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:45,316 - root - INFO - Loading google LLM...
2025-07-10 07:49:45,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,413 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,414 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,421 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,422 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:45,572 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:45,660 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:45,682 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:45,694 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,696 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,769 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:45,771 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:45,776 - root - INFO - Loading google LLM...
2025-07-10 07:49:45,874 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:45,882 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:45,883 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:45,884 - root - INFO - Loading google LLM...
2025-07-10 07:49:46,006 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,006 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,007 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,007 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,342 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,342 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,344 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:46,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,344 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:46,381 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:46,385 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:46,511 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:46,516 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:46,518 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:46,521 - root - INFO - Loading google LLM...
2025-07-10 07:49:46,521 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:46,523 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:46,525 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:46,529 - root - INFO - Loading google LLM...
2025-07-10 07:49:46,640 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:46,740 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:46,764 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:46,770 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:46,779 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:46,860 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:46,860 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:46,862 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:46,862 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:46,862 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:46,864 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:46,864 - root - INFO - Loading google LLM...
2025-07-10 07:49:46,865 - root - INFO - Loading google LLM...
2025-07-10 07:49:46,866 - root - INFO - Loading google LLM...
2025-07-10 07:49:47,030 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,061 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,065 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,061 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,061 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,062 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:47,063 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,064 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,066 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:47,088 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,168 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,188 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,267 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,267 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,364 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:47,366 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,367 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:47,368 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:47,369 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,372 - root - INFO - Loading google LLM...
2025-07-10 07:49:47,373 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:47,379 - root - INFO - Loading google LLM...
2025-07-10 07:49:47,381 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:47,382 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:47,383 - root - INFO - Loading google LLM...
2025-07-10 07:49:47,480 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:47,484 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:47,487 - root - INFO - Loading google LLM...
2025-07-10 07:49:47,526 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:47,526 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:47,535 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,561 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,562 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:47,566 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:47,566 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,561 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,566 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,568 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,666 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,568 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,568 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:47,583 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:47,761 - root - INFO - Loading google LLM...
2025-07-10 07:49:47,771 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,780 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:49:47,764 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,067 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:48,071 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:48,073 - root - INFO - Loading google LLM...
2025-07-10 07:49:48,083 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,084 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,084 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:48,085 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:49:48,087 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:48,160 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:49:48,163 - root - INFO - Loading google LLM...
2025-07-10 07:49:48,164 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,167 - root - INFO - Loading google LLM...
2025-07-10 07:49:48,374 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,398 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,842 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:48,843 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:48,843 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:48,843 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:48,844 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:48,844 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:48,845 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,845 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,845 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:48,845 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:48,845 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:48,846 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:48,847 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:49:50,020 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:50,019 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:50,024 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:50,048 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:50,108 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:50,108 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:50,118 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:50,119 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:50,167 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:50,167 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:50,172 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:49:50,030 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c3b81ea666bbfb72d88e222d573d7881', 'date': 'Thu, 10 Jul 2025 07:49:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '225a5254-9734-4372-b9f9-ad0f8374d48d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c3b81ea666bbfb72d88e222d573d7881', 'date': 'Thu, 10 Jul 2025 07:49:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '225a5254-9734-4372-b9f9-ad0f8374d48d', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:50,174 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '80ec89c648c211794a69e652f6272ab8', 'date': 'Thu, 10 Jul 2025 07:49:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '18', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5c567612-e14e-4ef1-9977-42b4d4e19143', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '80ec89c648c211794a69e652f6272ab8', 'date': 'Thu, 10 Jul 2025 07:49:48 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '18', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '5c567612-e14e-4ef1-9977-42b4d4e19143', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:50,469 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:51,123 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:51,161 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:51,192 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:49:51,171 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '465d2c8409a3767f45fc6fd229d3c50d', 'date': 'Thu, 10 Jul 2025 07:49:49 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bb0db217-397c-47c6-9f4a-ff12d7a3cb3b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 642, in max_marginal_relevance_search
    self._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '465d2c8409a3767f45fc6fd229d3c50d', 'date': 'Thu, 10 Jul 2025 07:49:49 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bb0db217-397c-47c6-9f4a-ff12d7a3cb3b', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:53,118 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:53,120 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:53,121 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:53,121 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:53,123 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:49:53,131 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5070c8d4051ee3dc01f615314cfe365f', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '20804c01-bbf0-4f3e-b122-ba6e307ead92', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 242, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5070c8d4051ee3dc01f615314cfe365f', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '20804c01-bbf0-4f3e-b122-ba6e307ead92', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:53,130 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1b1fb413c2d00e031b61172fe059243f', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '40339a53-a61a-4352-b089-4877f90fc8e9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 242, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1b1fb413c2d00e031b61172fe059243f', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '40339a53-a61a-4352-b089-4877f90fc8e9', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:53,130 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '2bbbcac5d26954fd0e71c57b9931a459', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a1075974-c841-4a1d-ae09-513fc82fc82e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 242, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '2bbbcac5d26954fd0e71c57b9931a459', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a1075974-c841-4a1d-ae09-513fc82fc82e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:53,131 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'be86f86976c080e6c085896be7205c08', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '65eca4ca-d3d9-400b-b982-1f602dbb05cd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 242, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'be86f86976c080e6c085896be7205c08', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '65eca4ca-d3d9-400b-b982-1f602dbb05cd', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:53,163 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4790cedc2ca907c3c5630001d7e4b1a2', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b125befc-5b3b-4576-bbe5-bf8c559c9d16', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/vectorstores/base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 650, in max_marginal_relevance_search
    query_embedding = self.embeddings.embed_query(query)
  File "/app/ingestion_retrieval/retrieval.py", line 242, in embed_query
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4790cedc2ca907c3c5630001d7e4b1a2', 'date': 'Thu, 10 Jul 2025 07:49:50 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'b125befc-5b3b-4576-bbe5-bf8c559c9d16', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:49:56,041 - root - INFO - Model pre-warmed successfully
2025-07-10 07:49:56,042 - root - INFO - HR Assistant started successfully
2025-07-10 07:50:12,065 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:12,128 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:12,195 - root - INFO - Configuration validated successfully
2025-07-10 07:50:12,197 - root - INFO - Pre-warming the model...
2025-07-10 07:50:12,199 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:12,202 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:12,497 - root - INFO - Configuration validated successfully
2025-07-10 07:50:12,499 - root - INFO - Pre-warming the model...
2025-07-10 07:50:12,502 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:12,504 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:12,517 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:12,519 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:12,806 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:13,000 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:13,008 - root - INFO - Configuration validated successfully
2025-07-10 07:50:13,094 - root - INFO - Pre-warming the model...
2025-07-10 07:50:13,098 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:13,103 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:13,103 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:13,306 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:13,397 - root - INFO - Configuration validated successfully
2025-07-10 07:50:13,401 - root - INFO - Pre-warming the model...
2025-07-10 07:50:13,405 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:13,414 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:13,795 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:13,804 - root - INFO - Configuration validated successfully
2025-07-10 07:50:13,899 - root - INFO - Pre-warming the model...
2025-07-10 07:50:13,903 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:13,907 - root - INFO - Configuration validated successfully
2025-07-10 07:50:13,907 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:13,909 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:13,996 - root - INFO - Pre-warming the model...
2025-07-10 07:50:14,009 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:14,003 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:14,013 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:14,195 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:14,496 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:14,707 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:14,706 - root - INFO - Configuration validated successfully
2025-07-10 07:50:14,894 - root - INFO - Pre-warming the model...
2025-07-10 07:50:14,802 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:14,910 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:15,094 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:14,907 - root - INFO - Configuration validated successfully
2025-07-10 07:50:15,104 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:15,303 - root - INFO - Pre-warming the model...
2025-07-10 07:50:15,494 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:15,498 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:15,503 - root - INFO - Configuration validated successfully
2025-07-10 07:50:15,601 - root - INFO - Pre-warming the model...
2025-07-10 07:50:15,611 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:15,702 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:15,703 - root - INFO - Configuration validated successfully
2025-07-10 07:50:15,707 - root - INFO - Pre-warming the model...
2025-07-10 07:50:15,707 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:15,796 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:15,893 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:15,998 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:16,105 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:16,304 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:16,301 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:16,498 - root - INFO - Configuration validated successfully
2025-07-10 07:50:16,594 - root - INFO - Pre-warming the model...
2025-07-10 07:50:16,508 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:16,602 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:16,696 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:16,702 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:17,501 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:17,602 - root - INFO - Configuration validated successfully
2025-07-10 07:50:17,697 - root - INFO - Pre-warming the model...
2025-07-10 07:50:17,795 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:17,800 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:18,508 - root - INFO - Configuration validated successfully
2025-07-10 07:50:18,602 - root - INFO - Pre-warming the model...
2025-07-10 07:50:18,498 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:18,604 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:18,610 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:18,613 - root - INFO - Configuration validated successfully
2025-07-10 07:50:18,708 - root - INFO - Pre-warming the model...
2025-07-10 07:50:18,795 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:18,798 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:18,795 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:18,801 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:19,101 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:19,308 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:19,498 - root - INFO - Configuration validated successfully
2025-07-10 07:50:19,500 - root - INFO - Pre-warming the model...
2025-07-10 07:50:19,598 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:19,608 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:19,812 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:20,008 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:20,201 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:20,197 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:20,197 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:20,511 - root - INFO - Configuration validated successfully
2025-07-10 07:50:20,596 - root - INFO - Configuration validated successfully
2025-07-10 07:50:20,604 - root - INFO - Pre-warming the model...
2025-07-10 07:50:20,599 - root - INFO - Pre-warming the model...
2025-07-10 07:50:20,698 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:20,703 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:20,711 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:20,704 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:20,896 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:20,804 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:21,396 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:21,397 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:21,694 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:21,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:21,713 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:22,818 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:22,820 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:23,145 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:23,148 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:23,148 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:23,401 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:23,401 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:23,405 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '824a7e75fb22b1447fa4ec813819599a', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bff7b5c4-3a86-4333-9dd4-36e63f368382', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '824a7e75fb22b1447fa4ec813819599a', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bff7b5c4-3a86-4333-9dd4-36e63f368382', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:23,406 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0e5bb438b06bbc5297b72e68c80f2456', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '024b741a-a9b6-420f-a626-1616ebee75f8', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0e5bb438b06bbc5297b72e68c80f2456', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '024b741a-a9b6-420f-a626-1616ebee75f8', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:23,416 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '824a7e75fb22b1447fa4ec813819599a', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bff7b5c4-3a86-4333-9dd4-36e63f368382', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '824a7e75fb22b1447fa4ec813819599a', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bff7b5c4-3a86-4333-9dd4-36e63f368382', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:23,417 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0e5bb438b06bbc5297b72e68c80f2456', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '024b741a-a9b6-420f-a626-1616ebee75f8', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '0e5bb438b06bbc5297b72e68c80f2456', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '024b741a-a9b6-420f-a626-1616ebee75f8', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:23,800 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,028 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,110 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:24,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,367 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,465 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,518 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:24,527 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f36401e9fcb72299397354f933d6a7d0', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6b617185-d00b-475e-a749-da08be9a1cc4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f36401e9fcb72299397354f933d6a7d0', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6b617185-d00b-475e-a749-da08be9a1cc4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:24,548 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f36401e9fcb72299397354f933d6a7d0', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6b617185-d00b-475e-a749-da08be9a1cc4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'f36401e9fcb72299397354f933d6a7d0', 'date': 'Thu, 10 Jul 2025 07:50:23 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6b617185-d00b-475e-a749-da08be9a1cc4', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:24,651 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:24,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,701 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,703 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:24,705 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:50:25,206 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:25,215 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:25,299 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8baa79b61a9453a5bc287389b9e291f', 'date': 'Thu, 10 Jul 2025 07:50:24 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bd0fe42b-7a1e-4aa9-92a9-a4e1704002e5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8baa79b61a9453a5bc287389b9e291f', 'date': 'Thu, 10 Jul 2025 07:50:24 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bd0fe42b-7a1e-4aa9-92a9-a4e1704002e5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:25,331 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8baa79b61a9453a5bc287389b9e291f', 'date': 'Thu, 10 Jul 2025 07:50:24 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bd0fe42b-7a1e-4aa9-92a9-a4e1704002e5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'd8baa79b61a9453a5bc287389b9e291f', 'date': 'Thu, 10 Jul 2025 07:50:24 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '7', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'bd0fe42b-7a1e-4aa9-92a9-a4e1704002e5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:25,607 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:25,616 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:25,619 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:25,617 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:25,699 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '74f63c1e71f8dea19590a9586b78b032', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '19', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '30db29d5-d542-43c3-b279-a2c150819d13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '74f63c1e71f8dea19590a9586b78b032', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '19', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '30db29d5-d542-43c3-b279-a2c150819d13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:25,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:25,797 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '74f63c1e71f8dea19590a9586b78b032', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '19', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '30db29d5-d542-43c3-b279-a2c150819d13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '74f63c1e71f8dea19590a9586b78b032', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '19', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '30db29d5-d542-43c3-b279-a2c150819d13', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:25,916 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:26,001 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:26,303 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:26,298 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:26,305 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:26,308 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:26,311 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:50:26,402 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fa02f1e5dab809734b3bd5f38f2abf19', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7e6bd4a-480b-4159-9d70-c25a98771eeb', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fa02f1e5dab809734b3bd5f38f2abf19', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7e6bd4a-480b-4159-9d70-c25a98771eeb', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:26,494 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fa02f1e5dab809734b3bd5f38f2abf19', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7e6bd4a-480b-4159-9d70-c25a98771eeb', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'fa02f1e5dab809734b3bd5f38f2abf19', 'date': 'Thu, 10 Jul 2025 07:50:25 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd7e6bd4a-480b-4159-9d70-c25a98771eeb', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,195 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,198 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,210 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dfb471a63a88caa16d6c348c625a8117', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0089f8a8-2238-4492-8d77-9f22bef9b4a0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dfb471a63a88caa16d6c348c625a8117', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0089f8a8-2238-4492-8d77-9f22bef9b4a0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,318 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dfb471a63a88caa16d6c348c625a8117', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0089f8a8-2238-4492-8d77-9f22bef9b4a0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'dfb471a63a88caa16d6c348c625a8117', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0089f8a8-2238-4492-8d77-9f22bef9b4a0', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,219 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '73dc383d393e88272b69ac74778d1e8b', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09a9c65a-95b8-4ce3-a828-b887921823b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '73dc383d393e88272b69ac74778d1e8b', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09a9c65a-95b8-4ce3-a828-b887921823b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,433 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,417 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '73dc383d393e88272b69ac74778d1e8b', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09a9c65a-95b8-4ce3-a828-b887921823b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '73dc383d393e88272b69ac74778d1e8b', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '9', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09a9c65a-95b8-4ce3-a828-b887921823b5', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,504 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,665 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,506 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,601 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,665 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:27,604 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c7c1ade6dd4914ba4703cba3f8f7adb6', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '7af24b6d-ff4f-491c-b8bc-825c21e1ad25', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c7c1ade6dd4914ba4703cba3f8f7adb6', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '7af24b6d-ff4f-491c-b8bc-825c21e1ad25', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,705 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b075884fed47efb757412aa279a3f3a5', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '2', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0ad427f9-1c08-4855-a092-4c730580849f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b075884fed47efb757412aa279a3f3a5', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '2', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0ad427f9-1c08-4855-a092-4c730580849f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,895 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c7c1ade6dd4914ba4703cba3f8f7adb6', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '7af24b6d-ff4f-491c-b8bc-825c21e1ad25', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'c7c1ade6dd4914ba4703cba3f8f7adb6', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '7af24b6d-ff4f-491c-b8bc-825c21e1ad25', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,697 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ee09674664a74cf20b9a511ba982b079', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2062dc4a-d2da-4f1a-acc5-25163c402d85', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ee09674664a74cf20b9a511ba982b079', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2062dc4a-d2da-4f1a-acc5-25163c402d85', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,100 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ee09674664a74cf20b9a511ba982b079', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2062dc4a-d2da-4f1a-acc5-25163c402d85', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ee09674664a74cf20b9a511ba982b079', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '2062dc4a-d2da-4f1a-acc5-25163c402d85', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,995 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b075884fed47efb757412aa279a3f3a5', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '2', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0ad427f9-1c08-4855-a092-4c730580849f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'b075884fed47efb757412aa279a3f3a5', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '2', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '0ad427f9-1c08-4855-a092-4c730580849f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,810 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '81cc995870209c4f919477ea14d9e034', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45467aa7-3a02-444f-9d4f-930b376019d7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '81cc995870209c4f919477ea14d9e034', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45467aa7-3a02-444f-9d4f-930b376019d7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,708 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ae2f563a74a634b1a71ef8196d3cbc18', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd9433021-05a6-433e-9dc5-72890ac9a584', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ae2f563a74a634b1a71ef8196d3cbc18', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd9433021-05a6-433e-9dc5-72890ac9a584', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:27,895 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '257e7d7d7d4e5fd864f6f991b156d378', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '684d39ce-0d87-4683-86d4-95cef9c4a476', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '257e7d7d7d4e5fd864f6f991b156d378', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '684d39ce-0d87-4683-86d4-95cef9c4a476', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,304 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ae2f563a74a634b1a71ef8196d3cbc18', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd9433021-05a6-433e-9dc5-72890ac9a584', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': 'ae2f563a74a634b1a71ef8196d3cbc18', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'd9433021-05a6-433e-9dc5-72890ac9a584', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,214 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:28,197 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:28,294 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '81cc995870209c4f919477ea14d9e034', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45467aa7-3a02-444f-9d4f-930b376019d7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '81cc995870209c4f919477ea14d9e034', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '10', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '45467aa7-3a02-444f-9d4f-930b376019d7', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,312 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:50:28,306 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '257e7d7d7d4e5fd864f6f991b156d378', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '684d39ce-0d87-4683-86d4-95cef9c4a476', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '257e7d7d7d4e5fd864f6f991b156d378', 'date': 'Thu, 10 Jul 2025 07:50:26 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '684d39ce-0d87-4683-86d4-95cef9c4a476', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,604 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1567b987f1c4240e01a20875b3a849a9', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a81f8640-b8ce-4d21-a5de-a2ff74e88880', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1567b987f1c4240e01a20875b3a849a9', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a81f8640-b8ce-4d21-a5de-a2ff74e88880', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,998 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1567b987f1c4240e01a20875b3a849a9', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a81f8640-b8ce-4d21-a5de-a2ff74e88880', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '1567b987f1c4240e01a20875b3a849a9', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '8', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': 'a81f8640-b8ce-4d21-a5de-a2ff74e88880', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,607 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4a7bbe89f909d5c0d9d66337413e0a6a', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '24', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6cafe2b8-1cdf-45dd-a843-34e254d80b6f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4a7bbe89f909d5c0d9d66337413e0a6a', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '24', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6cafe2b8-1cdf-45dd-a843-34e254d80b6f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:29,394 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4a7bbe89f909d5c0d9d66337413e0a6a', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '24', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6cafe2b8-1cdf-45dd-a843-34e254d80b6f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '4a7bbe89f909d5c0d9d66337413e0a6a', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '24', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '6cafe2b8-1cdf-45dd-a843-34e254d80b6f', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:28,994 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '639c4ddfd4fe266cb8e6ad5f0a8bc7fb', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09701c8c-5526-447b-b85e-c35421e4573e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '639c4ddfd4fe266cb8e6ad5f0a8bc7fb', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09701c8c-5526-447b-b85e-c35421e4573e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:29,801 - root - ERROR - Startup failed: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '639c4ddfd4fe266cb8e6ad5f0a8bc7fb', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09701c8c-5526-447b-b85e-c35421e4573e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
Traceback (most recent call last):
  File "/app/main.py", line 122, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 185, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 268, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/qdrant.py", line 1109, in _validate_collection_for_dense
    vector_size = len(dense_embeddings.embed_documents(["dummy_text"])[0])
  File "/app/ingestion_retrieval/retrieval.py", line 235, in embed_documents
    resp = self.client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in embed
    responses = [
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 208, in <listcomp>
    responses = [
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/cohere/client.py", line 211, in <lambda>
    lambda text_batch: BaseCohere.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/base_client.py", line 1107, in embed
    _response = self._raw_client.embed(
  File "/opt/venv/lib/python3.10/site-packages/cohere/raw_base_client.py", line 1710, in embed
    raise TooManyRequestsError(
cohere.errors.too_many_requests_error.TooManyRequestsError: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 UTC', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '639c4ddfd4fe266cb8e6ad5f0a8bc7fb', 'date': 'Thu, 10 Jul 2025 07:50:27 GMT', 'content-length': '372', 'x-envoy-upstream-service-time': '13', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'}, status_code: 429, body: {'id': '09701c8c-5526-447b-b85e-c35421e4573e', 'message': "You are using a Trial key, which is limited to 40 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"}
2025-07-10 07:50:51,468 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:51,540 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:51,846 - root - INFO - Configuration validated successfully
2025-07-10 07:50:51,928 - root - INFO - Pre-warming the model...
2025-07-10 07:50:51,947 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:51,956 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:52,044 - root - INFO - Configuration validated successfully
2025-07-10 07:50:52,047 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:52,038 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:52,050 - root - INFO - Pre-warming the model...
2025-07-10 07:50:52,054 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:52,128 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:52,349 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:52,427 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:52,628 - root - INFO - Configuration validated successfully
2025-07-10 07:50:52,629 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:52,635 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:52,646 - root - INFO - Pre-warming the model...
2025-07-10 07:50:52,654 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:52,728 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:52,734 - root - INFO - Configuration validated successfully
2025-07-10 07:50:52,748 - root - INFO - Pre-warming the model...
2025-07-10 07:50:52,751 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:52,840 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:52,939 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:53,035 - root - INFO - Configuration validated successfully
2025-07-10 07:50:53,037 - root - INFO - Pre-warming the model...
2025-07-10 07:50:53,130 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:53,136 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:53,334 - root - INFO - Configuration validated successfully
2025-07-10 07:50:53,336 - root - INFO - Pre-warming the model...
2025-07-10 07:50:53,338 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:53,341 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:53,340 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:53,629 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:53,630 - root - INFO - Configuration validated successfully
2025-07-10 07:50:53,639 - root - INFO - Pre-warming the model...
2025-07-10 07:50:53,641 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:53,727 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:53,631 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:53,733 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:53,947 - root - INFO - Configuration validated successfully
2025-07-10 07:50:54,027 - root - INFO - Pre-warming the model...
2025-07-10 07:50:53,834 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:53,934 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:53,938 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:54,043 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:54,130 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:54,627 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:54,642 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:54,627 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:55,129 - root - INFO - Configuration validated successfully
2025-07-10 07:50:55,230 - root - INFO - Pre-warming the model...
2025-07-10 07:50:55,229 - root - INFO - Configuration validated successfully
2025-07-10 07:50:55,427 - root - INFO - Pre-warming the model...
2025-07-10 07:50:55,332 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:55,627 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:55,428 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:55,528 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:55,640 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:55,536 - root - INFO - Configuration validated successfully
2025-07-10 07:50:55,828 - root - INFO - Pre-warming the model...
2025-07-10 07:50:55,729 - root - INFO - Configuration validated successfully
2025-07-10 07:50:55,939 - root - INFO - Pre-warming the model...
2025-07-10 07:50:55,851 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:55,929 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:56,227 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:56,028 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:56,136 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:56,527 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:56,228 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:56,538 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:56,828 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:56,628 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:56,637 - root - INFO - Configuration validated successfully
2025-07-10 07:50:57,127 - root - INFO - Pre-warming the model...
2025-07-10 07:50:57,227 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:57,137 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:57,327 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:57,537 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:50:57,935 - root - INFO - Configuration validated successfully
2025-07-10 07:50:58,028 - root - INFO - Configuration validated successfully
2025-07-10 07:50:58,135 - root - INFO - Pre-warming the model...
2025-07-10 07:50:58,243 - root - INFO - Configuration validated successfully
2025-07-10 07:50:58,232 - root - INFO - Pre-warming the model...
2025-07-10 07:50:58,329 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:58,429 - root - INFO - Pre-warming the model...
2025-07-10 07:50:58,642 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:58,436 - root - INFO - Starting HR Assistant...
2025-07-10 07:50:58,528 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:50:58,628 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:58,828 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:59,028 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:50:59,335 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:10,792 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:10,854 - root - INFO - Configuration validated successfully
2025-07-10 07:51:10,855 - root - INFO - Pre-warming the model...
2025-07-10 07:51:10,856 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:10,858 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:10,906 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:11,021 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:11,109 - root - INFO - Configuration validated successfully
2025-07-10 07:51:11,110 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:11,110 - root - INFO - Pre-warming the model...
2025-07-10 07:51:11,113 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:11,114 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:11,411 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:11,423 - root - INFO - Configuration validated successfully
2025-07-10 07:51:11,502 - root - INFO - Pre-warming the model...
2025-07-10 07:51:11,505 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:11,508 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:11,512 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:11,804 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:11,812 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:11,913 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:11,915 - root - INFO - Configuration validated successfully
2025-07-10 07:51:11,916 - root - INFO - Pre-warming the model...
2025-07-10 07:51:11,919 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:12,007 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:12,026 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:12,302 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:12,403 - root - INFO - Configuration validated successfully
2025-07-10 07:51:12,304 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:12,404 - root - INFO - Configuration validated successfully
2025-07-10 07:51:12,419 - root - INFO - Pre-warming the model...
2025-07-10 07:51:12,502 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:12,406 - root - INFO - Pre-warming the model...
2025-07-10 07:51:12,612 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:12,605 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:12,607 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:12,615 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:12,803 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:12,908 - root - INFO - Configuration validated successfully
2025-07-10 07:51:12,804 - root - INFO - Configuration validated successfully
2025-07-10 07:51:12,914 - root - INFO - Pre-warming the model...
2025-07-10 07:51:12,917 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:12,915 - root - INFO - Pre-warming the model...
2025-07-10 07:51:13,021 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:13,018 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:13,023 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:13,213 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:13,302 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:13,319 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:13,403 - root - INFO - Configuration validated successfully
2025-07-10 07:51:13,506 - root - INFO - Pre-warming the model...
2025-07-10 07:51:13,503 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:13,512 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:13,603 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:13,712 - root - INFO - Configuration validated successfully
2025-07-10 07:51:13,716 - root - INFO - Pre-warming the model...
2025-07-10 07:51:13,807 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:13,814 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:13,913 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:14,009 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:14,208 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:14,304 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:14,304 - root - INFO - Configuration validated successfully
2025-07-10 07:51:14,502 - root - INFO - Pre-warming the model...
2025-07-10 07:51:14,511 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:14,514 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:14,609 - root - INFO - Configuration validated successfully
2025-07-10 07:51:14,614 - root - INFO - Pre-warming the model...
2025-07-10 07:51:14,702 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:14,807 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:14,908 - root - INFO - Configuration validated successfully
2025-07-10 07:51:14,920 - root - INFO - Pre-warming the model...
2025-07-10 07:51:14,914 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:14,922 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:15,102 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:15,004 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:15,012 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:15,305 - root - INFO - Configuration validated successfully
2025-07-10 07:51:15,314 - root - INFO - Pre-warming the model...
2025-07-10 07:51:15,402 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:15,413 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:15,414 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:15,809 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:15,903 - root - INFO - Configuration validated successfully
2025-07-10 07:51:15,910 - root - INFO - Pre-warming the model...
2025-07-10 07:51:15,918 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:15,903 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:16,006 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:16,024 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:16,109 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:16,310 - root - INFO - Starting HR Assistant...
2025-07-10 07:51:16,505 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:16,908 - root - INFO - Configuration validated successfully
2025-07-10 07:51:16,915 - root - INFO - Pre-warming the model...
2025-07-10 07:51:17,002 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:17,015 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:17,113 - root - INFO - Configuration validated successfully
2025-07-10 07:51:17,115 - root - INFO - Pre-warming the model...
2025-07-10 07:51:17,210 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:51:17,214 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:51:17,404 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:17,709 - ingestion_retrieval.retrieval - INFO - Using Cohere embedding model: embed-multilingual-v2.0
2025-07-10 07:51:19,203 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:22,857 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,056 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,063 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,472 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,473 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,472 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,482 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,483 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,486 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,488 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,726 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,723 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:24,730 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,733 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,724 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:24,726 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,733 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:51:24,937 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:24,937 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:25,267 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:25,266 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:25,267 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:25,504 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:25,504 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:25,536 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:25,542 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:25,613 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:25,615 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:25,616 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:25,616 - root - INFO - Loading google LLM...
2025-07-10 07:51:25,617 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:25,618 - root - INFO - Loading google LLM...
2025-07-10 07:51:25,754 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:25,756 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:25,759 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:25,759 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,049 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:26,050 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:26,077 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:26,077 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:26,102 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,128 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:26,130 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:26,130 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:26,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,133 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:26,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,135 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,145 - root - INFO - Loading google LLM...
2025-07-10 07:51:26,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:26,170 - root - INFO - Loading google LLM...
2025-07-10 07:51:26,401 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:26,925 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:26,939 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:26,952 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:27,000 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:27,004 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:27,007 - root - INFO - Loading google LLM...
2025-07-10 07:51:27,360 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:27,380 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:27,391 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:27,392 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:27,393 - root - INFO - Loading google LLM...
2025-07-10 07:51:27,419 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:27,421 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:27,424 - root - INFO - Loading google LLM...
2025-07-10 07:51:27,478 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:27,480 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:27,510 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:27,506 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:27,503 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:27,910 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:27,911 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:27,921 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:27,972 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:27,981 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:27,980 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:28,015 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:28,027 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:28,049 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:28,051 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:28,211 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:28,214 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:28,214 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:28,214 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:28,215 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:28,218 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:28,219 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:28,227 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:28,229 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:28,231 - root - INFO - Loading google LLM...
2025-07-10 07:51:28,233 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:28,234 - root - INFO - Loading google LLM...
2025-07-10 07:51:28,240 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:28,238 - root - INFO - Loading google LLM...
2025-07-10 07:51:28,242 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:28,245 - root - INFO - Loading google LLM...
2025-07-10 07:51:28,310 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,052 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,053 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,054 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,054 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,055 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,059 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,060 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,067 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,127 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:29,141 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:29,141 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:29,142 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:51:29,189 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:51:29,211 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:29,225 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:29,313 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:29,317 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:29,403 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:29,405 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,409 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:51:29,603 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,603 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:29,607 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,614 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:29,616 - root - INFO - Loading google LLM...
2025-07-10 07:51:29,621 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,627 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:29,702 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:29,703 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:29,703 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,704 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:29,725 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:29,706 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,707 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:29,721 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:29,721 - root - INFO - Loading google LLM...
2025-07-10 07:51:29,724 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:51:29,725 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:29,727 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:29,730 - root - INFO - Loading google LLM...
2025-07-10 07:51:29,806 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:51:29,808 - root - INFO - Loading google LLM...
2025-07-10 07:51:29,811 - root - INFO - Loading google LLM...
2025-07-10 07:51:29,817 - root - INFO - Loading google LLM...
2025-07-10 07:51:30,029 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:30,109 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
2025-07-10 07:51:30,204 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:19,349 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:19,363 - root - INFO - Configuration validated successfully
2025-07-10 07:53:19,366 - root - INFO - Pre-warming the model...
2025-07-10 07:53:19,368 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:19,370 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:19,374 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:19,376 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:19,447 - root - INFO - Configuration validated successfully
2025-07-10 07:53:19,449 - root - INFO - Pre-warming the model...
2025-07-10 07:53:19,452 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:19,457 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:19,466 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:19,658 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:19,745 - root - INFO - Configuration validated successfully
2025-07-10 07:53:19,748 - root - INFO - Pre-warming the model...
2025-07-10 07:53:19,753 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:19,756 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:19,759 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:19,769 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:19,846 - root - INFO - Configuration validated successfully
2025-07-10 07:53:19,850 - root - INFO - Pre-warming the model...
2025-07-10 07:53:19,853 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:19,856 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:19,948 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:20,047 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:20,145 - root - INFO - Configuration validated successfully
2025-07-10 07:53:20,150 - root - INFO - Pre-warming the model...
2025-07-10 07:53:20,154 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:20,157 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:20,246 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:20,261 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:20,359 - root - INFO - Configuration validated successfully
2025-07-10 07:53:20,445 - root - INFO - Pre-warming the model...
2025-07-10 07:53:20,450 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:20,456 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:20,547 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:20,554 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:20,650 - root - INFO - Configuration validated successfully
2025-07-10 07:53:20,655 - root - INFO - Pre-warming the model...
2025-07-10 07:53:20,755 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:20,757 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:20,851 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:21,062 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:21,250 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:21,446 - root - INFO - Configuration validated successfully
2025-07-10 07:53:21,454 - root - INFO - Configuration validated successfully
2025-07-10 07:53:21,544 - root - INFO - Pre-warming the model...
2025-07-10 07:53:21,644 - root - INFO - Pre-warming the model...
2025-07-10 07:53:21,748 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:21,945 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:21,945 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:22,045 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:22,046 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:22,061 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:22,164 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:22,155 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:22,254 - root - INFO - Configuration validated successfully
2025-07-10 07:53:22,452 - root - INFO - Pre-warming the model...
2025-07-10 07:53:22,454 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:22,455 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:22,558 - root - INFO - Configuration validated successfully
2025-07-10 07:53:22,652 - root - INFO - Pre-warming the model...
2025-07-10 07:53:22,651 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:22,659 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:22,761 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:22,768 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:22,854 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:23,063 - root - INFO - Configuration validated successfully
2025-07-10 07:53:23,149 - root - INFO - Pre-warming the model...
2025-07-10 07:53:23,245 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:23,346 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:23,450 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:23,753 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:24,054 - root - INFO - Configuration validated successfully
2025-07-10 07:53:24,147 - root - INFO - Pre-warming the model...
2025-07-10 07:53:24,245 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:24,344 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:24,462 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:24,645 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:24,752 - root - INFO - Configuration validated successfully
2025-07-10 07:53:24,844 - root - INFO - Pre-warming the model...
2025-07-10 07:53:24,945 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:25,045 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:25,047 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:25,252 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:25,353 - root - INFO - Configuration validated successfully
2025-07-10 07:53:25,252 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:25,444 - root - INFO - Pre-warming the model...
2025-07-10 07:53:25,455 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:25,549 - root - INFO - Configuration validated successfully
2025-07-10 07:53:25,550 - root - INFO - Starting HR Assistant...
2025-07-10 07:53:25,544 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:25,552 - root - INFO - Pre-warming the model...
2025-07-10 07:53:25,555 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:25,563 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:25,654 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:25,746 - root - INFO - Configuration validated successfully
2025-07-10 07:53:25,749 - root - INFO - Pre-warming the model...
2025-07-10 07:53:25,756 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:25,759 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:53:25,853 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:53:25,954 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:53:26,849 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:26,851 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:26,853 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:27,754 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:27,756 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:28,050 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:28,050 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:28,063 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:28,463 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:28,572 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:28,818 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:28,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:28,822 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:28,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:28,931 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:29,325 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:29,327 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:29,335 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:29,747 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:29,749 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:29,748 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:29,750 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:29,753 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:29,754 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:29,759 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:29,762 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:29,765 - root - INFO - Loading groq LLM...
2025-07-10 07:53:29,766 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:29,769 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:29,770 - root - INFO - Loading groq LLM...
2025-07-10 07:53:30,046 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:30,502 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:30,503 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:30,510 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:30,519 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:30,521 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:30,523 - root - INFO - Loading groq LLM...
2025-07-10 07:53:30,636 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:30,655 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:30,660 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:30,662 - root - INFO - Loading groq LLM...
2025-07-10 07:53:30,827 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:30,827 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:30,835 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:30,844 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:30,846 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:31,414 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:31,416 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:31,423 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:31,423 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:31,426 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:31,429 - root - INFO - Loading groq LLM...
2025-07-10 07:53:31,498 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:31,514 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:31,557 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:31,575 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:31,576 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:31,578 - root - INFO - Loading groq LLM...
2025-07-10 07:53:31,909 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:31,915 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:31,916 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:31,917 - root - INFO - Loading groq LLM...
2025-07-10 07:53:32,046 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:32,052 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:32,053 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:32,053 - root - INFO - Loading groq LLM...
2025-07-10 07:53:32,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:32,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:32,711 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:32,752 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:32,772 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:32,785 - root - INFO - Loading groq LLM...
2025-07-10 07:53:32,854 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:32,873 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:32,877 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:32,882 - root - INFO - Loading groq LLM...
2025-07-10 07:53:32,949 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:32,954 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:32,958 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:32,979 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:32,981 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:33,046 - root - INFO - Loading groq LLM...
2025-07-10 07:53:33,050 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:33,061 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:33,051 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:33,070 - root - INFO - Loading groq LLM...
2025-07-10 07:53:33,149 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:33,157 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:33,354 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:33,457 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:33,463 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:33,464 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:33,470 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:33,472 - root - INFO - Loading groq LLM...
2025-07-10 07:53:33,749 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:33,846 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:33,950 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:33,953 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:34,150 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:34,156 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:34,245 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:34,250 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:34,252 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:34,258 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:34,255 - root - INFO - Loading groq LLM...
2025-07-10 07:53:34,260 - root - INFO - Loading groq LLM...
2025-07-10 07:53:34,847 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:35,250 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:35,252 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:35,346 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:35,453 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:35,460 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:35,556 - root - INFO - Loading groq LLM...
2025-07-10 07:53:35,546 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:35,748 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:53:35,848 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:35,852 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:35,945 - root - INFO - Loading groq LLM...
2025-07-10 07:53:39,037 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:39,343 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:39,610 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:39,636 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:39,637 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:40,270 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:40,321 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:40,323 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:40,658 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:40,735 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:40,736 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:40,748 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:40,750 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:40,751 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:40,752 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:41,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:41,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:41,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:41,195 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:41,361 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:41,361 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:41,362 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:41,416 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:41,418 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:41,419 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:41,427 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:41,445 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:41,446 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:41,868 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:41,870 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:41,879 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:41,890 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:41,928 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:41,929 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:41,963 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:41,964 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:42,119 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:42,119 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:42,122 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:42,124 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:43,222 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:43,237 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:43,237 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:43,237 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:43,240 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:53:43,241 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:53:43,555 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:43,557 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:43,861 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:46,899 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:53:48,841 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:53:49,240 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:49,241 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:53:49,640 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:49,838 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:49,839 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:50,137 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:50,139 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:51,115 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:51,116 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:51,124 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:51,170 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:51,180 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:51,184 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:51,189 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:51,199 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:51,202 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:51,447 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:53:51,460 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 8.000000 seconds
2025-07-10 07:53:51,835 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:53:51,835 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:53:51,835 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:53:51,837 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 7.000000 seconds
2025-07-10 07:53:51,839 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 7.000000 seconds
2025-07-10 07:53:51,839 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 7.000000 seconds
2025-07-10 07:53:54,322 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:54,323 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:56,775 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:56,777 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:58,959 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:53:58,961 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:53:59,847 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:59,847 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:59,847 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:53:59,899 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:59,902 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:59,917 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:59,920 - root - INFO - HR Assistant started successfully
2025-07-10 07:53:59,977 - root - INFO - Model pre-warmed successfully
2025-07-10 07:53:59,979 - root - INFO - HR Assistant started successfully
2025-07-10 07:54:00,115 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-10 07:54:00,119 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 15.000000 seconds
2025-07-10 07:54:03,180 - admin_interface - INFO - Successfully updated .env file
2025-07-10 07:54:03,184 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:54:03,185 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:54:03,238 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:54:03,242 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:54:03,263 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:54:03,266 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:54:03,267 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:54:03,273 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:54:16,823 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:54:16,855 - root - INFO - Model pre-warmed successfully
2025-07-10 07:54:16,856 - root - INFO - HR Assistant started successfully
2025-07-10 07:54:24,824 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:54:24,825 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:54:59,110 - root - INFO - Received question: what are import policies should know
2025-07-10 07:54:59,111 - root - INFO - Chain init: 0.00s
2025-07-10 07:55:00,985 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:55:04,719 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:55:07,213 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 07:55:07,224 - root - INFO - Question processing: 8.11s
2025-07-10 07:55:07,225 - root - INFO - ⏱️ ask_hr took 8.12 seconds
2025-07-10 07:55:52,906 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:55:52,912 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:55:52,919 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 07:55:52,921 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 07:55:54,022 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:55:54,216 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:55:54,883 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:55:55,205 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:55:55,206 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:55:55,454 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:56:05,851 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:56:05,854 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:56:48,297 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-10 07:56:48,301 - admin_interface - INFO - Successfully updated .env file
2025-07-10 07:57:20,374 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:20,459 - root - INFO - Configuration validated successfully
2025-07-10 07:57:20,460 - root - INFO - Pre-warming the model...
2025-07-10 07:57:20,462 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:20,463 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:20,548 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:20,560 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:20,730 - root - INFO - Configuration validated successfully
2025-07-10 07:57:20,732 - root - INFO - Pre-warming the model...
2025-07-10 07:57:20,734 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:20,736 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:20,927 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:20,935 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:21,228 - root - INFO - Configuration validated successfully
2025-07-10 07:57:21,230 - root - INFO - Pre-warming the model...
2025-07-10 07:57:21,233 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:21,238 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:21,441 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:21,530 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:21,649 - root - INFO - Configuration validated successfully
2025-07-10 07:57:21,725 - root - INFO - Pre-warming the model...
2025-07-10 07:57:21,731 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:21,738 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:21,744 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:21,948 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:22,134 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:22,337 - root - INFO - Configuration validated successfully
2025-07-10 07:57:22,338 - root - INFO - Pre-warming the model...
2025-07-10 07:57:22,341 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:22,434 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:22,627 - root - INFO - Configuration validated successfully
2025-07-10 07:57:22,634 - root - INFO - Pre-warming the model...
2025-07-10 07:57:22,639 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:22,642 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:22,739 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:22,928 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:23,027 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:23,139 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:23,226 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:23,228 - root - INFO - Configuration validated successfully
2025-07-10 07:57:23,332 - root - INFO - Pre-warming the model...
2025-07-10 07:57:23,338 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:23,339 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:23,440 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:23,534 - root - INFO - Configuration validated successfully
2025-07-10 07:57:23,543 - root - INFO - Pre-warming the model...
2025-07-10 07:57:23,626 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:23,631 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:23,737 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:23,837 - root - INFO - Configuration validated successfully
2025-07-10 07:57:23,838 - root - INFO - Configuration validated successfully
2025-07-10 07:57:23,840 - root - INFO - Pre-warming the model...
2025-07-10 07:57:23,841 - root - INFO - Pre-warming the model...
2025-07-10 07:57:23,844 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:23,925 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:23,844 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:23,933 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:24,026 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:24,027 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:24,029 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:24,326 - root - INFO - Configuration validated successfully
2025-07-10 07:57:24,337 - root - INFO - Pre-warming the model...
2025-07-10 07:57:24,425 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:24,434 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:24,531 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:24,732 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:24,926 - root - INFO - Configuration validated successfully
2025-07-10 07:57:24,934 - root - INFO - Pre-warming the model...
2025-07-10 07:57:24,936 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:25,027 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:25,029 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:25,241 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:25,333 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:25,434 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:25,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:25,739 - root - INFO - Configuration validated successfully
2025-07-10 07:57:25,735 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:25,830 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:25,826 - root - INFO - Pre-warming the model...
2025-07-10 07:57:25,840 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:25,845 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:26,129 - root - INFO - Configuration validated successfully
2025-07-10 07:57:26,138 - root - INFO - Pre-warming the model...
2025-07-10 07:57:26,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:26,238 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:26,330 - root - INFO - Configuration validated successfully
2025-07-10 07:57:26,325 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:26,434 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:26,425 - root - INFO - Pre-warming the model...
2025-07-10 07:57:26,427 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:26,431 - root - INFO - Configuration validated successfully
2025-07-10 07:57:26,442 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:26,525 - root - INFO - Pre-warming the model...
2025-07-10 07:57:26,537 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:26,542 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:26,534 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:26,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:26,830 - root - INFO - Configuration validated successfully
2025-07-10 07:57:26,933 - root - INFO - Pre-warming the model...
2025-07-10 07:57:26,837 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:26,934 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:26,938 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:27,025 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:27,127 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:27,130 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:27,133 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:27,643 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:27,826 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:28,127 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:28,127 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:28,429 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:28,927 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:29,032 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:29,326 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:29,328 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:29,331 - root - INFO - Loading google LLM...
2025-07-10 07:57:29,426 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:29,428 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:29,836 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:29,837 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:29,837 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:29,838 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,168 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,174 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,210 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:30,245 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:30,247 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:30,248 - root - INFO - Loading google LLM...
2025-07-10 07:57:30,453 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,454 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,459 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,460 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:30,462 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,464 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:30,559 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:30,560 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:30,562 - root - INFO - Loading google LLM...
2025-07-10 07:57:30,567 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:30,569 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:30,571 - root - INFO - Loading google LLM...
2025-07-10 07:57:30,661 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,672 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,673 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,675 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,673 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,674 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,674 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:30,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:30,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,503 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:31,556 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:31,558 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:31,559 - root - INFO - Loading google LLM...
2025-07-10 07:57:31,601 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:31,642 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:31,643 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:31,645 - root - INFO - Loading google LLM...
2025-07-10 07:57:31,902 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,905 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:31,905 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:32,184 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:32,187 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:32,187 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:32,230 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:32,231 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:32,232 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:32,233 - root - INFO - Loading google LLM...
2025-07-10 07:57:32,234 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:32,236 - root - INFO - Loading google LLM...
2025-07-10 07:57:32,331 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:32,476 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:32,478 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:32,481 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:32,481 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:32,505 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:32,544 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:32,545 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:32,548 - root - INFO - Loading google LLM...
2025-07-10 07:57:32,549 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:32,552 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:32,554 - root - INFO - Loading google LLM...
2025-07-10 07:57:32,565 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:32,626 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:32,629 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:32,631 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:32,633 - root - INFO - Loading google LLM...
2025-07-10 07:57:33,060 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:33,062 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:33,062 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:33,564 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:33,569 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:33,564 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:33,565 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:33,567 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:33,567 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:57:33,728 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:33,731 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:33,743 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:33,745 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:33,747 - root - INFO - Loading google LLM...
2025-07-10 07:57:33,748 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:33,748 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:33,748 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:33,752 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:33,752 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:33,753 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:33,825 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:33,827 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:33,831 - root - INFO - Loading google LLM...
2025-07-10 07:57:33,833 - root - INFO - Loading google LLM...
2025-07-10 07:57:33,835 - root - INFO - Loading google LLM...
2025-07-10 07:57:33,836 - root - INFO - Loading google LLM...
2025-07-10 07:57:33,836 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:33,843 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:33,845 - root - INFO - Loading google LLM...
2025-07-10 07:57:34,332 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:34,338 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:34,339 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:34,340 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:34,341 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:34,338 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:34,343 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:34,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:34,910 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:34,910 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:34,917 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:35,523 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:35,525 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:35,525 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:35,527 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:35,726 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:35,726 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:35,732 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:35,734 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:35,833 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:35,836 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:36,061 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:36,066 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:57:36,082 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:36,084 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:37,856 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 07:57:37,856 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 07:57:38,109 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:38,112 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:38,271 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:38,272 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:38,272 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:38,274 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:38,277 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:38,279 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:38,897 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:38,898 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:39,340 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:39,341 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:39,342 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:39,342 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:39,505 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:39,507 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:40,331 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:40,333 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:40,325 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 07:57:40,325 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 07:57:40,493 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:40,494 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:40,494 - root - INFO - Model pre-warmed successfully
2025-07-10 07:57:40,496 - root - INFO - HR Assistant started successfully
2025-07-10 07:57:42,421 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:42,428 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:42,430 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:57:42,432 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:57:43,534 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:43,780 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:57:44,359 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:57:44,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:44,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:57:44,950 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:57:55,550 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:55,579 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:55,653 - root - INFO - Configuration validated successfully
2025-07-10 07:57:55,656 - root - INFO - Pre-warming the model...
2025-07-10 07:57:55,659 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:55,660 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:55,751 - root - INFO - Configuration validated successfully
2025-07-10 07:57:55,752 - root - INFO - Pre-warming the model...
2025-07-10 07:57:55,754 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:55,757 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:55,766 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:55,779 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:55,943 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:55,949 - root - INFO - Configuration validated successfully
2025-07-10 07:57:55,951 - root - INFO - Pre-warming the model...
2025-07-10 07:57:55,956 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:55,962 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:55,963 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:56,162 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:56,250 - root - INFO - Configuration validated successfully
2025-07-10 07:57:56,253 - root - INFO - Pre-warming the model...
2025-07-10 07:57:56,259 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:56,261 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:56,262 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:56,350 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:56,645 - root - INFO - Configuration validated successfully
2025-07-10 07:57:56,652 - root - INFO - Pre-warming the model...
2025-07-10 07:57:56,655 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:56,661 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:56,745 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:56,746 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:56,761 - root - INFO - Configuration validated successfully
2025-07-10 07:57:56,843 - root - INFO - Pre-warming the model...
2025-07-10 07:57:56,854 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:56,856 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:56,944 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:57,150 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:57,246 - root - INFO - Configuration validated successfully
2025-07-10 07:57:57,256 - root - INFO - Pre-warming the model...
2025-07-10 07:57:57,259 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:57,344 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:57,355 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:57,348 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:57,449 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:57,657 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:57,747 - root - INFO - Configuration validated successfully
2025-07-10 07:57:57,842 - root - INFO - Pre-warming the model...
2025-07-10 07:57:57,751 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:57,851 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:57,943 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:57,947 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:58,147 - root - INFO - Configuration validated successfully
2025-07-10 07:57:58,156 - root - INFO - Pre-warming the model...
2025-07-10 07:57:58,242 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:58,342 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:58,345 - root - INFO - Configuration validated successfully
2025-07-10 07:57:58,346 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:58,363 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:58,353 - root - INFO - Pre-warming the model...
2025-07-10 07:57:58,451 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:58,458 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:58,543 - root - INFO - Configuration validated successfully
2025-07-10 07:57:58,552 - root - INFO - Pre-warming the model...
2025-07-10 07:57:58,553 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:58,554 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:58,563 - root - INFO - Configuration validated successfully
2025-07-10 07:57:58,643 - root - INFO - Pre-warming the model...
2025-07-10 07:57:58,643 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:58,648 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:58,752 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:58,849 - root - INFO - Configuration validated successfully
2025-07-10 07:57:58,943 - root - INFO - Pre-warming the model...
2025-07-10 07:57:58,948 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:58,958 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:59,045 - root - INFO - Configuration validated successfully
2025-07-10 07:57:59,056 - root - INFO - Pre-warming the model...
2025-07-10 07:57:59,148 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:57:59,242 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:57:59,243 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:59,246 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:59,345 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:59,649 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:59,659 - root - INFO - Starting HR Assistant...
2025-07-10 07:57:59,757 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:57:59,950 - root - INFO - Configuration validated successfully
2025-07-10 07:58:00,048 - root - INFO - Pre-warming the model...
2025-07-10 07:58:00,142 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:58:00,242 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:58:00,443 - root - INFO - Starting HR Assistant...
2025-07-10 07:58:00,468 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:58:00,546 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:58:00,748 - root - INFO - Configuration validated successfully
2025-07-10 07:58:00,756 - root - INFO - Pre-warming the model...
2025-07-10 07:58:00,844 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:58:00,854 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:58:01,146 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:01,146 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:01,546 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:58:01,749 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:58:01,946 - root - INFO - Configuration validated successfully
2025-07-10 07:58:01,956 - root - INFO - Pre-warming the model...
2025-07-10 07:58:02,043 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 07:58:02,052 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 07:58:02,352 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:02,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:02,648 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:02,750 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 07:58:03,550 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:03,744 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:04,249 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:04,552 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:04,746 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:04,844 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:05,150 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:05,150 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:05,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:05,163 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:05,165 - root - INFO - Loading google LLM...
2025-07-10 07:58:05,461 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:05,464 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:05,682 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:05,683 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:05,684 - root - INFO - Loading google LLM...
2025-07-10 07:58:05,768 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:05,775 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:05,885 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:05,886 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:05,887 - root - INFO - Loading google LLM...
2025-07-10 07:58:06,077 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:06,080 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,145 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:06,146 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:06,146 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:06,151 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,349 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:06,355 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,357 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,730 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,733 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,737 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:06,866 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:06,869 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:06,924 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:06,925 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:06,926 - root - INFO - Loading google LLM...
2025-07-10 07:58:06,930 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:06,931 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:06,932 - root - INFO - Loading google LLM...
2025-07-10 07:58:07,193 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,193 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:07,199 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:07,199 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,200 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,262 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,262 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,345 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:07,345 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:07,377 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:07,379 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:07,381 - root - INFO - Loading google LLM...
2025-07-10 07:58:07,453 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:07,455 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:07,456 - root - INFO - Loading google LLM...
2025-07-10 07:58:07,464 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,465 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,477 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:07,480 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:07,538 - root - INFO - Loading google LLM...
2025-07-10 07:58:07,771 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,772 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,774 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,772 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:07,776 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:07,935 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,936 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:07,940 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:08,103 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:08,105 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:08,441 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:08,492 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:08,493 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:08,495 - root - INFO - Loading google LLM...
2025-07-10 07:58:08,955 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:08,963 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:08,962 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:08,969 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:09,144 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,145 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,145 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,146 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,146 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,150 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,147 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,151 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,152 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,631 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:09,631 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:09,633 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:09,635 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:09,642 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:09,643 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:09,644 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:09,644 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:09,645 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:09,651 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 07:58:09,674 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,738 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,740 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,748 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,750 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,751 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,751 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,754 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,756 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,763 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,764 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,766 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,767 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:09,769 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:09,771 - root - INFO - Loading google LLM...
2025-07-10 07:58:09,857 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:09,857 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:09,938 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:10,284 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:10,284 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:10,292 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:10,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:10,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:10,999 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:11,708 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:11,710 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:11,725 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:12,107 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:12,109 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:12,114 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:12,122 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:12,125 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:12,129 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:12,136 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 07:58:12,453 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:12,455 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:12,923 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:12,925 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:13,078 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:13,081 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 47
}
].
2025-07-10 07:58:13,138 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:13,139 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:13,321 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,017 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:58:14,019 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,019 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,019 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,019 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,019 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,019 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,020 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 46
}
].
2025-07-10 07:58:14,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:14,325 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:14,340 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:14,346 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 07:58:14,347 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 07:58:14,764 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:14,765 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:15,186 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:15,188 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:15,336 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 44
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 44
}
]
2025-07-10 07:58:15,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 07:58:15,595 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:15,596 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:15,596 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:15,597 - root - INFO - HR Assistant started successfully
2025-07-10 07:58:16,316 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
2025-07-10 07:58:16,316 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
2025-07-10 07:58:16,316 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
2025-07-10 07:58:16,570 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 07:58:16,641 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
2025-07-10 07:58:16,641 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
2025-07-10 07:58:16,646 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
2025-07-10 07:58:16,649 - root - ERROR - Startup failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count"
  quota_id: "GenerateContentInputTokensPerModelPerMinute-FreeTier"
  quota_dimensions {
    key: "model"
    value: "gemma-3-12b"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
  quota_value: 15000
}
, links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, retry_delay {
  seconds: 43
}
]
2025-07-10 07:58:16,898 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 07:58:17,540 - root - INFO - Model pre-warmed successfully
2025-07-10 07:58:17,549 - root - INFO - HR Assistant started successfully
2025-07-10 00:59:47,086 - root - INFO - Starting HR Assistant...
2025-07-10 00:59:47,226 - root - INFO - Configuration validated successfully
2025-07-10 00:59:47,226 - root - INFO - Pre-warming the model...
2025-07-10 00:59:47,228 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 00:59:47,228 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 00:59:47,363 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 00:59:50,164 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 00:59:52,054 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:59:53,541 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 00:59:53,676 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 00:59:53,689 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 00:59:53,689 - root - INFO - Loading groq LLM...
2025-07-10 00:59:55,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 00:59:57,579 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 00:59:59,957 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 00:59:59,999 - root - INFO - Model pre-warmed successfully
2025-07-10 00:59:59,999 - root - INFO - HR Assistant started successfully
2025-07-10 01:00:03,582 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:00:03,991 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:00:04,650 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 01:00:04,979 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:00:04,979 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 01:00:05,153 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:00:05,213 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 01:00:05,219 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 01:00:05,222 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 01:00:05,222 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 01:00:08,981 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 01:00:08,981 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 01:00:20,479 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 01:00:20,480 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 01:00:26,031 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-10 01:00:26,033 - admin_interface - INFO - Successfully updated .env file
2025-07-10 01:00:26,036 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 01:00:26,039 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 01:00:26,189 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:00:26,189 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:00:26,202 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 01:00:26,202 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 01:00:26,202 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 01:00:26,202 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 01:01:18,922 - root - INFO - Starting HR Assistant...
2025-07-10 01:01:19,053 - root - INFO - Configuration validated successfully
2025-07-10 01:01:19,053 - root - INFO - Pre-warming the model...
2025-07-10 01:01:19,053 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 01:01:19,053 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 01:01:19,185 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 01:01:21,517 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:01:22,243 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:01:23,153 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 01:01:23,284 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:01:23,286 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:01:23,286 - root - INFO - Loading google LLM...
2025-07-10 01:01:23,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:01:25,208 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 01:01:28,416 - root - INFO - Model pre-warmed successfully
2025-07-10 01:01:28,420 - root - INFO - HR Assistant started successfully
2025-07-10 01:02:05,127 - root - INFO - Received question: can you explain me the car Registration Fees and taxes policy
2025-07-10 01:02:05,127 - root - INFO - Chain init: 0.00s
2025-07-10 01:02:06,168 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:02:08,348 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 01:02:11,818 - root - INFO - Question processing: 6.69s
2025-07-10 01:02:11,818 - root - INFO - ⏱️ ask_hr took 6.69 seconds
2025-07-10 01:08:55,847 - root - INFO - Starting HR Assistant...
2025-07-10 01:08:56,005 - root - INFO - Configuration validated successfully
2025-07-10 01:08:56,005 - root - INFO - Pre-warming the model...
2025-07-10 01:08:56,005 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 01:08:56,007 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 01:08:56,170 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 01:08:59,496 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:09:00,090 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:09:01,265 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 01:09:01,423 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:09:01,423 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:09:01,424 - root - INFO - Loading google LLM...
2025-07-10 01:09:01,678 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:09:03,289 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 01:09:08,772 - root - INFO - Model pre-warmed successfully
2025-07-10 01:09:08,772 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:03,868 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:03,894 - root - INFO - Configuration validated successfully
2025-07-10 08:10:03,896 - root - INFO - Pre-warming the model...
2025-07-10 08:10:03,899 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:03,901 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:03,968 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:04,369 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:04,665 - root - INFO - Configuration validated successfully
2025-07-10 08:10:04,667 - root - INFO - Pre-warming the model...
2025-07-10 08:10:04,673 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:04,674 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:04,765 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:04,962 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:05,070 - root - INFO - Configuration validated successfully
2025-07-10 08:10:05,074 - root - INFO - Pre-warming the model...
2025-07-10 08:10:05,075 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:05,161 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:05,168 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:05,171 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:05,366 - root - INFO - Configuration validated successfully
2025-07-10 08:10:05,461 - root - INFO - Pre-warming the model...
2025-07-10 08:10:05,562 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:05,569 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:05,674 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:05,678 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:05,967 - root - INFO - Configuration validated successfully
2025-07-10 08:10:06,061 - root - INFO - Pre-warming the model...
2025-07-10 08:10:06,156 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:06,253 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:06,354 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:06,756 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:06,859 - root - INFO - Configuration validated successfully
2025-07-10 08:10:06,954 - root - INFO - Pre-warming the model...
2025-07-10 08:10:06,960 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:06,960 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:06,963 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:07,068 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:07,163 - root - INFO - Configuration validated successfully
2025-07-10 08:10:07,253 - root - INFO - Pre-warming the model...
2025-07-10 08:10:07,168 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:07,262 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:07,355 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:07,366 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:07,462 - root - INFO - Configuration validated successfully
2025-07-10 08:10:07,553 - root - INFO - Pre-warming the model...
2025-07-10 08:10:07,559 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:07,564 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:07,664 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:08,165 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:08,462 - root - INFO - Configuration validated successfully
2025-07-10 08:10:08,554 - root - INFO - Pre-warming the model...
2025-07-10 08:10:08,564 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:08,653 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:08,755 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:08,755 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:08,957 - root - INFO - Configuration validated successfully
2025-07-10 08:10:08,965 - root - INFO - Pre-warming the model...
2025-07-10 08:10:09,054 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:09,066 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:09,072 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:09,059 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:09,165 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:09,356 - root - INFO - Configuration validated successfully
2025-07-10 08:10:09,361 - root - INFO - Configuration validated successfully
2025-07-10 08:10:09,364 - root - INFO - Pre-warming the model...
2025-07-10 08:10:09,463 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:09,464 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:09,453 - root - INFO - Pre-warming the model...
2025-07-10 08:10:09,464 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:09,654 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:09,654 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:09,567 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:09,656 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:09,662 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:09,763 - root - INFO - Configuration validated successfully
2025-07-10 08:10:09,853 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:09,865 - root - INFO - Configuration validated successfully
2025-07-10 08:10:10,053 - root - INFO - Pre-warming the model...
2025-07-10 08:10:09,867 - root - INFO - Configuration validated successfully
2025-07-10 08:10:10,063 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:09,869 - root - INFO - Pre-warming the model...
2025-07-10 08:10:10,061 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:10,067 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:09,961 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:10,064 - root - INFO - Pre-warming the model...
2025-07-10 08:10:10,066 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:10,172 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:10,165 - root - INFO - Starting HR Assistant...
2025-07-10 08:10:10,158 - root - INFO - Configuration validated successfully
2025-07-10 08:10:10,262 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:10,162 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:10,353 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:10,262 - root - INFO - Pre-warming the model...
2025-07-10 08:10:10,266 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:10,273 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:10,356 - root - INFO - Configuration validated successfully
2025-07-10 08:10:10,360 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:10,461 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:10,462 - root - INFO - Pre-warming the model...
2025-07-10 08:10:10,557 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:10:10,466 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:10,561 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:10:10,660 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:10,755 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:10:10,758 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:11,357 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:11,863 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:11,864 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:12,755 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:12,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:12,956 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:13,660 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:13,962 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:14,069 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:14,164 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:14,165 - root - INFO - Loading google LLM...
2025-07-10 08:10:14,360 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:14,370 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:14,455 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:14,456 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:14,460 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:14,466 - root - INFO - Loading google LLM...
2025-07-10 08:10:14,474 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:14,567 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:14,575 - root - INFO - Loading google LLM...
2025-07-10 08:10:14,755 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:14,855 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,055 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,056 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,056 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,278 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,356 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,360 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:15,460 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:15,466 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:15,472 - root - INFO - Loading google LLM...
2025-07-10 08:10:15,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,560 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,751 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,751 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,756 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,764 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:15,856 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:15,857 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:15,859 - root - INFO - Loading google LLM...
2025-07-10 08:10:15,955 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,956 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:15,959 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:15,959 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:16,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,233 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,420 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,431 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,396 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:16,455 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,473 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:16,475 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:16,477 - root - INFO - Loading google LLM...
2025-07-10 08:10:16,562 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,790 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:16,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,795 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:16,810 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:16,811 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:16,813 - root - INFO - Loading google LLM...
2025-07-10 08:10:16,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:16,954 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:16,955 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:16,959 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:16,985 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:16,986 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:16,989 - root - INFO - Loading google LLM...
2025-07-10 08:10:17,189 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:17,197 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:17,214 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:17,216 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:17,217 - root - INFO - Loading google LLM...
2025-07-10 08:10:17,369 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:17,375 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:17,375 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:17,385 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:17,386 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:17,387 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:17,388 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:17,389 - root - INFO - Loading google LLM...
2025-07-10 08:10:17,391 - root - INFO - Loading google LLM...
2025-07-10 08:10:17,687 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:17,691 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:17,694 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:17,695 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:17,696 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:17,698 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:17,713 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:17,714 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:17,754 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:17,755 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:17,757 - root - INFO - Loading google LLM...
2025-07-10 08:10:17,760 - root - INFO - Loading google LLM...
2025-07-10 08:10:18,015 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:18,016 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:18,027 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:18,031 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:18,032 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:18,033 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:18,034 - root - INFO - Loading google LLM...
2025-07-10 08:10:18,035 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:18,037 - root - INFO - Loading google LLM...
2025-07-10 08:10:18,342 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:18,343 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:18,350 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:18,354 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:18,359 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:18,396 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:18,454 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:18,456 - root - INFO - Loading google LLM...
2025-07-10 08:10:18,457 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:10:18,468 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:18,471 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:18,472 - root - INFO - Loading google LLM...
2025-07-10 08:10:18,949 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:18,950 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:18,950 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:18,950 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:18,954 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:19,170 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:19,183 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:19,358 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:19,747 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:19,748 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:19,921 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:19,922 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:19,928 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:19,930 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:20,419 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:20,436 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:20,822 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:20,823 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:21,825 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:21,827 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:22,187 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:10:22,374 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:10:22,870 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:22,871 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:23,619 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:23,620 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:23,954 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:23,955 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:25,020 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:25,027 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:25,926 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:25,927 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:26,841 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:26,843 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:27,915 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:27,917 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:28,399 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:28,401 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:29,041 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:29,042 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:29,232 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:29,234 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:30,310 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:30,311 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:31,137 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:31,139 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:31,305 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:31,306 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:32,368 - root - INFO - Model pre-warmed successfully
2025-07-10 08:10:32,369 - root - INFO - HR Assistant started successfully
2025-07-10 08:10:37,904 - root - INFO - Received question: can you explain me the car Registration Fees and taxes policy\
2025-07-10 08:10:37,906 - root - INFO - Chain init: 0.00s
2025-07-10 08:10:38,866 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:41,011 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:10:43,898 - root - INFO - Question processing: 5.99s
2025-07-10 08:10:43,900 - root - INFO - ⏱️ ask_hr took 6.00 seconds
2025-07-10 08:10:52,450 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:52,695 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:10:53,198 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:10:53,435 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:10:53,443 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:53,444 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:53,445 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:10:53,447 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:10:53,614 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:10:53,797 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:11:13,106 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:11:13,107 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:11:16,644 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:11:16,645 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:11:21,640 - admin_interface - INFO - Successfully updated .env file
2025-07-10 08:11:21,645 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:11:21,646 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:11:21,693 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:11:21,695 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:11:21,706 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:11:21,707 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:11:21,709 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:11:21,710 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:11:26,543 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:11:26,545 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:02,626 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:02,812 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:02,826 - root - INFO - Configuration validated successfully
2025-07-10 08:12:02,829 - root - INFO - Pre-warming the model...
2025-07-10 08:12:02,831 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:02,832 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:03,025 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:03,031 - root - INFO - Configuration validated successfully
2025-07-10 08:12:03,107 - root - INFO - Pre-warming the model...
2025-07-10 08:12:03,109 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:03,108 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:03,113 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:03,323 - root - INFO - Configuration validated successfully
2025-07-10 08:12:03,406 - root - INFO - Pre-warming the model...
2025-07-10 08:12:03,408 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:03,411 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:03,414 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:03,512 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:03,809 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:04,008 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:04,110 - root - INFO - Configuration validated successfully
2025-07-10 08:12:04,218 - root - INFO - Pre-warming the model...
2025-07-10 08:12:04,209 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:04,221 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:04,406 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:04,326 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:04,610 - root - INFO - Configuration validated successfully
2025-07-10 08:12:04,613 - root - INFO - Pre-warming the model...
2025-07-10 08:12:04,614 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:04,615 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:04,910 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:04,916 - root - INFO - Configuration validated successfully
2025-07-10 08:12:05,015 - root - INFO - Pre-warming the model...
2025-07-10 08:12:05,019 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:05,109 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:05,321 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:05,419 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:05,513 - root - INFO - Configuration validated successfully
2025-07-10 08:12:05,511 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:05,613 - root - INFO - Pre-warming the model...
2025-07-10 08:12:05,522 - root - INFO - Configuration validated successfully
2025-07-10 08:12:05,706 - root - INFO - Pre-warming the model...
2025-07-10 08:12:05,711 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:05,707 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:05,711 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:05,719 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:05,721 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:06,011 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:06,315 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:06,603 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:06,606 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:06,607 - root - INFO - Configuration validated successfully
2025-07-10 08:12:06,711 - root - INFO - Pre-warming the model...
2025-07-10 08:12:06,706 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:06,714 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:06,705 - root - INFO - Configuration validated successfully
2025-07-10 08:12:06,810 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:06,816 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:07,000 - root - INFO - Pre-warming the model...
2025-07-10 08:12:07,006 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:07,008 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:07,009 - root - INFO - Configuration validated successfully
2025-07-10 08:12:07,100 - root - INFO - Pre-warming the model...
2025-07-10 08:12:07,112 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:07,114 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:07,504 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:07,511 - root - INFO - Configuration validated successfully
2025-07-10 08:12:07,512 - root - INFO - Configuration validated successfully
2025-07-10 08:12:07,600 - root - INFO - Pre-warming the model...
2025-07-10 08:12:07,514 - root - INFO - Pre-warming the model...
2025-07-10 08:12:07,709 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:07,615 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:07,800 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:07,899 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:07,905 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:07,914 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:08,208 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:08,401 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:08,518 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:08,601 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:08,716 - root - INFO - Configuration validated successfully
2025-07-10 08:12:08,719 - root - INFO - Pre-warming the model...
2025-07-10 08:12:08,720 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:08,800 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:08,912 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:08,912 - root - INFO - Configuration validated successfully
2025-07-10 08:12:09,013 - root - INFO - Pre-warming the model...
2025-07-10 08:12:09,016 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:09,111 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:09,212 - root - INFO - Configuration validated successfully
2025-07-10 08:12:09,214 - root - INFO - Pre-warming the model...
2025-07-10 08:12:09,215 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:09,300 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:09,410 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:09,507 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:09,602 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:09,619 - root - INFO - Starting HR Assistant...
2025-07-10 08:12:09,717 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:10,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:10,410 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:10,505 - root - INFO - Configuration validated successfully
2025-07-10 08:12:10,607 - root - INFO - Pre-warming the model...
2025-07-10 08:12:10,608 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:12:10,708 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:10,710 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:12:10,807 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:11,103 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:11,300 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:11,321 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:12:11,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:11,500 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:12,506 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:12,514 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:12,711 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:12,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:13,015 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:13,017 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:13,018 - root - INFO - Loading groq LLM...
2025-07-10 08:12:13,101 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:13,202 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:13,312 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:13,705 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:13,712 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:13,802 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:13,802 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:14,000 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:14,007 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:14,015 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:14,014 - root - INFO - Loading groq LLM...
2025-07-10 08:12:14,206 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:14,300 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:14,303 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:14,305 - root - INFO - Loading groq LLM...
2025-07-10 08:12:14,501 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:14,703 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:14,724 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:14,709 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:14,805 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:14,803 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:14,909 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:15,024 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:15,026 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:15,028 - root - INFO - Loading groq LLM...
2025-07-10 08:12:15,030 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:15,100 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:15,103 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:15,202 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:15,203 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:15,205 - root - INFO - Loading groq LLM...
2025-07-10 08:12:15,222 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:15,300 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:15,306 - root - INFO - Loading groq LLM...
2025-07-10 08:12:15,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:15,403 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:15,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:15,907 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:15,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:16,101 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:16,013 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:16,202 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:16,508 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:16,510 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:16,512 - root - INFO - Loading groq LLM...
2025-07-10 08:12:16,526 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:16,602 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:16,606 - root - INFO - Loading groq LLM...
2025-07-10 08:12:16,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:16,810 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:17,024 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:17,302 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:17,305 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:17,407 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:17,412 - root - INFO - Loading groq LLM...
2025-07-10 08:12:17,505 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:17,512 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:17,505 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:17,602 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:17,606 - root - INFO - Loading groq LLM...
2025-07-10 08:12:17,507 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:17,711 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:18,106 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:18,111 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:18,112 - root - INFO - Loading groq LLM...
2025-07-10 08:12:18,112 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:18,204 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:18,300 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:18,400 - root - INFO - Loading groq LLM...
2025-07-10 08:12:18,416 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:18,506 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:18,510 - root - INFO - Loading groq LLM...
2025-07-10 08:12:18,903 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:18,914 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:18,903 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:19,000 - root - INFO - Loading groq LLM...
2025-07-10 08:12:19,104 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:19,200 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:19,210 - root - INFO - Loading groq LLM...
2025-07-10 08:12:19,412 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:12:20,005 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:20,009 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:20,101 - root - INFO - Loading groq LLM...
2025-07-10 08:12:20,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:20,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:20,604 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:20,701 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:20,799 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:20,805 - root - INFO - Loading groq LLM...
2025-07-10 08:12:21,214 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:21,816 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:22,201 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:22,405 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:22,505 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:22,512 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:22,801 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:23,510 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:24,002 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:24,401 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:24,408 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:24,501 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:24,501 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:24,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:25,205 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:25,422 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:25,505 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:25,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:26,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:26,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:26,303 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:26,508 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:27,101 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:27,301 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:27,502 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:27,502 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:28,101 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:28,401 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:28,408 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:28,411 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:28,406 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:29,502 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:29,804 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:29,808 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:29,917 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:12:30,404 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:30,499 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:30,610 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:30,707 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:30,800 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:12:30,809 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:12:30,902 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:31,402 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:31,607 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:31,614 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:31,804 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:32,411 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:32,509 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:32,703 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:32,723 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:32,724 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:33,200 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:33,206 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:34,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:34,410 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:34,515 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:34,710 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:34,712 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:34,919 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:34,920 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:35,421 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,421 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,453 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:35,454 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:35,460 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:35,461 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:35,594 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,642 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:35,645 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,645 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:35,687 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:35,688 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:35,922 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:12:35,923 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,923 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,923 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,924 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:12:35,938 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-07-10 08:12:35,940 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 28.000000 seconds
2025-07-10 08:12:35,956 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:35,957 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:36,001 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:36,004 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:36,004 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:36,005 - root - INFO - Model pre-warmed successfully
2025-07-10 08:12:36,007 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:36,008 - root - INFO - HR Assistant started successfully
2025-07-10 08:12:36,201 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:12:37,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:12:38,956 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:12:39,159 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:13:05,025 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:13:05,070 - root - INFO - Model pre-warmed successfully
2025-07-10 08:13:05,073 - root - INFO - HR Assistant started successfully
2025-07-10 08:13:30,285 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:13:30,286 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:13:36,898 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:13:36,901 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:13:36,904 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:13:36,907 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:13:38,036 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:13:38,037 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:13:38,267 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:13:38,267 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:13:38,999 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:13:38,999 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:13:39,258 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:13:39,258 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:13:57,846 - root - INFO - Received question: can you explain the dress code policy
2025-07-10 08:13:57,848 - root - INFO - Chain init: 0.00s
2025-07-10 08:13:58,654 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:14:00,784 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:14:01,949 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 08:14:01,961 - root - INFO - Question processing: 4.11s
2025-07-10 08:14:01,962 - root - INFO - ⏱️ ask_hr took 4.12 seconds
2025-07-10 08:14:36,056 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:14:36,058 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:14:36,059 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:14:36,061 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:14:37,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:14:37,225 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:14:39,106 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:14:39,107 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:14:39,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:14:39,305 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:16:27,539 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:16:27,544 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:16:34,293 - admin_interface - INFO - Backup created: hr_assistant_backup_20250710_081633.tar.gz
2025-07-10 08:16:38,671 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:16:38,672 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:17:06,045 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:17:06,048 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:17:06,050 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:17:06,052 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:17:07,288 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:17:08,275 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:17:08,461 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:17:36,010 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:17:36,012 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:17:36,019 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:17:36,023 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:17:36,923 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:17:37,749 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:17:37,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:18:06,012 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:18:06,013 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:18:06,015 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:18:06,016 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:18:07,528 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:18:08,350 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:18:08,533 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:18:39,061 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:18:39,886 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:18:40,131 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:18:40,137 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:18:40,138 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:18:40,139 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 08:18:40,140 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 08:19:22,543 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:22,633 - root - INFO - Configuration validated successfully
2025-07-10 08:19:22,636 - root - INFO - Pre-warming the model...
2025-07-10 08:19:22,638 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:22,639 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:22,650 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:22,837 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:22,838 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:22,938 - root - INFO - Configuration validated successfully
2025-07-10 08:19:22,940 - root - INFO - Configuration validated successfully
2025-07-10 08:19:22,944 - root - INFO - Pre-warming the model...
2025-07-10 08:19:22,950 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:22,946 - root - INFO - Pre-warming the model...
2025-07-10 08:19:23,029 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:23,035 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:23,049 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:23,128 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:23,234 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:23,334 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:23,531 - root - INFO - Configuration validated successfully
2025-07-10 08:19:23,532 - root - INFO - Pre-warming the model...
2025-07-10 08:19:23,533 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:23,535 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:23,549 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:23,945 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:24,038 - root - INFO - Configuration validated successfully
2025-07-10 08:19:24,128 - root - INFO - Pre-warming the model...
2025-07-10 08:19:24,138 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:24,140 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:24,230 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:24,238 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:24,237 - root - INFO - Configuration validated successfully
2025-07-10 08:19:24,328 - root - INFO - Pre-warming the model...
2025-07-10 08:19:24,336 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:24,337 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:24,329 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:24,334 - root - INFO - Configuration validated successfully
2025-07-10 08:19:24,438 - root - INFO - Pre-warming the model...
2025-07-10 08:19:24,441 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:24,442 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:24,445 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:24,541 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:24,832 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:24,834 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:24,837 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:25,041 - root - INFO - Configuration validated successfully
2025-07-10 08:19:25,133 - root - INFO - Pre-warming the model...
2025-07-10 08:19:25,148 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:25,140 - root - INFO - Configuration validated successfully
2025-07-10 08:19:25,236 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:25,240 - root - INFO - Configuration validated successfully
2025-07-10 08:19:25,340 - root - INFO - Pre-warming the model...
2025-07-10 08:19:25,342 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:25,335 - root - INFO - Pre-warming the model...
2025-07-10 08:19:25,345 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:25,432 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:25,440 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:25,528 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:25,736 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:25,836 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:25,842 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:25,934 - root - INFO - Configuration validated successfully
2025-07-10 08:19:26,033 - root - INFO - Pre-warming the model...
2025-07-10 08:19:26,034 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:26,037 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:26,340 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:26,345 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:26,436 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:26,731 - root - INFO - Configuration validated successfully
2025-07-10 08:19:26,732 - root - INFO - Configuration validated successfully
2025-07-10 08:19:26,733 - root - INFO - Pre-warming the model...
2025-07-10 08:19:26,736 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:26,739 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:26,735 - root - INFO - Pre-warming the model...
2025-07-10 08:19:26,828 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:26,835 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:26,933 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:26,934 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:26,944 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:27,144 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:27,137 - root - INFO - Configuration validated successfully
2025-07-10 08:19:27,239 - root - INFO - Pre-warming the model...
2025-07-10 08:19:27,240 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:27,330 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:27,238 - root - INFO - Configuration validated successfully
2025-07-10 08:19:27,532 - root - INFO - Pre-warming the model...
2025-07-10 08:19:27,529 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:27,537 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:27,540 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:27,729 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:27,929 - root - INFO - Configuration validated successfully
2025-07-10 08:19:27,935 - root - INFO - Pre-warming the model...
2025-07-10 08:19:27,942 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:27,939 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:28,042 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:28,233 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:29,841 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:30,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:30,239 - root - INFO - Configuration validated successfully
2025-07-10 08:19:30,241 - root - INFO - Pre-warming the model...
2025-07-10 08:19:30,242 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:30,245 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:30,539 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:30,549 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:30,544 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:31,233 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:31,433 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:31,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:31,829 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:32,136 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:32,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:32,050 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:32,234 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:32,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:33,230 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:33,234 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:33,331 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:33,531 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:33,538 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:33,826 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:33,932 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:34,032 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:34,035 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:34,039 - root - INFO - Loading google LLM...
2025-07-10 08:19:34,237 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:34,350 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:34,430 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:34,430 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:34,432 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:34,445 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:34,433 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:34,447 - root - INFO - Loading google LLM...
2025-07-10 08:19:34,753 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:34,753 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:34,754 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:34,757 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:34,763 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:34,769 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:34,782 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:34,791 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:34,829 - root - INFO - Loading google LLM...
2025-07-10 08:19:34,831 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:34,835 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:34,839 - root - INFO - Loading google LLM...
2025-07-10 08:19:34,842 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:34,853 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:34,864 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:34,869 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:34,928 - root - INFO - Loading google LLM...
2025-07-10 08:19:34,946 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:35,250 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,252 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,252 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,255 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:35,328 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,330 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,334 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:35,341 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:35,365 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:35,428 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:35,429 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,433 - root - INFO - Loading google LLM...
2025-07-10 08:19:35,439 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:35,441 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:35,443 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:35,446 - root - INFO - Loading google LLM...
2025-07-10 08:19:35,455 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:35,457 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:35,459 - root - INFO - Loading google LLM...
2025-07-10 08:19:35,652 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,653 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,988 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,988 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:35,995 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,019 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:36,021 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:36,027 - root - INFO - Loading google LLM...
2025-07-10 08:19:36,284 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:36,285 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:36,288 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,327 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:36,328 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:36,332 - root - INFO - Loading google LLM...
2025-07-10 08:19:36,562 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:36,568 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,573 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:36,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:36,617 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:36,618 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:36,620 - root - INFO - Loading google LLM...
2025-07-10 08:19:36,898 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:36,968 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,968 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,969 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,971 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,978 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:36,983 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:36,993 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:36,988 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:36,989 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:37,002 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:36,997 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:36,999 - root - INFO - Loading google LLM...
2025-07-10 08:19:37,001 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:37,004 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:37,012 - root - INFO - Loading google LLM...
2025-07-10 08:19:37,017 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:37,019 - root - INFO - Loading google LLM...
2025-07-10 08:19:37,027 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:37,028 - root - INFO - Loading google LLM...
2025-07-10 08:19:37,033 - root - INFO - Loading google LLM...
2025-07-10 08:19:37,077 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:37,123 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:37,124 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:37,460 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:37,461 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:37,461 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:37,461 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:37,462 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:37,462 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:37,462 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:37,540 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:37,720 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:38,043 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:19:38,068 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:38,071 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:38,074 - root - INFO - Loading google LLM...
2025-07-10 08:19:38,366 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:38,457 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:38,555 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:39,190 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:39,191 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:39,192 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:39,192 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:39,192 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:39,531 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:39,533 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:40,021 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:19:40,719 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:40,720 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:41,177 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:41,587 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:41,589 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:41,909 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:19:41,915 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:41,915 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:41,916 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:41,917 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:42,071 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:19:42,242 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:42,243 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:42,252 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:19:42,400 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:42,413 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:42,416 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:42,419 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:42,427 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:42,435 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:19:42,448 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:19:42,851 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:42,853 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:43,065 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:43,065 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:43,067 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:43,068 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:43,552 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:19:43,562 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:43,562 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:43,563 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:43,570 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:44,053 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:44,054 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:44,294 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 08:19:44,470 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:19:44,785 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:19:44,794 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:44,796 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:44,786 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 128, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 08:19:45,227 - root - INFO - Model pre-warmed successfully
2025-07-10 08:19:45,229 - root - INFO - HR Assistant started successfully
2025-07-10 08:19:59,637 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:59,650 - root - INFO - Configuration validated successfully
2025-07-10 08:19:59,655 - root - INFO - Pre-warming the model...
2025-07-10 08:19:59,657 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:59,658 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:59,738 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:19:59,753 - root - INFO - Starting HR Assistant...
2025-07-10 08:19:59,853 - root - INFO - Configuration validated successfully
2025-07-10 08:19:59,935 - root - INFO - Pre-warming the model...
2025-07-10 08:19:59,939 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:19:59,942 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:19:59,951 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:00,249 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:00,441 - root - INFO - Configuration validated successfully
2025-07-10 08:20:00,536 - root - INFO - Pre-warming the model...
2025-07-10 08:20:00,447 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:00,551 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:00,637 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:00,646 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:00,738 - root - INFO - Configuration validated successfully
2025-07-10 08:20:00,744 - root - INFO - Pre-warming the model...
2025-07-10 08:20:00,753 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:00,755 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:00,836 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:01,036 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:01,049 - root - INFO - Configuration validated successfully
2025-07-10 08:20:01,149 - root - INFO - Pre-warming the model...
2025-07-10 08:20:01,243 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:01,249 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:01,350 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:01,652 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:02,038 - root - INFO - Configuration validated successfully
2025-07-10 08:20:02,051 - root - INFO - Pre-warming the model...
2025-07-10 08:20:02,136 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:02,155 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:02,148 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:02,244 - root - INFO - Configuration validated successfully
2025-07-10 08:20:02,251 - root - INFO - Pre-warming the model...
2025-07-10 08:20:02,249 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:02,340 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:02,353 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:02,436 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:02,443 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:02,444 - root - INFO - Configuration validated successfully
2025-07-10 08:20:02,451 - root - INFO - Pre-warming the model...
2025-07-10 08:20:02,545 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:02,551 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:02,644 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:03,054 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:03,336 - root - INFO - Configuration validated successfully
2025-07-10 08:20:03,436 - root - INFO - Pre-warming the model...
2025-07-10 08:20:03,341 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:03,539 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:03,644 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:03,642 - root - INFO - Configuration validated successfully
2025-07-10 08:20:03,835 - root - INFO - Pre-warming the model...
2025-07-10 08:20:03,936 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:03,955 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:03,948 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:04,056 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:04,042 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:04,157 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:04,236 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:04,343 - root - INFO - Configuration validated successfully
2025-07-10 08:20:04,347 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:04,350 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:04,344 - root - INFO - Configuration validated successfully
2025-07-10 08:20:04,344 - root - INFO - Configuration validated successfully
2025-07-10 08:20:04,445 - root - INFO - Pre-warming the model...
2025-07-10 08:20:04,353 - root - INFO - Pre-warming the model...
2025-07-10 08:20:04,454 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:04,440 - root - INFO - Pre-warming the model...
2025-07-10 08:20:04,548 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:04,440 - root - INFO - Configuration validated successfully
2025-07-10 08:20:04,647 - root - INFO - Pre-warming the model...
2025-07-10 08:20:04,653 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:04,445 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:04,445 - root - INFO - Configuration validated successfully
2025-07-10 08:20:04,751 - root - INFO - Pre-warming the model...
2025-07-10 08:20:04,836 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:04,446 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:04,848 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:04,536 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:04,636 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:04,736 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:05,041 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:04,836 - root - INFO - Configuration validated successfully
2025-07-10 08:20:05,144 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:04,846 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:05,145 - root - INFO - Pre-warming the model...
2025-07-10 08:20:04,946 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:05,238 - root - INFO - Starting HR Assistant...
2025-07-10 08:20:05,040 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:05,154 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:05,347 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:05,346 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:05,441 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:05,342 - root - INFO - Configuration validated successfully
2025-07-10 08:20:05,546 - root - INFO - Pre-warming the model...
2025-07-10 08:20:05,549 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:20:05,636 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:20:05,843 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:20:06,132 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:06,838 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:06,936 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:07,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:07,834 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:08,027 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:08,329 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:08,634 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:08,935 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:09,030 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:09,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:09,430 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:09,631 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:09,644 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:09,726 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:09,727 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:09,733 - root - INFO - Loading google LLM...
2025-07-10 08:20:10,312 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:10,314 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:10,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:10,318 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,333 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:10,334 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:10,334 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:10,336 - root - INFO - Loading google LLM...
2025-07-10 08:20:10,354 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:10,355 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:10,356 - root - INFO - Loading google LLM...
2025-07-10 08:20:10,667 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:10,667 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:10,668 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,669 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,716 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:10,716 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:10,972 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,972 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,973 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,973 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,973 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:10,973 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:20:11,175 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,379 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,381 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:11,382 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:11,388 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:11,388 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:11,389 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:11,391 - root - INFO - Loading google LLM...
2025-07-10 08:20:11,390 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:11,394 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,394 - root - INFO - Loading google LLM...
2025-07-10 08:20:11,550 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:11,577 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:11,578 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:11,579 - root - INFO - Loading google LLM...
2025-07-10 08:20:11,713 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,714 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,735 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,736 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,736 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,736 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,736 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,737 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:11,736 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:20:11,838 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:20:12,051 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:12,051 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:20:12,060 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:12,064 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:20:12,065 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:12,066 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:20:12,067 - root - INFO - Loading google LLM...
2025-07-10 08:20:12,067 - root - INFO - Loading google LLM...
2025-07-10 08:22:37,450 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:37,462 - root - INFO - Configuration validated successfully
2025-07-10 08:22:37,463 - root - INFO - Pre-warming the model...
2025-07-10 08:22:37,465 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:37,466 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:37,524 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:37,727 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:37,748 - root - INFO - Configuration validated successfully
2025-07-10 08:22:37,824 - root - INFO - Pre-warming the model...
2025-07-10 08:22:37,826 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:37,828 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:37,931 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:38,031 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:38,233 - root - INFO - Configuration validated successfully
2025-07-10 08:22:38,323 - root - INFO - Pre-warming the model...
2025-07-10 08:22:38,328 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:38,336 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:38,426 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:38,625 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:38,730 - root - INFO - Configuration validated successfully
2025-07-10 08:22:38,824 - root - INFO - Pre-warming the model...
2025-07-10 08:22:38,828 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:38,832 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:38,830 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:38,932 - root - INFO - Configuration validated successfully
2025-07-10 08:22:39,026 - root - INFO - Pre-warming the model...
2025-07-10 08:22:39,041 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:39,025 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:39,044 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:39,126 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:39,327 - root - INFO - Configuration validated successfully
2025-07-10 08:22:39,330 - root - INFO - Pre-warming the model...
2025-07-10 08:22:39,335 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:39,332 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:39,425 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:39,628 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:39,738 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:39,837 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:39,933 - root - INFO - Configuration validated successfully
2025-07-10 08:22:39,934 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:40,024 - root - INFO - Pre-warming the model...
2025-07-10 08:22:40,034 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:40,028 - root - INFO - Configuration validated successfully
2025-07-10 08:22:40,142 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:40,128 - root - INFO - Configuration validated successfully
2025-07-10 08:22:40,324 - root - INFO - Pre-warming the model...
2025-07-10 08:22:40,129 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:40,140 - root - INFO - Pre-warming the model...
2025-07-10 08:22:40,336 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:40,327 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:40,430 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:40,343 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:40,432 - root - INFO - Configuration validated successfully
2025-07-10 08:22:40,441 - root - INFO - Pre-warming the model...
2025-07-10 08:22:40,524 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:40,424 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:40,527 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:40,533 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:40,534 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:40,547 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:40,739 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:40,832 - root - INFO - Configuration validated successfully
2025-07-10 08:22:40,837 - root - INFO - Pre-warming the model...
2025-07-10 08:22:40,839 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:40,928 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:41,124 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:41,133 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:41,232 - root - INFO - Configuration validated successfully
2025-07-10 08:22:41,236 - root - INFO - Pre-warming the model...
2025-07-10 08:22:41,329 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:41,424 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:41,540 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:41,731 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:41,926 - root - INFO - Configuration validated successfully
2025-07-10 08:22:42,023 - root - INFO - Pre-warming the model...
2025-07-10 08:22:42,027 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:42,032 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:42,040 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:42,138 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:42,225 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:42,242 - root - INFO - Configuration validated successfully
2025-07-10 08:22:42,332 - root - INFO - Pre-warming the model...
2025-07-10 08:22:42,333 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:42,332 - root - INFO - Configuration validated successfully
2025-07-10 08:22:42,335 - root - INFO - Pre-warming the model...
2025-07-10 08:22:42,334 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:42,335 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:42,434 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:42,433 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:42,432 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:42,627 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:42,826 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:42,928 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:43,031 - root - INFO - Starting HR Assistant...
2025-07-10 08:22:43,035 - root - INFO - Configuration validated successfully
2025-07-10 08:22:43,135 - root - INFO - Pre-warming the model...
2025-07-10 08:22:43,136 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:43,138 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:43,224 - root - INFO - Configuration validated successfully
2025-07-10 08:22:43,234 - root - INFO - Pre-warming the model...
2025-07-10 08:22:43,235 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:22:43,237 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:22:43,330 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:43,326 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:22:43,425 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:43,526 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:44,034 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:44,537 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:44,925 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:44,927 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:45,227 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:45,238 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:45,338 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:45,425 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:45,437 - root - INFO - Loading google LLM...
2025-07-10 08:22:45,429 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:45,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:45,926 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:45,928 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:46,276 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:46,326 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:46,326 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:46,330 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:46,344 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:46,346 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:46,349 - root - INFO - Loading google LLM...
2025-07-10 08:22:46,438 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:46,599 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:46,627 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:46,925 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:46,942 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:46,932 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:46,933 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:46,944 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:47,025 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:47,030 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:47,126 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:47,130 - root - INFO - Loading google LLM...
2025-07-10 08:22:47,137 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:47,138 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:47,143 - root - INFO - Loading google LLM...
2025-07-10 08:22:47,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:47,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:47,425 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:47,525 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:47,525 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:47,528 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:47,531 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:47,533 - root - INFO - Loading google LLM...
2025-07-10 08:22:47,545 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:47,548 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:47,553 - root - INFO - Loading google LLM...
2025-07-10 08:22:47,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:47,634 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:47,653 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:47,724 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:47,726 - root - INFO - Loading google LLM...
2025-07-10 08:22:47,752 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:47,824 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:47,996 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:47,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:48,001 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:48,004 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:48,005 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:48,020 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:48,058 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:48,061 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:48,330 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:22:48,579 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:48,582 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:48,585 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:48,603 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:48,624 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:48,626 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:48,627 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:48,630 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:48,630 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:48,632 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:48,641 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:48,639 - root - INFO - Loading google LLM...
2025-07-10 08:22:48,649 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:48,647 - root - INFO - Loading google LLM...
2025-07-10 08:22:48,647 - root - INFO - Loading google LLM...
2025-07-10 08:22:48,652 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:48,661 - root - INFO - Loading google LLM...
2025-07-10 08:22:48,902 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:48,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:48,941 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,028 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:49,056 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:49,058 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:49,060 - root - INFO - Loading google LLM...
2025-07-10 08:22:49,092 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,092 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:49,126 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,126 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,152 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:49,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,225 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:49,228 - root - INFO - Loading google LLM...
2025-07-10 08:22:49,643 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:49,648 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:49,651 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,657 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:49,664 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:49,682 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:49,684 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:49,687 - root - INFO - Loading google LLM...
2025-07-10 08:22:49,973 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:49,976 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:49,976 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:50,015 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:50,145 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:50,147 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:50,156 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:50,157 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:50,157 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:50,158 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:50,159 - root - INFO - Loading google LLM...
2025-07-10 08:22:50,160 - root - INFO - Loading google LLM...
2025-07-10 08:22:50,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:50,427 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:50,632 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:50,633 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:50,633 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:50,635 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:51,149 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:22:51,157 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:22:51,159 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:22:51,160 - root - INFO - Loading google LLM...
2025-07-10 08:22:51,224 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:51,227 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:51,502 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:22:51,620 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:51,793 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:51,826 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:51,957 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:51,958 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:51,965 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:52,199 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:52,825 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:52,826 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:53,015 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:22:53,103 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:53,104 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:53,106 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:53,107 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:53,281 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:53,284 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:53,766 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:53,767 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:54,006 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:22:54,257 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:54,258 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:54,424 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:54,424 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:54,425 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:54,426 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:54,681 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:22:54,917 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:54,919 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:55,196 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:55,197 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:55,245 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:55,247 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:55,493 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:55,494 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:56,233 - root - INFO - Model pre-warmed successfully
2025-07-10 08:22:56,234 - root - INFO - HR Assistant started successfully
2025-07-10 08:22:56,409 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 08:22:56,986 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 08:23:10,968 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:10,988 - root - INFO - Configuration validated successfully
2025-07-10 08:23:10,989 - root - INFO - Pre-warming the model...
2025-07-10 08:23:10,997 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:10,998 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:11,054 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:11,153 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:11,177 - root - INFO - Configuration validated successfully
2025-07-10 08:23:11,252 - root - INFO - Pre-warming the model...
2025-07-10 08:23:11,276 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:11,280 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:11,353 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:11,774 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:11,870 - root - INFO - Configuration validated successfully
2025-07-10 08:23:11,952 - root - INFO - Pre-warming the model...
2025-07-10 08:23:11,954 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:11,957 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:11,971 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:12,158 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:12,265 - root - INFO - Configuration validated successfully
2025-07-10 08:23:12,352 - root - INFO - Pre-warming the model...
2025-07-10 08:23:12,354 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:12,365 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:12,364 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:12,460 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:12,470 - root - INFO - Configuration validated successfully
2025-07-10 08:23:12,552 - root - INFO - Pre-warming the model...
2025-07-10 08:23:12,564 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:12,654 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:12,671 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:12,763 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:12,969 - root - INFO - Configuration validated successfully
2025-07-10 08:23:13,053 - root - INFO - Pre-warming the model...
2025-07-10 08:23:13,060 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:13,065 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:13,159 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:13,164 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:13,262 - root - INFO - Configuration validated successfully
2025-07-10 08:23:13,352 - root - INFO - Pre-warming the model...
2025-07-10 08:23:13,355 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:13,361 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:13,466 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:13,560 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:13,558 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:13,754 - root - INFO - Configuration validated successfully
2025-07-10 08:23:13,765 - root - INFO - Pre-warming the model...
2025-07-10 08:23:13,852 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:13,767 - root - INFO - Configuration validated successfully
2025-07-10 08:23:13,953 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:13,957 - root - INFO - Pre-warming the model...
2025-07-10 08:23:13,965 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:14,053 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:14,054 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:14,064 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:14,162 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:14,266 - root - INFO - Configuration validated successfully
2025-07-10 08:23:14,355 - root - INFO - Pre-warming the model...
2025-07-10 08:23:14,360 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:14,365 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:14,656 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:14,860 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:14,870 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:15,056 - root - INFO - Configuration validated successfully
2025-07-10 08:23:15,062 - root - INFO - Pre-warming the model...
2025-07-10 08:23:15,064 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:15,068 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:15,153 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:15,070 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:15,154 - root - INFO - Configuration validated successfully
2025-07-10 08:23:15,168 - root - INFO - Configuration validated successfully
2025-07-10 08:23:15,173 - root - INFO - Pre-warming the model...
2025-07-10 08:23:15,175 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:15,253 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:15,252 - root - INFO - Pre-warming the model...
2025-07-10 08:23:15,261 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:15,259 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:15,265 - root - INFO - Configuration validated successfully
2025-07-10 08:23:15,267 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:15,357 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:15,365 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:15,355 - root - INFO - Pre-warming the model...
2025-07-10 08:23:15,454 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:15,455 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:15,564 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:16,264 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:16,358 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:16,455 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:16,569 - root - INFO - Configuration validated successfully
2025-07-10 08:23:16,652 - root - INFO - Pre-warming the model...
2025-07-10 08:23:16,664 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:16,669 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:16,669 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:16,757 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:16,858 - root - INFO - Configuration validated successfully
2025-07-10 08:23:16,952 - root - INFO - Pre-warming the model...
2025-07-10 08:23:16,963 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:16,964 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:17,055 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:17,062 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:17,357 - root - INFO - Configuration validated successfully
2025-07-10 08:23:17,366 - root - INFO - Pre-warming the model...
2025-07-10 08:23:17,452 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:17,463 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:17,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:17,656 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:17,558 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:17,665 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:18,064 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:19,053 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:19,056 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:19,354 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:19,755 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:19,960 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:19,961 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:20,056 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,059 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,065 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,152 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:20,162 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:20,167 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:20,173 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:20,178 - root - INFO - Loading google LLM...
2025-07-10 08:23:20,174 - root - INFO - Loading google LLM...
2025-07-10 08:23:20,253 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:20,258 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,453 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,456 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,678 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:20,682 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,753 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:20,679 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:20,774 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:20,776 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:20,781 - root - INFO - Loading google LLM...
2025-07-10 08:23:20,926 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:20,953 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:20,959 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:20,966 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:20,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,058 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:21,155 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,157 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:21,265 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,263 - root - INFO - Loading google LLM...
2025-07-10 08:23:21,554 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,673 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:21,680 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:21,681 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:21,682 - root - INFO - Loading google LLM...
2025-07-10 08:23:21,913 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,921 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,921 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,921 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:21,924 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:22,036 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:22,115 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,122 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:22,123 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,125 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,126 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:22,488 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,488 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,494 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,496 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,497 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,503 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,504 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,505 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,662 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:22,662 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:22,667 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:22,672 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,684 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,685 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,686 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,820 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:22,820 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:22,823 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,823 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,825 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,825 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:22,831 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,832 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,832 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,833 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,835 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,837 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,834 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,839 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,835 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,841 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:22,852 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:22,853 - root - INFO - Loading google LLM...
2025-07-10 08:23:22,989 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:22,989 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:22,994 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:23,004 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:23,058 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:23,061 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:23,063 - root - INFO - Loading google LLM...
2025-07-10 08:23:23,311 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:23,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:23,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:23,321 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:23,486 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:23,493 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:23,504 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:23,517 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:23,518 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,061 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:24,066 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:24,063 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,071 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:24,073 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:24,074 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:24,076 - root - INFO - Loading google LLM...
2025-07-10 08:23:24,079 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:24,080 - root - INFO - Loading google LLM...
2025-07-10 08:23:24,303 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,632 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:24,633 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,640 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:24,640 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,651 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:24,656 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:24,658 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:24,658 - root - INFO - Loading google LLM...
2025-07-10 08:23:24,848 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,850 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,852 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,864 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:24,957 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:25,064 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:26,252 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:26,254 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:26,262 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:26,282 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:26,401 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:26,403 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:26,569 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:23:27,402 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:27,404 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:27,693 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:27,695 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:27,707 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:27,708 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:28,325 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:23:28,411 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:23:28,580 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:28,582 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:28,743 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:28,745 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:28,824 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:28,825 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:29,809 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:29,810 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:29,810 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:29,812 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:30,731 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:30,732 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:30,726 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 08:23:30,741 - root - ERROR - Startup failed: 429 Resource has been exhausted (e.g. check quota).
Traceback (most recent call last):
  File "/app/main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4772, in invoke
    return self._call_with_config(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/app/ingestion_retrieval/retrieval.py", line 306, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 1940, in _call_with_config
    context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3774, in <dictcomp>
    output = {key: future.result() for key, future in zip(steps, futures)}
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3758, in _invoke_step
    return context.run(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 5431, in invoke
    return self.bound.invoke(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 389, in invoke
    self.generate_prompt(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 766, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 973, in generate
    return self._generate_helper(
  File "/opt/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 792, in _generate_helper
    self._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/llms.py", line 102, in _generate
    chat_result = self.client._generate(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 1441, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 231, in _chat_with_retry
    return _chat_with_retry(**params)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 222, in _chat_with_retry
    raise e
  File "/opt/venv/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 206, in _chat_with_retry
    return generation_method(**kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 868, in generate_content
    response = rpc(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 156, in retry_target
    next_sleep = _retry_error_helper(
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py", line 214, in _retry_error_helper
    raise final_exc from source_exc
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).
2025-07-10 08:23:30,970 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:30,973 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:30,980 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:30,988 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:31,811 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:31,812 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:33,039 - root - INFO - Model pre-warmed successfully
2025-07-10 08:23:33,041 - root - INFO - HR Assistant started successfully
2025-07-10 08:23:47,076 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:47,087 - root - INFO - Configuration validated successfully
2025-07-10 08:23:47,089 - root - INFO - Pre-warming the model...
2025-07-10 08:23:47,090 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:47,140 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:47,148 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:47,362 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:47,542 - root - INFO - Configuration validated successfully
2025-07-10 08:23:47,543 - root - INFO - Pre-warming the model...
2025-07-10 08:23:47,545 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:47,549 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:47,751 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:47,845 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:47,951 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:47,953 - root - INFO - Configuration validated successfully
2025-07-10 08:23:47,956 - root - INFO - Pre-warming the model...
2025-07-10 08:23:48,040 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:48,042 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:48,063 - root - INFO - Configuration validated successfully
2025-07-10 08:23:48,140 - root - INFO - Pre-warming the model...
2025-07-10 08:23:48,141 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:48,146 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:48,240 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:48,256 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:48,546 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:48,659 - root - INFO - Configuration validated successfully
2025-07-10 08:23:48,741 - root - INFO - Pre-warming the model...
2025-07-10 08:23:48,746 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:48,750 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:48,855 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:48,949 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:49,152 - root - INFO - Configuration validated successfully
2025-07-10 08:23:49,240 - root - INFO - Pre-warming the model...
2025-07-10 08:23:49,340 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:49,347 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:49,447 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:49,748 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:49,941 - root - INFO - Configuration validated successfully
2025-07-10 08:23:49,950 - root - INFO - Pre-warming the model...
2025-07-10 08:23:49,957 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:50,040 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:50,145 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:50,448 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:50,655 - root - INFO - Configuration validated successfully
2025-07-10 08:23:50,740 - root - INFO - Pre-warming the model...
2025-07-10 08:23:50,840 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:50,849 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:50,959 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:51,449 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:51,745 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:51,943 - root - INFO - Configuration validated successfully
2025-07-10 08:23:51,947 - root - INFO - Pre-warming the model...
2025-07-10 08:23:51,945 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:52,047 - root - INFO - Configuration validated successfully
2025-07-10 08:23:52,140 - root - INFO - Pre-warming the model...
2025-07-10 08:23:52,054 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:52,243 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:52,248 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:52,243 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:52,349 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:52,341 - root - INFO - Configuration validated successfully
2025-07-10 08:23:52,452 - root - INFO - Pre-warming the model...
2025-07-10 08:23:52,349 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:52,455 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:52,550 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:52,549 - root - INFO - Configuration validated successfully
2025-07-10 08:23:52,547 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:52,647 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:52,740 - root - INFO - Pre-warming the model...
2025-07-10 08:23:52,852 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:52,850 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:52,941 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:53,157 - root - INFO - Configuration validated successfully
2025-07-10 08:23:53,341 - root - INFO - Pre-warming the model...
2025-07-10 08:23:53,353 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:53,355 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:53,553 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:53,641 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:53,960 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:54,244 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:54,246 - root - INFO - Configuration validated successfully
2025-07-10 08:23:54,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:54,348 - root - INFO - Pre-warming the model...
2025-07-10 08:23:54,440 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:54,447 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:54,551 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:54,561 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:54,651 - root - INFO - Configuration validated successfully
2025-07-10 08:23:54,741 - root - INFO - Pre-warming the model...
2025-07-10 08:23:54,740 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:54,751 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:54,840 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:54,842 - root - INFO - Configuration validated successfully
2025-07-10 08:23:54,948 - root - INFO - Pre-warming the model...
2025-07-10 08:23:54,944 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:55,040 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:55,049 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:55,045 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:55,157 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:55,247 - root - INFO - Starting HR Assistant...
2025-07-10 08:23:55,342 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:55,349 - root - INFO - Configuration validated successfully
2025-07-10 08:23:55,450 - root - INFO - Pre-warming the model...
2025-07-10 08:23:55,443 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:55,452 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 08:23:55,544 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 08:23:55,557 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 08:23:56,048 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:56,346 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:56,346 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:56,744 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:57,246 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:57,443 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:57,542 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:57,542 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:57,643 - root - INFO - Loading google LLM...
2025-07-10 08:23:57,643 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:57,743 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:57,748 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:57,757 - root - INFO - Loading google LLM...
2025-07-10 08:23:57,942 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:57,942 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:58,245 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:58,550 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:58,642 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:58,689 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:58,743 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:58,747 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:58,949 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:58,961 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:58,963 - root - INFO - Loading google LLM...
2025-07-10 08:23:59,041 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:59,044 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:59,055 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:59,058 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:59,061 - root - INFO - Loading google LLM...
2025-07-10 08:23:59,142 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:59,267 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:59,510 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:59,510 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:59,510 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:59,515 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:59,515 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:59,516 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:59,808 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:23:59,813 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:23:59,814 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:23:59,827 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:23:59,828 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:23:59,829 - root - INFO - Loading google LLM...
2025-07-10 08:24:00,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,005 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:24:00,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:24:00,011 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:24:00,148 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,184 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,198 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:00,208 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:00,210 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:00,212 - root - INFO - Loading google LLM...
2025-07-10 08:24:00,262 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:00,592 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,596 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,602 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:00,603 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:00,648 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:00,650 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:00,653 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:00,655 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:00,658 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:00,661 - root - INFO - Loading google LLM...
2025-07-10 08:24:00,662 - root - INFO - Loading google LLM...
2025-07-10 08:24:00,741 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:00,743 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:00,744 - root - INFO - Loading google LLM...
2025-07-10 08:24:00,831 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,832 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:00,832 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:00,832 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:01,046 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:01,051 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:01,067 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:01,141 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:01,142 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:01,142 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:01,249 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:01,145 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:01,148 - root - INFO - Loading google LLM...
2025-07-10 08:24:01,347 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:01,241 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:01,246 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:01,259 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:01,358 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:01,444 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:01,453 - root - INFO - Loading google LLM...
2025-07-10 08:24:01,455 - root - INFO - Loading google LLM...
2025-07-10 08:24:01,455 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:01,459 - root - INFO - Loading google LLM...
2025-07-10 08:24:01,548 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:01,747 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:02,042 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:02,045 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:02,056 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:02,059 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:02,061 - root - INFO - Loading google LLM...
2025-07-10 08:24:02,233 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:02,235 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:02,235 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:02,236 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:02,238 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:02,250 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:02,251 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:02,253 - root - INFO - Loading google LLM...
2025-07-10 08:24:02,477 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:02,496 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:02,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:02,722 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:02,733 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:02,735 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:02,736 - root - INFO - Loading google LLM...
2025-07-10 08:24:02,892 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 08:24:02,901 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:02,903 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:02,904 - root - INFO - Loading google LLM...
2025-07-10 08:24:03,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:03,226 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:03,321 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:03,542 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:03,601 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:03,602 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:03,883 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:03,884 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:03,884 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:03,884 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:03,893 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:04,158 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:04,159 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:04,740 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:04,941 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 08:24:04,959 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:04,961 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:04,963 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:04,965 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:05,695 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:05,697 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:05,851 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:05,852 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:06,019 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:06,021 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:06,440 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:06,442 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:06,444 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:06,445 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:06,663 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:24:06,663 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..
2025-07-10 08:24:06,833 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:06,834 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:07,184 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:07,186 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:07,492 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:07,494 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:07,812 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:24:07,813 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:24:08,161 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:08,162 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:08,637 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:24:08,637 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:24:08,645 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:08,647 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:08,887 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:08,887 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:24:08,899 - root - INFO - Model pre-warmed successfully
2025-07-10 08:24:08,903 - root - INFO - HR Assistant started successfully
2025-07-10 08:24:13,250 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:13,252 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:45,142 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:45,148 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:45,151 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:45,154 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:24:48,206 - admin_interface - ERROR - Error getting system status: [Errno -2] Name or service not known
2025-07-10 08:24:50,363 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:24:50,365 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:27:11,218 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:27:11,302 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:27:11,304 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:27:11,305 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:27:11,863 - admin_interface - ERROR - Error getting system status: [Errno -2] Name or service not known
2025-07-10 08:27:14,569 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:27:14,570 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:27:27,426 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:27:27,428 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:27:27,431 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:27:27,440 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 08:27:33,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:27:34,395 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:27:34,502 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 08:27:34,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:27:35,138 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 08:27:35,326 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 08:27:40,140 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 08:27:40,142 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:31:59,223 - root - INFO - Starting HR Assistant...
2025-07-10 01:31:59,359 - root - INFO - Configuration validated successfully
2025-07-10 01:31:59,359 - root - INFO - Pre-warming the model...
2025-07-10 01:31:59,359 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 01:31:59,359 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 01:31:59,495 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 01:32:02,538 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:32:03,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:32:04,676 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 01:32:04,821 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:32:04,823 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:32:04,823 - root - INFO - Loading google LLM...
2025-07-10 01:32:05,076 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:32:06,632 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 01:32:09,701 - root - INFO - Model pre-warmed successfully
2025-07-10 01:32:09,701 - root - INFO - HR Assistant started successfully
2025-07-10 01:32:16,824 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:32:16,826 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:32:16,827 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:32:16,827 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:32:19,168 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:32:19,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:32:20,072 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 01:32:20,313 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:32:20,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 01:32:20,534 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:32:24,152 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:32:24,152 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:37:02,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:37:02,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 01:37:03,268 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 01:37:03,441 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 01:37:03,446 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:37:03,446 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:37:03,447 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 01:37:03,447 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 01:37:03,620 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 01:37:03,845 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:14:11,804 - root - INFO - Starting HR Assistant...
2025-07-10 02:14:11,947 - root - INFO - Configuration validated successfully
2025-07-10 02:14:11,947 - root - INFO - Pre-warming the model...
2025-07-10 02:14:11,949 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 02:14:11,949 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 02:14:12,135 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 02:14:15,245 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:14:16,069 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:14:17,810 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 02:14:17,948 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 02:14:17,954 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 02:14:17,955 - root - INFO - Loading google LLM...
2025-07-10 02:14:18,238 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:14:19,770 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:14:23,900 - root - INFO - Model pre-warmed successfully
2025-07-10 02:14:23,900 - root - INFO - HR Assistant started successfully
2025-07-10 02:14:38,985 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 02:14:38,988 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 02:14:38,989 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 02:14:38,989 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 02:14:41,246 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:14:41,420 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:14:41,810 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:14:42,004 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:14:42,009 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:14:42,188 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:14:48,359 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 02:14:48,359 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 02:21:01,010 - root - INFO - Received question: what is the feul limit of all grades
2025-07-10 02:21:01,017 - root - INFO - Chain init: 0.00s
2025-07-10 02:21:02,116 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:21:04,542 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:21:07,926 - root - INFO - Question processing: 6.91s
2025-07-10 02:21:07,926 - root - INFO - ⏱️ ask_hr took 6.92 seconds
2025-07-10 02:21:36,106 - root - INFO - Received question: from grade m-5 to grade m-7
2025-07-10 02:21:36,106 - root - INFO - Chain init: 0.00s
2025-07-10 02:21:36,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:21:38,575 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:21:41,529 - root - INFO - Question processing: 5.42s
2025-07-10 02:21:41,529 - root - INFO - ⏱️ ask_hr took 5.42 seconds
2025-07-10 02:23:16,443 - root - INFO - Received question: can you tell the previous question?
2025-07-10 02:23:16,443 - root - INFO - Chain init: 0.00s
2025-07-10 02:23:17,178 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:23:18,713 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:23:22,577 - root - INFO - Question processing: 6.13s
2025-07-10 02:23:22,577 - root - INFO - ⏱️ ask_hr took 6.13 seconds
2025-07-10 02:24:06,068 - root - INFO - Received question: Fuel policy of All Grade
2025-07-10 02:24:06,069 - root - INFO - Chain init: 0.00s
2025-07-10 02:24:06,746 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:24:08,309 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:24:11,460 - root - INFO - Question processing: 5.39s
2025-07-10 02:24:11,460 - root - INFO - ⏱️ ask_hr took 5.39 seconds
2025-07-10 02:24:37,047 - root - INFO - Received question: I am asking about how many liters are alot
2025-07-10 02:24:37,047 - root - INFO - Chain init: 0.00s
2025-07-10 02:24:38,192 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:24:39,612 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:24:43,627 - root - INFO - Question processing: 6.58s
2025-07-10 02:24:43,627 - root - INFO - ⏱️ ask_hr took 6.58 seconds
2025-07-10 02:25:54,720 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-10 02:25:54,722 - admin_interface - INFO - Successfully updated .env file
2025-07-10 02:25:58,137 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:25:58,137 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:26:12,778 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:26:13,481 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:26:13,653 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:26:13,658 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 02:26:13,658 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 02:26:13,660 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 02:26:13,660 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 02:26:41,036 - root - INFO - Starting HR Assistant...
2025-07-10 02:26:41,180 - root - INFO - Configuration validated successfully
2025-07-10 02:26:41,182 - root - INFO - Pre-warming the model...
2025-07-10 02:26:41,182 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 02:26:41,182 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 02:26:41,319 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 02:26:43,619 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:26:44,299 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:26:45,275 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 02:26:45,418 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:26:45,418 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:26:45,418 - root - INFO - Loading groq LLM...
2025-07-10 02:26:47,339 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:26:49,046 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:26:50,551 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:26:50,614 - root - INFO - Model pre-warmed successfully
2025-07-10 02:26:50,615 - root - INFO - HR Assistant started successfully
2025-07-10 02:26:50,620 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:26:50,620 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:26:50,620 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:26:50,623 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:26:52,996 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:26:53,737 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:26:53,917 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:26:59,753 - root - INFO - Received question: is M7 fuel policy is 200 liter
2025-07-10 02:26:59,754 - root - INFO - Chain init: 0.00s
2025-07-10 02:27:00,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:27:02,704 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:27:04,595 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:27:04,600 - root - INFO - Question processing: 4.85s
2025-07-10 02:27:04,600 - root - INFO - ⏱️ ask_hr took 4.85 seconds
2025-07-10 02:27:11,422 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:27:12,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:27:12,576 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:27:12,583 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:27:12,583 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:27:12,583 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:27:12,585 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:27:23,096 - root - INFO - Received question: Monthly Fuel Consumption Limit"
2025-07-10 02:27:23,096 - root - INFO - Chain init: 0.00s
2025-07-10 02:27:31,703 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:27:33,517 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:27:35,144 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:27:35,153 - root - INFO - Question processing: 12.06s
2025-07-10 02:27:35,153 - root - INFO - ⏱️ ask_hr took 12.06 seconds
2025-07-10 02:28:12,477 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:28:13,028 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:28:13,457 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:28:13,466 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:28:13,467 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:28:13,467 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:28:13,467 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:28:42,034 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:28:43,102 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:28:43,431 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:28:43,438 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:28:43,438 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:28:43,438 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:28:43,440 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:29:10,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:29:12,063 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:29:12,312 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:29:12,318 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:29:12,319 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:29:12,321 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:29:12,321 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:29:35,379 - root - INFO - Received question: 	Car Fuel Consumption Limit for Employees \
2025-07-10 02:29:35,383 - root - INFO - Chain init: 0.00s
2025-07-10 02:29:36,330 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:29:38,200 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:29:41,127 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:29:41,133 - root - INFO - Question processing: 5.75s
2025-07-10 02:29:41,133 - root - INFO - ⏱️ ask_hr took 5.75 seconds
2025-07-10 02:29:43,500 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:29:44,337 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:29:44,531 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:29:44,536 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:29:44,536 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:29:44,537 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:29:44,537 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:30:06,339 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:30:07,034 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:30:07,359 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:30:11,457 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:30:12,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:30:15,676 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:30:16,399 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:30:16,983 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:30:19,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:30:20,841 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:30:21,250 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:30:30,153 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 02:30:30,155 - root - WARNING - Path does not exist: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 02:30:30,155 - root - WARNING - HTTPException in ingest_api: Path does not exist: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 02:32:40,584 - root - INFO - Starting HR Assistant...
2025-07-10 02:32:40,729 - root - INFO - Configuration validated successfully
2025-07-10 02:32:40,731 - root - INFO - Pre-warming the model...
2025-07-10 02:32:40,731 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 02:32:40,731 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 02:32:40,879 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 02:32:43,723 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:32:44,779 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:32:46,919 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 02:32:47,076 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:32:47,077 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:32:47,077 - root - INFO - Loading groq LLM...
2025-07-10 02:32:49,147 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:32:51,536 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:32:52,729 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:32:52,749 - root - INFO - Model pre-warmed successfully
2025-07-10 02:32:52,750 - root - INFO - HR Assistant started successfully
2025-07-10 02:33:09,673 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:33:09,684 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:33:10,710 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:33:10,711 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:33:10,984 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:33:10,985 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:33:10,999 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:33:11,001 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:33:11,001 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:33:11,001 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:33:24,751 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:33:24,753 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:33:38,287 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-10 02:33:38,299 - root - INFO - Successfully uploaded 3 files to Data
2025-07-10 02:33:39,899 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 02:33:39,899 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 02:33:39,899 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 02:33:52,178 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-10 02:33:55,282 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:33:56,349 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:34:04,575 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 02:34:08,208 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 02:34:10,284 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 02:34:10,289 - root - INFO - Successfully ingested documents. Chunks ingested: 156
2025-07-10 02:34:10,330 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 02:34:20,235 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:34:20,236 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:34:21,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:34:21,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 02:34:21,633 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:34:21,634 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:34:21,645 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:34:21,645 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:34:21,646 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:34:21,646 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:34:43,545 - root - INFO - Received question: Car Fuel Consumption Limit for Employees 
2025-07-10 02:34:43,545 - root - INFO - Chain init: 0.00s
2025-07-10 02:34:44,976 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:34:47,371 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:34:50,549 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:34:50,557 - root - INFO - Question processing: 7.01s
2025-07-10 02:34:50,558 - root - INFO - ⏱️ ask_hr took 7.01 seconds
2025-07-10 02:36:28,107 - root - INFO - Received question: What are the monthly car fuel consumption limits for employees of QADRI-Group by grade, and who bears the fuel cost for official and personal use within the city?
2025-07-10 02:36:28,107 - root - INFO - Chain init: 0.00s
2025-07-10 02:36:29,269 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:36:31,012 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:36:35,198 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:36:35,205 - root - INFO - Question processing: 7.10s
2025-07-10 02:36:35,206 - root - INFO - ⏱️ ask_hr took 7.10 seconds
2025-07-10 02:36:56,468 - root - INFO - Received question: car fuel limit by grade
2025-07-10 02:36:56,468 - root - INFO - Chain init: 0.00s
2025-07-10 02:36:57,599 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:36:59,253 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:37:01,469 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:37:01,474 - root - INFO - Question processing: 5.00s
2025-07-10 02:37:01,476 - root - INFO - ⏱️ ask_hr took 5.01 seconds
2025-07-10 02:37:16,231 - root - INFO - Received question: m7 leave policy
2025-07-10 02:37:16,231 - root - INFO - Chain init: 0.00s
2025-07-10 02:37:17,031 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:37:18,675 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:37:21,706 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:37:21,712 - root - INFO - Question processing: 5.48s
2025-07-10 02:37:21,712 - root - INFO - ⏱️ ask_hr took 5.48 seconds
2025-07-10 02:37:59,909 - root - INFO - Received question: Anual leave policy for m7 grade
2025-07-10 02:37:59,909 - root - INFO - Chain init: 0.00s
2025-07-10 02:38:00,950 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:38:03,307 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:38:05,940 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:38:05,977 - root - INFO - Question processing: 6.07s
2025-07-10 02:38:05,979 - root - INFO - ⏱️ ask_hr took 6.07 seconds
2025-07-10 02:48:39,022 - root - INFO - Starting HR Assistant...
2025-07-10 02:48:39,196 - root - INFO - Configuration validated successfully
2025-07-10 02:48:39,196 - root - INFO - Pre-warming the model...
2025-07-10 02:48:39,196 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 02:48:39,201 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 02:48:39,346 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 02:48:42,657 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:48:43,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:48:45,781 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 02:48:45,931 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:48:45,931 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:48:45,931 - root - INFO - Loading groq LLM...
2025-07-10 02:48:47,770 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:48:49,490 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:48:51,653 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:48:51,710 - root - INFO - Model pre-warmed successfully
2025-07-10 02:48:51,714 - root - INFO - HR Assistant started successfully
2025-07-10 02:50:26,258 - root - INFO - Received question: What are the working hours for non-management employees (N-1 to N-5) and their break timings?
2025-07-10 02:50:26,296 - root - INFO - Chain init: 0.00s
2025-07-10 02:50:27,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:50:29,608 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:50:32,405 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:50:32,448 - root - INFO - Question processing: 6.15s
2025-07-10 02:50:32,450 - root - INFO - ⏱️ ask_hr took 6.19 seconds
2025-07-10 02:51:21,349 - root - INFO - Received question: What is the process and condition for a Grade M-7 employee to receive ownership of a company car before it reaches maturity life?
2025-07-10 02:51:21,349 - root - INFO - Chain init: 0.00s
2025-07-10 02:51:22,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:51:24,165 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:51:26,247 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:51:26,252 - root - INFO - Question processing: 4.90s
2025-07-10 02:51:26,252 - root - INFO - ⏱️ ask_hr took 4.90 seconds
2025-07-10 02:52:06,069 - root - INFO - Received question: What should an employee do if they witness an unsafe working condition or practice?
2025-07-10 02:52:06,069 - root - INFO - Chain init: 0.00s
2025-07-10 02:52:08,319 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:52:10,063 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:52:12,430 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:52:12,439 - root - INFO - Question processing: 6.37s
2025-07-10 02:52:12,440 - root - INFO - ⏱️ ask_hr took 6.37 seconds
2025-07-10 02:52:53,969 - root - INFO - Received question: What are the disciplinary actions for an employee who marks proxy attendance for a colleague?
2025-07-10 02:52:53,970 - root - INFO - Chain init: 0.00s
2025-07-10 02:52:54,846 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:52:56,495 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:52:57,896 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:52:57,900 - root - INFO - Question processing: 3.93s
2025-07-10 02:52:57,903 - root - INFO - ⏱️ ask_hr took 3.93 seconds
2025-07-10 02:53:53,791 - root - INFO - Received question: If a company car is involved in an accident, what steps must the employee take, and what happens if they fail to report it?
2025-07-10 02:53:53,791 - root - INFO - Chain init: 0.00s
2025-07-10 02:53:55,155 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:53:57,218 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:53:59,640 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:53:59,951 - root - INFO - Question processing: 6.16s
2025-07-10 02:53:59,953 - root - INFO - ⏱️ ask_hr took 6.16 seconds
2025-07-10 02:57:20,538 - root - INFO - Starting HR Assistant...
2025-07-10 02:57:20,676 - root - INFO - Configuration validated successfully
2025-07-10 02:57:20,676 - root - INFO - Pre-warming the model...
2025-07-10 02:57:20,678 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 02:57:20,678 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 02:57:20,820 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 02:57:24,374 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 02:57:25,393 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:57:28,027 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 02:57:28,176 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 02:57:28,176 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 02:57:28,176 - root - INFO - Loading groq LLM...
2025-07-10 02:57:30,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:57:32,138 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:57:34,339 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:57:34,356 - root - INFO - Model pre-warmed successfully
2025-07-10 02:57:34,356 - root - INFO - HR Assistant started successfully
2025-07-10 02:57:47,755 - root - INFO - Received question:  What are the consequences if a Grade M-7 employee fails to report a car theft within the required time frame, and what financial impact can it have?
2025-07-10 02:57:47,756 - root - INFO - Chain init: 0.00s
2025-07-10 02:57:48,415 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:57:49,986 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:57:53,744 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:57:53,749 - root - INFO - Question processing: 5.99s
2025-07-10 02:57:53,750 - root - INFO - ⏱️ ask_hr took 6.00 seconds
2025-07-10 02:58:30,221 - root - INFO - Received question: If an employee repeatedly arrives late to work and is also found misusing time online, how will the company handle this behavior under policy?
2025-07-10 02:58:30,222 - root - INFO - Chain init: 0.00s
2025-07-10 02:58:31,551 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:58:33,125 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:58:35,343 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:58:35,347 - root - INFO - Question processing: 5.13s
2025-07-10 02:58:35,349 - root - INFO - ⏱️ ask_hr took 5.13 seconds
2025-07-10 02:59:01,840 - root - INFO - Received question:  Can an employee take leave on a Friday adjacent to the weekend more than five times a year without it affecting their leave balance? Explain any exceptions.
2025-07-10 02:59:01,840 - root - INFO - Chain init: 0.00s
2025-07-10 02:59:02,867 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:59:04,808 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:59:07,663 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:59:07,675 - root - INFO - Question processing: 5.83s
2025-07-10 02:59:07,675 - root - INFO - ⏱️ ask_hr took 5.83 seconds
2025-07-10 02:59:34,548 - root - INFO - Received question:  If an employee has exhausted their leave quota but wants to take time off during their notice period, what options are available under company policy?
2025-07-10 02:59:34,549 - root - INFO - Chain init: 0.00s
2025-07-10 02:59:38,718 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 02:59:40,774 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 02:59:42,697 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 02:59:42,701 - root - INFO - Question processing: 8.15s
2025-07-10 02:59:42,713 - root - INFO - ⏱️ ask_hr took 8.16 seconds
2025-07-10 03:00:13,840 - root - INFO - Received question: What costs is a Grade M-7 employee personally liable for if their car is damaged due to their own negligence, and how does this affect the maintenance cost limit?
2025-07-10 03:00:13,840 - root - INFO - Chain init: 0.00s
2025-07-10 03:00:15,625 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:00:17,421 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:00:20,162 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:00:20,164 - root - INFO - Question processing: 6.32s
2025-07-10 03:00:20,170 - root - INFO - ⏱️ ask_hr took 6.33 seconds
2025-07-10 03:03:57,865 - root - INFO - Received question: If a Grade M-7 employee leaves Qadri Group before their car reaches maturity life, what are the two possible outcomes regarding the car ownership, and how is the refund or payment calculated in each case?
2025-07-10 03:03:57,867 - root - INFO - Chain init: 0.00s
2025-07-10 03:04:00,318 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:04:04,780 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:04:08,798 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:04:08,845 - root - INFO - Question processing: 10.98s
2025-07-10 03:04:08,846 - root - INFO - ⏱️ ask_hr took 10.98 seconds
2025-07-10 03:05:00,688 - root - INFO - Received question: If a public holiday falls between a half-day leave and a full-day leave, will the holiday be deducted from the leave quota? Under what condition can it be considered a regular holiday?
2025-07-10 03:05:00,688 - root - INFO - Chain init: 0.00s
2025-07-10 03:05:01,951 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:05:04,192 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:05:06,475 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:05:06,483 - root - INFO - Question processing: 5.79s
2025-07-10 03:05:06,483 - root - INFO - ⏱️ ask_hr took 5.79 seconds
2025-07-10 03:10:44,520 - root - INFO - Starting HR Assistant...
2025-07-10 03:10:44,691 - root - INFO - Configuration validated successfully
2025-07-10 03:10:44,691 - root - INFO - Pre-warming the model...
2025-07-10 03:10:44,691 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 03:10:44,693 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 03:10:44,836 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:10:48,797 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:10:49,947 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:10:52,246 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 03:10:52,414 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:10:52,414 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:10:52,414 - root - INFO - Loading groq LLM...
2025-07-10 03:10:54,256 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:10:56,045 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:10:58,337 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:10:58,357 - root - INFO - Model pre-warmed successfully
2025-07-10 03:10:58,357 - root - INFO - HR Assistant started successfully
2025-07-10 03:13:36,323 - root - INFO - Received question: If a Grade M-8 employee's company car is stolen and not recovered within a reasonable time, and the employee failed to report the theft within 1 hour, what are the financial consequences, and how does this affect their future eligibility for car allotment?
2025-07-10 03:13:36,328 - root - INFO - Chain init: 0.00s
2025-07-10 03:13:38,133 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:13:40,855 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:13:43,466 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:13:43,477 - root - INFO - Question processing: 7.15s
2025-07-10 03:13:43,478 - root - INFO - ⏱️ ask_hr took 7.15 seconds
2025-07-10 03:14:18,565 - root - INFO - Received question: If an employee uses more than five Friday leaves in a year and then applies for a Friday leave due to a special medical condition, will Saturday be deducted? What is the policy for exceptions?
2025-07-10 03:14:18,565 - root - INFO - Chain init: 0.00s
2025-07-10 03:14:20,061 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:14:22,203 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:14:24,631 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:14:24,635 - root - INFO - Question processing: 6.07s
2025-07-10 03:14:24,637 - root - INFO - ⏱️ ask_hr took 6.07 seconds
2025-07-10 03:14:58,177 - root - INFO - Received question: In continuation of my previous question — will the Friday leave during Eid-ul-Fitr week also count toward the five-leave Friday cap?
2025-07-10 03:14:58,205 - root - INFO - Chain init: 0.00s
2025-07-10 03:14:59,094 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:15:00,976 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:15:03,533 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:15:03,539 - root - INFO - Question processing: 5.33s
2025-07-10 03:15:03,539 - root - INFO - ⏱️ ask_hr took 5.36 seconds
2025-07-10 03:17:56,047 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:17:59,900 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:18:00,814 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:18:04,263 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:18:05,326 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:18:49,916 - root - INFO - Starting HR Assistant...
2025-07-10 03:18:50,052 - root - INFO - Configuration validated successfully
2025-07-10 03:18:50,052 - root - INFO - Pre-warming the model...
2025-07-10 03:18:50,052 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 03:18:50,052 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 03:18:50,190 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:18:52,442 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:18:55,283 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-10 03:18:55,284 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":7.327e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 269, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":7.327e-6}'
2025-07-10 03:18:55,303 - root - ERROR - Startup failed: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":7.327e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 186, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 269, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":7.327e-6}'
2025-07-10 03:19:15,764 - root - INFO - Starting HR Assistant...
2025-07-10 03:19:15,922 - root - INFO - Configuration validated successfully
2025-07-10 03:19:15,922 - root - INFO - Pre-warming the model...
2025-07-10 03:19:15,923 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 03:19:15,923 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 03:19:16,081 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:19:19,092 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:19:20,351 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-10 03:19:20,351 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.859e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 269, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.859e-6}'
2025-07-10 03:19:20,360 - root - ERROR - Startup failed: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.859e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 186, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 269, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1076, in _validate_collection_for_dense
    collection_info = client.get_collection(collection_name=collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.859e-6}'
2025-07-10 03:25:31,655 - root - INFO - Starting HR Assistant...
2025-07-10 03:25:31,801 - root - INFO - Configuration validated successfully
2025-07-10 03:25:31,801 - root - INFO - Pre-warming the model...
2025-07-10 03:25:31,801 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 03:25:31,802 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 03:25:31,944 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:25:37,227 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:25:38,610 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-10 03:25:38,613 - ingestion_retrieval.retrieval - INFO - Creating collection: HRDOCS
2025-07-10 03:25:39,279 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:25:39,279 - ingestion_retrieval.retrieval - INFO - Collection 'HRDOCS' created successfully
2025-07-10 03:25:39,283 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: QdrantVectorStore.__init__() got an unexpected keyword argument 'create_collection_if_not_exists'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 300, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
TypeError: QdrantVectorStore.__init__() got an unexpected keyword argument 'create_collection_if_not_exists'
2025-07-10 03:25:39,284 - root - ERROR - Startup failed: QdrantVectorStore.__init__() got an unexpected keyword argument 'create_collection_if_not_exists'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 209, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 300, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
TypeError: QdrantVectorStore.__init__() got an unexpected keyword argument 'create_collection_if_not_exists'
2025-07-10 03:26:30,510 - root - INFO - Starting HR Assistant...
2025-07-10 03:26:30,648 - root - INFO - Configuration validated successfully
2025-07-10 03:26:30,648 - root - INFO - Pre-warming the model...
2025-07-10 03:26:30,648 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 03:26:30,648 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 03:26:30,791 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:26:33,682 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:26:35,028 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:26:39,526 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 03:26:39,673 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:26:39,673 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:26:39,673 - root - INFO - Loading groq LLM...
2025-07-10 03:26:42,527 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:26:44,586 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:26:46,732 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:26:46,757 - root - INFO - Model pre-warmed successfully
2025-07-10 03:26:46,757 - root - INFO - HR Assistant started successfully
2025-07-10 03:27:36,498 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:27:36,524 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:27:36,528 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:27:36,532 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:27:40,054 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:27:40,055 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:27:41,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:27:42,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:27:42,219 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:27:42,446 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:28:00,595 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:28:02,759 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:28:02,991 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:28:06,558 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:28:08,545 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:28:12,909 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:28:15,559 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:28:16,679 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:28:22,125 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:28:23,109 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:28:23,361 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:28:31,240 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 03:28:31,242 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:28:31,242 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:28:31,379 - root - INFO - Successfully ingested documents. Chunks ingested: 0
2025-07-10 03:28:31,379 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:28:33,701 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-10 03:28:33,711 - root - INFO - Successfully uploaded 3 files to Data
2025-07-10 03:28:35,220 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 03:28:35,220 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:28:35,221 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:28:51,437 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:28:53,896 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:28:54,888 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:29:11,489 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 03:29:15,392 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 03:29:15,397 - root - INFO - Successfully ingested documents. Chunks ingested: 115
2025-07-10 03:29:15,400 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:29:30,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:29:30,721 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:29:31,809 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:29:32,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:29:32,105 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:29:32,110 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:29:32,114 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:29:32,114 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:29:32,114 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:29:32,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:30:43,448 - root - INFO - Received question: If a Grade M-8 employee's company car is stolen and not recovered within a reasonable time, and the employee failed to report the theft within 1 hour, what are the financial consequences, and how does this affect their future eligibility for car allotment?
2025-07-10 03:30:43,448 - root - INFO - Chain init: 0.00s
2025-07-10 03:30:45,708 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:30:48,988 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:30:56,589 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:30:56,594 - root - INFO - Question processing: 13.15s
2025-07-10 03:30:56,594 - root - INFO - ⏱️ ask_hr took 13.15 seconds
2025-07-10 03:31:29,375 - root - INFO - Received question: If an employee uses more than five Friday leaves in a year and then applies for a Friday leave due to a special medical condition, will Saturday be deducted? What is the policy for exceptions?
2025-07-10 03:31:29,376 - root - INFO - Chain init: 0.00s
2025-07-10 03:31:31,062 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:31:34,925 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:31:37,149 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:31:37,191 - root - INFO - Question processing: 7.82s
2025-07-10 03:31:37,193 - root - INFO - ⏱️ ask_hr took 7.82 seconds
2025-07-10 03:32:29,273 - root - INFO - Received question: In continuation of my previous question — will the Friday leave during Eid-ul-Fitr week also count toward the five-leave Friday cap?
2025-07-10 03:32:29,275 - root - INFO - Chain init: 0.00s
2025-07-10 03:32:31,894 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:32:34,254 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:32:36,790 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:32:36,831 - root - INFO - Question processing: 7.56s
2025-07-10 03:32:36,831 - root - INFO - ⏱️ ask_hr took 7.56 seconds
2025-07-10 03:35:49,256 - root - INFO - Received question: Fuel Limits for all grades
2025-07-10 03:35:49,256 - root - INFO - Chain init: 0.00s
2025-07-10 03:35:51,257 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:35:52,948 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:35:55,857 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:35:55,902 - root - INFO - Question processing: 6.65s
2025-07-10 03:35:55,903 - root - INFO - ⏱️ ask_hr took 6.65 seconds
2025-07-10 03:38:56,477 - root - INFO - Starting HR Assistant...
2025-07-10 03:38:56,671 - root - INFO - Configuration validated successfully
2025-07-10 03:38:56,671 - root - INFO - Pre-warming the model...
2025-07-10 03:38:56,671 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 03:38:56,671 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 03:38:56,831 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:39:01,109 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:39:02,436 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:39:04,830 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 03:39:04,992 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:39:04,993 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:39:04,994 - root - INFO - Loading groq LLM...
2025-07-10 03:39:07,489 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:39:09,544 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:39:12,545 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:39:12,567 - root - INFO - Model pre-warmed successfully
2025-07-10 03:39:12,567 - root - INFO - HR Assistant started successfully
2025-07-10 03:39:47,144 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:39:48,801 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:39:49,158 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:39:52,768 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:39:54,000 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:40:15,460 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:40:16,398 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:40:17,134 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:40:20,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:40:21,798 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:40:22,481 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:40:29,091 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-10 03:40:29,093 - root - INFO - Successfully uploaded 3 files to Data
2025-07-10 03:40:30,637 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 03:40:30,638 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:40:30,639 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:40:40,896 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-10 03:40:43,327 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:40:44,578 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:40:54,418 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 03:40:59,038 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 03:40:59,042 - root - INFO - Successfully ingested documents. Chunks ingested: 115
2025-07-10 03:40:59,043 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 03:41:12,641 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:41:12,642 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 03:41:14,055 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:41:14,055 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 03:41:14,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:41:14,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:41:14,768 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:41:14,769 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:41:14,770 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 03:41:14,770 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 03:42:27,740 - root - INFO - Received question: If a Grade M-8 employee's company car is stolen and not recovered within a reasonable time, and the employee failed to report the theft within 1 hour, what are the financial consequences, and how does this affect their future eligibility for car allotment?
2025-07-10 03:42:27,741 - root - INFO - Chain init: 0.00s
2025-07-10 03:42:28,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:42:30,616 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:42:34,845 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:42:34,854 - root - INFO - Question processing: 7.11s
2025-07-10 03:42:34,854 - root - INFO - ⏱️ ask_hr took 7.11 seconds
2025-07-10 03:45:53,115 - root - INFO - Received question: If an employee uses more than five Friday leaves in a year and then applies for a Friday leave due to a special medical condition, will Saturday be deducted? What is the policy for exceptions?
2025-07-10 03:45:53,115 - root - INFO - Chain init: 0.00s
2025-07-10 03:45:54,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:45:57,763 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:46:02,090 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:46:02,092 - root - INFO - Question processing: 8.98s
2025-07-10 03:46:02,092 - root - INFO - ⏱️ ask_hr took 8.98 seconds
2025-07-10 03:46:55,599 - root - INFO - Received question: In continuation of my previous question — will the Friday leave during Eid-ul-Fitr week also count toward the five-leave Friday cap?
2025-07-10 03:46:55,600 - root - INFO - Chain init: 0.00s
2025-07-10 03:46:56,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 03:46:58,499 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 03:47:00,116 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 03:47:00,121 - root - INFO - Question processing: 4.52s
2025-07-10 03:47:00,121 - root - INFO - ⏱️ ask_hr took 4.52 seconds
2025-07-10 05:14:08,227 - root - INFO - Received question: What are the working hours for management employees (M-1 to M-10), and what is the break time on Friday?
2025-07-10 05:14:08,235 - root - INFO - Chain init: 0.00s
2025-07-10 05:14:09,306 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:14:11,192 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:14:13,728 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:14:13,753 - root - INFO - Question processing: 5.52s
2025-07-10 05:14:13,757 - root - INFO - ⏱️ ask_hr took 5.53 seconds
2025-07-10 05:14:33,805 - root - INFO - Received question: How many annual leaves are granted to confirmed management staff per year?
2025-07-10 05:14:33,807 - root - INFO - Chain init: 0.00s
2025-07-10 05:14:35,781 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:14:37,109 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:14:38,068 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:14:38,079 - root - INFO - Question processing: 4.27s
2025-07-10 05:14:38,079 - root - INFO - ⏱️ ask_hr took 4.27 seconds
2025-07-10 05:15:12,019 - root - INFO - Received question: What is the fuel limit for employees in Grade M-9, and who bears the cost if the limit is exceeded?
2025-07-10 05:15:12,020 - root - INFO - Chain init: 0.00s
2025-07-10 05:15:13,571 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:15:15,259 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:15:17,017 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:15:17,059 - root - INFO - Question processing: 5.04s
2025-07-10 05:15:17,059 - root - INFO - ⏱️ ask_hr took 5.04 seconds
2025-07-10 05:16:12,968 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:16:13,924 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:16:14,259 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:16:14,319 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:16:14,320 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:16:14,323 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:16:14,323 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:16:20,342 - root - INFO - Received question: Can an employee mark manual attendance if their card is forgotten? What is the process?
2025-07-10 05:16:20,342 - root - INFO - Chain init: 0.00s
2025-07-10 05:16:21,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:16:24,379 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:16:26,032 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:16:26,041 - root - INFO - Question processing: 5.70s
2025-07-10 05:16:26,041 - root - INFO - ⏱️ ask_hr took 5.70 seconds
2025-07-10 05:16:44,695 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:16:46,000 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:16:46,491 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:16:46,539 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:16:46,540 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:16:46,540 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:16:46,541 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:16:47,573 - root - INFO - Received question: What is the policy regarding accepting gifts from vendors or clients?
2025-07-10 05:16:47,575 - root - INFO - Chain init: 0.00s
2025-07-10 05:16:48,973 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:16:50,948 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:16:53,166 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:16:53,170 - root - INFO - Question processing: 5.60s
2025-07-10 05:16:53,172 - root - INFO - ⏱️ ask_hr took 5.60 seconds
2025-07-10 05:17:18,271 - root - INFO - Received question: If a Grade M-7 employee wishes to transfer ownership of a company car before maturity, what are the conditions and payment formula?
2025-07-10 05:17:18,271 - root - INFO - Chain init: 0.00s
2025-07-10 05:17:19,103 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:17:20,664 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:17:22,041 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:17:22,049 - root - INFO - Question processing: 3.78s
2025-07-10 05:17:22,050 - root - INFO - ⏱️ ask_hr took 3.78 seconds
2025-07-10 05:17:39,887 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:17:40,462 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:17:40,644 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:17:40,649 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:17:40,649 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:17:40,652 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:17:40,652 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:17:41,541 - root - INFO - Received question: What is the consequence if an employee fails to report a company car accident or theft within 1 hour?
2025-07-10 05:17:41,543 - root - INFO - Chain init: 0.00s
2025-07-10 05:17:42,460 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:17:44,272 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:17:45,903 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:17:45,911 - root - INFO - Question processing: 4.37s
2025-07-10 05:17:45,911 - root - INFO - ⏱️ ask_hr took 4.37 seconds
2025-07-10 05:18:12,358 - root - INFO - Received question: Explain the sandwich leave policy and when a public holiday will be considered a leave day. What is the exception for half-day leaves?
2025-07-10 05:18:12,358 - root - INFO - Chain init: 0.00s
2025-07-10 05:18:13,653 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:18:15,880 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:18:17,493 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:18:17,507 - root - INFO - Question processing: 5.15s
2025-07-10 05:18:17,507 - root - INFO - ⏱️ ask_hr took 5.15 seconds
2025-07-10 05:18:41,346 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:18:42,415 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:18:42,664 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:18:42,671 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:18:42,673 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:18:42,673 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:18:42,674 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:18:51,892 - root - INFO - Received question: 	If an employee misuses time repeatedly (e.g., personal errands, extended breaks) and has multiple late arrivals, what are the disciplinary consequences according to policy?
2025-07-10 05:18:51,930 - root - INFO - Chain init: 0.00s
2025-07-10 05:18:52,988 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:18:55,130 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:18:56,493 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:18:56,499 - root - INFO - Question processing: 4.57s
2025-07-10 05:18:56,500 - root - INFO - ⏱️ ask_hr took 4.61 seconds
2025-07-10 05:19:39,670 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:19:40,988 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:19:41,568 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:19:41,581 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:19:41,581 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:19:41,581 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:19:41,585 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:20:09,077 - root - INFO - Received question: What are the two possible outcomes (including formulas) if a Grade M-7 employee leaves the company before the maturity life of their car? Reference clause(s) and financial implications.
2025-07-10 05:20:09,078 - root - INFO - Chain init: 0.00s
2025-07-10 05:20:10,579 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:20:12,428 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:20:15,270 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:20:15,276 - root - INFO - Question processing: 6.20s
2025-07-10 05:20:15,276 - root - INFO - ⏱️ ask_hr took 6.20 seconds
2025-07-10 05:20:39,803 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:20:40,814 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:20:41,471 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:20:41,477 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:20:41,477 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:20:41,477 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:20:41,478 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:21:39,984 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:21:41,219 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:21:41,657 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:21:41,664 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:21:41,665 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:21:41,665 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:21:41,666 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:22:39,745 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:22:41,229 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:22:41,642 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:22:41,684 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:22:41,687 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:22:41,687 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:22:41,687 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:23:39,827 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:23:40,371 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:23:40,542 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:23:40,553 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:23:40,553 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:23:40,553 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:23:40,553 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:24:40,241 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:24:41,029 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:24:41,203 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:24:41,208 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:24:41,209 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:24:41,209 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:24:41,209 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:25:40,129 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:25:41,381 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:25:41,791 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:25:41,793 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:25:41,793 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:25:41,793 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:25:41,793 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:26:40,526 - root - INFO - Starting HR Assistant...
2025-07-10 05:26:40,681 - root - INFO - Configuration validated successfully
2025-07-10 05:26:40,681 - root - INFO - Pre-warming the model...
2025-07-10 05:26:40,681 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 05:26:40,684 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 05:26:40,837 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 05:26:43,782 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:26:45,180 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:26:47,151 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 05:26:47,304 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:26:47,304 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:26:47,305 - root - INFO - Loading groq LLM...
2025-07-10 05:26:49,302 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:26:51,130 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:26:53,383 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:26:53,404 - root - INFO - Model pre-warmed successfully
2025-07-10 05:26:53,404 - root - INFO - HR Assistant started successfully
2025-07-10 05:26:56,077 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:26:57,031 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:26:57,286 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:26:57,292 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:26:57,292 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:26:57,292 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:26:57,293 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:27:40,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:27:41,641 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:27:41,982 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:27:41,988 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:27:41,990 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:27:41,990 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:27:41,990 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:28:41,091 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:28:42,747 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:28:43,084 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:28:43,090 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:28:43,091 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:28:43,091 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:28:43,091 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:29:13,872 - root - INFO - Received question: If a company car is stolen and the employee fails to report the incident within the required time, what are the financial consequences according to the policy?
2025-07-10 05:29:13,873 - root - INFO - Chain init: 0.00s
2025-07-10 05:29:15,654 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:29:17,884 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:29:19,417 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:29:19,460 - root - INFO - Question processing: 5.59s
2025-07-10 05:29:19,460 - root - INFO - ⏱️ ask_hr took 5.59 seconds
2025-07-10 05:29:40,817 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:29:43,501 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:29:44,190 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:29:44,199 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:29:44,201 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:29:44,202 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:29:44,203 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:29:46,078 - root - INFO - Received question:  Can a Grade M-7 employee lose the right to a company car permanently? Under what circumstances can the Board take this action?
2025-07-10 05:29:46,079 - root - INFO - Chain init: 0.00s
2025-07-10 05:29:48,082 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:29:50,399 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:29:51,622 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:29:51,629 - root - INFO - Question processing: 5.55s
2025-07-10 05:29:51,629 - root - INFO - ⏱️ ask_hr took 5.55 seconds
2025-07-10 05:30:40,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:30:40,988 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:30:41,191 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:30:41,199 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:30:41,201 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:30:41,202 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:30:41,202 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:31:40,713 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:31:41,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:31:42,206 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:31:42,210 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:31:42,213 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:31:42,213 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:31:42,214 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:32:42,664 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:32:43,879 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:32:44,358 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:32:44,363 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:32:44,363 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:32:44,364 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:32:44,364 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:33:40,506 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:33:43,541 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:33:44,035 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:33:44,041 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:33:44,041 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:33:44,041 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:33:44,041 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:34:40,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:34:43,371 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:34:43,950 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:34:43,956 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:34:43,956 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:34:43,956 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:34:43,956 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:35:40,282 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:35:41,761 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:35:42,088 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:35:42,090 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:35:42,090 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:35:42,090 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:35:42,090 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:36:41,180 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:36:42,741 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:36:43,083 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:36:43,088 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:36:43,088 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:36:43,088 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:36:43,090 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:37:07,802 - root - INFO - Received question: What will you pay me If I Join with a masters degree in finance
2025-07-10 05:37:07,802 - root - INFO - Chain init: 0.00s
2025-07-10 05:37:08,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:37:11,052 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:37:12,863 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:37:12,870 - root - INFO - Question processing: 5.07s
2025-07-10 05:37:12,871 - root - INFO - ⏱️ ask_hr took 5.07 seconds
2025-07-10 05:37:40,932 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:37:42,701 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:37:43,106 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:37:43,113 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:37:43,113 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:37:43,114 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:37:43,114 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:37:57,049 - root - INFO - Received question: what if i stay absent for 6 days in a row
2025-07-10 05:37:57,051 - root - INFO - Chain init: 0.00s
2025-07-10 05:37:58,216 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:38:00,493 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:38:03,392 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:38:03,400 - root - INFO - Question processing: 6.35s
2025-07-10 05:38:03,401 - root - INFO - ⏱️ ask_hr took 6.35 seconds
2025-07-10 05:38:40,539 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:38:42,492 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:38:42,824 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:38:42,829 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:38:42,831 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:38:42,831 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:38:42,831 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:39:40,676 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 05:39:42,069 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 05:39:42,397 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:39:42,413 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:39:42,414 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:39:42,415 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 05:39:42,415 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 05:40:02,405 - root - INFO - Received question: if i do 4 excess leave and my salary is 200k how much deducton
2025-07-10 05:40:02,405 - root - INFO - Chain init: 0.00s
2025-07-10 05:40:03,654 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 05:40:05,799 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 05:40:12,206 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 05:40:12,210 - root - INFO - Question processing: 9.80s
2025-07-10 05:40:12,212 - root - INFO - ⏱️ ask_hr took 9.81 seconds
2025-07-10 20:45:45,136 - root - INFO - Starting HR Assistant...
2025-07-10 20:45:45,352 - root - INFO - Configuration validated successfully
2025-07-10 20:45:45,355 - root - INFO - Pre-warming the model...
2025-07-10 20:45:45,355 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 20:45:45,355 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 20:45:45,497 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 20:45:48,657 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 20:45:49,944 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:45:52,621 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 20:45:52,789 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 20:45:52,789 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 20:45:52,789 - root - INFO - Loading groq LLM...
2025-07-10 20:45:55,136 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:45:59,807 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:46:02,566 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 20:46:02,589 - root - INFO - Model pre-warmed successfully
2025-07-10 20:46:02,592 - root - INFO - HR Assistant started successfully
2025-07-10 20:47:01,841 - root - INFO - Received question: m7 leave policy
2025-07-10 20:47:01,878 - root - INFO - Chain init: 0.00s
2025-07-10 20:47:02,902 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:47:04,748 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:47:07,015 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 20:47:07,024 - root - INFO - Question processing: 5.14s
2025-07-10 20:47:07,024 - root - INFO - ⏱️ ask_hr took 5.18 seconds
2025-07-10 20:50:57,515 - root - INFO - Starting HR Assistant...
2025-07-10 20:50:57,668 - root - INFO - Configuration validated successfully
2025-07-10 20:50:57,668 - root - INFO - Pre-warming the model...
2025-07-10 20:50:57,670 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 20:50:57,670 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 20:50:57,820 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 20:51:00,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 20:51:00,869 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:51:02,807 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 288, in get_hr_assistant_chain
    Mqr = MultiQueryRetriever(retriever=mmr_retriever)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
2025-07-10 20:51:02,813 - root - ERROR - Startup failed: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 186, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 288, in get_hr_assistant_chain
    Mqr = MultiQueryRetriever(retriever=mmr_retriever)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
2025-07-10 20:51:14,718 - root - INFO - Starting HR Assistant...
2025-07-10 20:51:14,911 - root - INFO - Configuration validated successfully
2025-07-10 20:51:14,913 - root - INFO - Pre-warming the model...
2025-07-10 20:51:14,914 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 20:51:14,914 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 20:51:15,079 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 20:51:17,468 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 20:51:18,780 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:51:20,689 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 288, in get_hr_assistant_chain
    Mqr = MultiQueryRetriever(retriever=mmr_retriever)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
2025-07-10 20:51:20,693 - root - ERROR - Startup failed: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 186, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 288, in get_hr_assistant_chain
    Mqr = MultiQueryRetriever(retriever=mmr_retriever)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for MultiQueryRetriever
llm_chain
  Field required [type=missing, input_value={'retriever': VectorStore...score_threshold': 0.4})}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
2025-07-10 20:56:31,638 - root - INFO - Starting HR Assistant...
2025-07-10 20:56:31,788 - root - INFO - Configuration validated successfully
2025-07-10 20:56:31,789 - root - INFO - Pre-warming the model...
2025-07-10 20:56:31,789 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 20:56:31,789 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 20:56:31,943 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 20:56:34,563 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 20:56:35,396 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:56:37,043 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 20:56:37,202 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 20:56:37,202 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 20:56:37,202 - root - INFO - Loading groq LLM...
2025-07-10 20:56:40,507 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 20:56:40,528 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test":', 'What is the purpose of a test?', 'How do I prepare for a test?', 'What are the benefits of testing something?', 'These rephrased questions can help retrieve relevant documents from a vector database by capturing different aspects and nuances of the original question, increasing the chances of finding relevant information.']
2025-07-10 20:56:41,660 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:56:43,502 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:56:44,379 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:56:45,706 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:56:46,168 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:56:47,672 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:56:48,251 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:56:49,734 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:56:50,229 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:56:51,627 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:56:53,600 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 20:56:53,605 - root - INFO - Model pre-warmed successfully
2025-07-10 20:56:53,606 - root - INFO - HR Assistant started successfully
2025-07-10 20:58:14,144 - root - INFO - Received question: if i do  4 leave more than my quota and my salary is 200k then how much deduction from my salary
2025-07-10 20:58:14,181 - root - INFO - Chain init: 0.00s
2025-07-10 20:58:15,963 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 20:58:15,966 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'If I exceed my annual leave quota by 4 days and my salary is $200,000, what is the impact on my take-home pay?', 'How much will my salary of $200,000 be reduced if I take 4 more leave days than my allocated quota?', 'What is the salary deduction for taking 4 extra leave days beyond my quota, assuming an annual salary of $200,000?']
2025-07-10 20:58:16,949 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:58:19,172 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:58:19,921 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:58:21,238 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:58:21,883 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:58:23,568 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:58:24,290 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 20:58:25,781 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 20:58:27,915 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 20:58:27,963 - root - INFO - Question processing: 13.78s
2025-07-10 20:58:27,963 - root - INFO - ⏱️ ask_hr took 13.82 seconds
2025-07-10 21:00:06,465 - root - INFO - Received question:  If a company car is stolen and the employee fails to report the incident within the required time, what are the financial consequences according to the policy?
2025-07-10 21:00:06,465 - root - INFO - Chain init: 0.00s
2025-07-10 21:00:08,749 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:00:08,754 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the penalties or fines imposed on an employee if they fail to notify the company within the specified timeframe in the event of a company car being stolen?', "What happens if an employee doesn't report a stolen company vehicle within the required timeframe, and how does it impact their financial responsibilities under the company policy?", "In the event of company car theft, what are the financial implications for an employee who fails to comply with the reporting deadline outlined in the company's policy?", 'These alternative questions can help overcome the limitations of distance-based similarity search by providing different perspectives on the original question, increasing the chances of retrieving relevant documents from the vector database.']
2025-07-10 21:00:09,773 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:00:11,456 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:00:12,188 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:00:13,924 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:00:14,583 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:00:15,815 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:00:16,471 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:00:17,709 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:00:18,119 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:00:19,599 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:00:22,740 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:00:22,748 - root - INFO - Question processing: 16.28s
2025-07-10 21:00:22,748 - root - INFO - ⏱️ ask_hr took 16.28 seconds
2025-07-10 21:03:39,744 - root - INFO - Starting HR Assistant...
2025-07-10 21:03:40,010 - root - INFO - Configuration validated successfully
2025-07-10 21:03:40,010 - root - INFO - Pre-warming the model...
2025-07-10 21:03:40,010 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:03:40,010 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:03:40,222 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:03:43,802 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:03:45,189 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:03:47,848 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:03:48,131 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 21:03:48,132 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 21:03:48,132 - root - INFO - Loading groq LLM...
2025-07-10 21:03:53,100 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:03:53,133 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test" to retrieve relevant documents from a vector database:', 'What is the purpose of a test?', 'Can I use a test to evaluate something?', 'How do I prepare for a test?', 'These alternative questions provide different perspectives on the original question, which can help overcome the limitations of distance-based similarity search by capturing various aspects of the concept of "test".']
2025-07-10 21:03:55,664 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:03:58,075 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:03:59,515 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:04:01,166 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:04:01,572 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:04:03,305 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:04:03,639 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:04:05,363 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:04:05,771 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:04:07,668 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:04:10,897 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:04:10,902 - root - INFO - Model pre-warmed successfully
2025-07-10 21:04:10,903 - root - INFO - HR Assistant started successfully
2025-07-10 21:05:34,799 - root - INFO - Received question: Under what conditions can a Grade M-7 employee request early ownership of a company car, and what is the required payment formula?
2025-07-10 21:05:34,799 - root - INFO - Chain init: 0.00s
2025-07-10 21:05:37,586 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:05:37,591 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the eligibility criteria for a Grade M-7 employee to acquire a company car ahead of schedule, and how is the purchase price calculated?', 'Can a Grade M-7 staff member accelerate the vesting period of a company-provided vehicle, and what is the formula used to determine the required payment amount?', 'What are the rules and regulations governing early car ownership for Grade M-7 employees, including the necessary prerequisites and the method for calculating the buyout price?']
2025-07-10 21:05:39,638 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:05:41,615 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:05:43,260 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:05:44,741 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:05:45,155 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:05:46,386 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:05:47,372 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:05:48,941 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:05:52,405 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:05:52,449 - root - INFO - Question processing: 17.65s
2025-07-10 21:05:52,449 - root - INFO - ⏱️ ask_hr took 17.65 seconds
2025-07-10 21:07:31,283 - root - INFO - Starting HR Assistant...
2025-07-10 21:07:31,440 - root - INFO - Configuration validated successfully
2025-07-10 21:07:31,440 - root - INFO - Pre-warming the model...
2025-07-10 21:07:31,443 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:07:31,443 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:07:31,610 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:07:34,565 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:07:35,745 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:07:38,268 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:07:38,427 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 21:07:38,427 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 21:07:38,429 - root - INFO - Loading groq LLM...
2025-07-10 21:07:42,509 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:07:42,530 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test":', 'What is the purpose of a test?', 'How do I prepare for a test?', 'What are the benefits of testing something?', 'These alternative questions can help retrieve relevant documents from a vector database by providing different perspectives on the original question, which can overcome some of the limitations of distance-based similarity search.']
2025-07-10 21:07:43,948 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:07:46,355 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:07:47,581 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:07:49,310 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:07:50,048 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:07:51,543 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:07:52,206 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:07:53,947 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:07:54,452 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:07:56,543 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:08:04,138 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:08:04,143 - root - INFO - Model pre-warmed successfully
2025-07-10 21:08:04,143 - root - INFO - HR Assistant started successfully
2025-07-10 21:15:07,723 - root - INFO - Starting HR Assistant...
2025-07-10 21:15:07,904 - root - INFO - Configuration validated successfully
2025-07-10 21:15:07,904 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:15:07,904 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:15:07,904 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:15:08,070 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:15:11,933 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:15:13,981 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:15:18,048 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:15:18,231 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 21:15:18,231 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 21:15:18,231 - root - INFO - Loading groq LLM...
2025-07-10 21:15:23,474 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:15:23,494 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:15:25,451 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 21:15:25,454 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test":', 'What is the purpose of a test?', 'How do I prepare for a test?', 'What are the benefits of testing in a particular context?', 'These rephrased questions can help retrieve relevant documents from a vector database by capturing different aspects and nuances of the original question, thereby increasing the chances of finding relevant results.']
2025-07-10 21:15:26,935 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:15:28,589 - root - ERROR - Startup failed: Unknown arguments: ['params']
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 334, in chain_with_memory
    output = answer_cache(question)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 326, in answer_cache
    output = retrieval_chain.invoke(chain_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 172, in _get_relevant_documents
    documents = self.retrieve_documents(queries, run_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 210, in retrieve_documents
    docs = self.retriever.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 651, in max_marginal_relevance_search
    return self.max_marginal_relevance_search_by_vector(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 683, in max_marginal_relevance_search_by_vector
    results = self.max_marginal_relevance_search_with_score_by_vector(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 716, in max_marginal_relevance_search_with_score_by_vector
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 552, in query_points
    assert len(kwargs) == 0, f"Unknown arguments: {list(kwargs.keys())}"
AssertionError: Unknown arguments: ['params']
2025-07-10 21:16:03,032 - root - INFO - Starting HR Assistant...
2025-07-10 21:16:03,214 - root - INFO - Configuration validated successfully
2025-07-10 21:16:03,214 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:16:03,216 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:16:03,216 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:16:03,383 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:16:06,212 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:16:08,011 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:16:12,622 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:16:12,786 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:16:12,786 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:16:12,788 - root - INFO - Loading google LLM...
2025-07-10 21:16:16,380 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:16:17,729 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:16:21,921 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:16:23,227 - root - ERROR - Startup failed: Unknown arguments: ['params']
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 334, in chain_with_memory
    output = answer_cache(question)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 326, in answer_cache
    output = retrieval_chain.invoke(chain_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 172, in _get_relevant_documents
    documents = self.retrieve_documents(queries, run_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 210, in retrieve_documents
    docs = self.retriever.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 651, in max_marginal_relevance_search
    return self.max_marginal_relevance_search_by_vector(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 683, in max_marginal_relevance_search_by_vector
    results = self.max_marginal_relevance_search_with_score_by_vector(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 716, in max_marginal_relevance_search_with_score_by_vector
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 552, in query_points
    assert len(kwargs) == 0, f"Unknown arguments: {list(kwargs.keys())}"
AssertionError: Unknown arguments: ['params']
2025-07-10 21:17:34,981 - root - INFO - Starting HR Assistant...
2025-07-10 21:17:35,121 - root - INFO - Configuration validated successfully
2025-07-10 21:17:35,121 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:17:35,121 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:17:35,121 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:17:35,286 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:17:38,710 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:17:39,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:17:42,985 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:17:43,155 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:17:43,155 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:17:43,155 - root - INFO - Loading google LLM...
2025-07-10 21:17:47,435 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:17:48,734 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:17:50,139 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:17:52,521 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:17:55,170 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:17:57,380 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:18:02,159 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:18:04,378 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:18:08,575 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:18:11,042 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:18:14,081 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:18:15,556 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:18:19,678 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:18:19,678 - root - INFO - HR Assistant started successfully
2025-07-10 21:20:35,871 - root - INFO - Starting HR Assistant...
2025-07-10 21:20:36,061 - root - INFO - Configuration validated successfully
2025-07-10 21:20:36,061 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:20:36,061 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:20:36,062 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:20:36,231 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:20:39,449 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:20:41,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:20:44,233 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:20:44,400 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:20:44,400 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:20:44,408 - root - INFO - Loading google LLM...
2025-07-10 21:20:48,231 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:20:49,660 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:20:50,299 - root - ERROR - Startup failed: Unknown arguments: ['fetch_k']
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 332, in chain_with_memory
    question = inputs["input"]
             ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 324, in answer_cache
    chain_inputs = {"input": question, "chat_history": chat_history}
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 172, in _get_relevant_documents
    documents = self.retrieve_documents(queries, run_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 210, in retrieve_documents
    docs = self.retriever.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1079, in _get_relevant_documents
    docs = self.vectorstore.similarity_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 468, in similarity_search
    results = self.similarity_search_with_score(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 512, in similarity_search_with_score
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 552, in query_points
    assert len(kwargs) == 0, f"Unknown arguments: {list(kwargs.keys())}"
AssertionError: Unknown arguments: ['fetch_k']
2025-07-10 21:21:01,900 - root - INFO - Starting HR Assistant...
2025-07-10 21:21:02,050 - root - INFO - Configuration validated successfully
2025-07-10 21:21:02,050 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:21:02,066 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:21:02,066 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:21:02,232 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:21:04,794 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:21:05,934 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:21:08,010 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:21:08,218 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:21:08,220 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:21:08,220 - root - INFO - Loading google LLM...
2025-07-10 21:21:11,646 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:21:12,891 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:21:13,625 - root - ERROR - Startup failed: Unknown arguments: ['fetch_k', 'params']
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 333, in chain_with_memory
    output = answer_cache(question)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 325, in answer_cache
    output = retrieval_chain.invoke(chain_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 172, in _get_relevant_documents
    documents = self.retrieve_documents(queries, run_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 210, in retrieve_documents
    docs = self.retriever.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1079, in _get_relevant_documents
    docs = self.vectorstore.similarity_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 468, in similarity_search
    results = self.similarity_search_with_score(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 512, in similarity_search_with_score
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 552, in query_points
    assert len(kwargs) == 0, f"Unknown arguments: {list(kwargs.keys())}"
AssertionError: Unknown arguments: ['fetch_k', 'params']
2025-07-10 21:22:14,434 - root - INFO - Starting HR Assistant...
2025-07-10 21:22:14,601 - root - INFO - Configuration validated successfully
2025-07-10 21:22:14,601 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:22:14,601 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:22:14,603 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:22:14,773 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:22:17,808 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:22:18,805 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:22:22,166 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:22:22,334 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:22:22,334 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:22:22,334 - root - INFO - Loading google LLM...
2025-07-10 21:22:26,625 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:22:27,770 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:22:30,816 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:22:31,885 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:22:33,042 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:22:34,605 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:22:35,991 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:22:36,907 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:22:36,907 - root - INFO - HR Assistant started successfully
2025-07-10 21:22:57,174 - root - INFO - Received question: “Under what conditions can a Grade M-7 employee request early ownership of a company car, and what is the required payment formula?”
2025-07-10 21:22:57,176 - root - INFO - Chain init: 0.00s
2025-07-10 21:22:58,720 - langchain.retrievers.multi_query - INFO - Generated queries: ['1. "What are the eligibility criteria for a Grade M-7 employee to acquire company car ownership prematurely, and how is the buyout price calculated?"', '2. "Company car early ownership policy for M-7 grade: requirements and payment structure."', '3. "Premature company car transfer to Grade M-7 employees: what circumstances allow it, and what is the cost?"']
2025-07-10 21:23:00,778 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:23:01,839 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:23:02,835 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:23:04,151 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:23:06,585 - root - INFO - Question processing: 9.41s
2025-07-10 21:23:06,585 - root - INFO - ⏱️ ask_hr took 9.41 seconds
2025-07-10 21:25:01,114 - root - INFO - Starting HR Assistant...
2025-07-10 21:25:01,301 - root - INFO - Configuration validated successfully
2025-07-10 21:25:01,303 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:25:01,304 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:25:01,304 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:25:01,494 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:25:05,649 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:25:08,195 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:25:11,117 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:25:11,305 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:25:11,305 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:25:11,305 - root - INFO - Loading google LLM...
2025-07-10 21:25:15,524 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:25:16,848 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:25:20,985 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:25:22,044 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:25:23,109 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:25:23,856 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:25:25,287 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:25:26,556 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:25:26,556 - root - INFO - HR Assistant started successfully
2025-07-10 21:26:10,901 - root - INFO - Received question: “Under what conditions can a Grade M-7 employee request early ownership of a company car, and what is the required payment formula?” Please Provide detail answer
2025-07-10 21:26:10,901 - root - INFO - Chain init: 0.00s
2025-07-10 21:26:13,350 - langchain.retrievers.multi_query - INFO - Generated queries: ['1. "What are the eligibility criteria for a Grade M-7 employee to acquire company car ownership before the standard timeframe, and how is the buyout price calculated?"', '2. "Company car early ownership policy: Grade M-7 employees - what circumstances allow for early purchase, and what is the formula used to determine the payment amount?"', '3. "For employees at the M-7 grade level, what are the specific situations that permit a request for early transfer of ownership of a company vehicle, and how is the final cost of the vehicle determined?"']
2025-07-10 21:26:16,237 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:26:17,490 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:26:18,728 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:26:19,875 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:26:22,534 - root - INFO - Question processing: 11.63s
2025-07-10 21:26:22,536 - root - INFO - ⏱️ ask_hr took 11.64 seconds
2025-07-10 21:26:57,134 - root - INFO - Received question: can you give it's formula as well
2025-07-10 21:26:57,142 - root - INFO - Chain init: 0.00s
2025-07-10 21:26:58,328 - langchain.retrievers.multi_query - INFO - Generated queries: ['Could you provide the formula for that?', 'What is the mathematical expression for it?', 'What equation represents it?']
2025-07-10 21:27:01,221 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:27:02,798 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:27:04,102 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:27:04,810 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:27:06,282 - root - INFO - Question processing: 9.14s
2025-07-10 21:27:06,282 - root - INFO - ⏱️ ask_hr took 9.15 seconds
2025-07-10 21:37:09,234 - root - INFO - Starting HR Assistant...
2025-07-10 21:37:09,477 - root - INFO - Configuration validated successfully
2025-07-10 21:37:09,477 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:37:09,477 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:37:09,479 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:37:09,650 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:37:12,850 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:37:14,424 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:37:16,310 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:37:16,474 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:37:16,474 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:37:16,475 - root - INFO - Loading google LLM...
2025-07-10 21:37:19,443 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:37:20,645 - langchain.retrievers.multi_query - INFO - Generated queries: ['testing', 'software quality assurance', 'validation and verification']
2025-07-10 21:37:21,722 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:37:23,561 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:37:24,785 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:37:26,260 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:37:26,987 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:37:28,967 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:37:30,035 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:37:31,435 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:37:32,807 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:37:32,807 - root - INFO - HR Assistant started successfully
2025-07-10 21:38:12,295 - root - INFO - Starting HR Assistant...
2025-07-10 21:38:12,449 - root - INFO - Configuration validated successfully
2025-07-10 21:38:12,449 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:38:12,449 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:38:12,449 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:38:12,622 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:38:16,141 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:38:17,708 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:38:21,265 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:38:21,434 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:38:21,434 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:38:21,434 - root - INFO - Loading google LLM...
2025-07-10 21:38:24,931 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:38:26,307 - langchain.retrievers.multi_query - INFO - Generated queries: ['testing methodologies', 'software testing approaches', 'types of software tests']
2025-07-10 21:38:28,336 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:38:31,398 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:38:33,502 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:38:35,612 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:38:37,071 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:38:39,076 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:38:40,117 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:38:41,588 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:38:43,082 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:38:43,082 - root - INFO - HR Assistant started successfully
2025-07-10 21:38:57,298 - root - INFO - Starting HR Assistant...
2025-07-10 21:38:57,492 - root - INFO - Configuration validated successfully
2025-07-10 21:38:57,492 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:38:57,492 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:38:57,493 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:38:57,695 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:39:01,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:39:03,080 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:39:04,786 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:39:05,000 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:39:05,000 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:39:05,000 - root - INFO - Loading google LLM...
2025-07-10 21:39:08,716 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:39:10,001 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:39:11,719 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:39:16,266 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:39:21,815 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:39:23,840 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:39:25,584 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:39:30,032 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:39:32,183 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:39:34,718 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:39:35,677 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:39:37,847 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:39:40,085 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:39:40,085 - root - INFO - HR Assistant started successfully
2025-07-10 21:41:11,884 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:41:11,900 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:41:11,900 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:41:11,900 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:41:15,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:41:15,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:41:16,604 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:41:16,604 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:41:17,087 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:41:17,087 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:41:28,526 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:41:29,995 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:41:30,328 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:41:34,438 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:41:36,263 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:41:43,257 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:41:44,488 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:41:44,901 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:41:48,455 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:41:50,005 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:41:50,348 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:41:54,627 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 21:41:54,628 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 21:41:54,628 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 21:41:54,788 - root - INFO - Successfully ingested documents. Chunks ingested: 0
2025-07-10 21:41:54,788 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 21:42:27,071 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-10 21:42:27,080 - root - INFO - Successfully uploaded 3 files to Data
2025-07-10 21:43:06,824 - root - INFO - Starting HR Assistant...
2025-07-10 21:43:07,023 - root - INFO - Configuration validated successfully
2025-07-10 21:43:07,023 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:43:07,023 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:43:07,023 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:43:07,205 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:43:10,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:43:11,220 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:43:14,031 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:43:14,240 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:43:14,241 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:43:14,241 - root - INFO - Loading google LLM...
2025-07-10 21:43:17,258 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:43:18,341 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:43:19,999 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:43:22,328 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:43:22,917 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:43:24,546 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:43:24,958 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:43:26,863 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:43:27,348 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:43:28,666 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:43:29,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:43:31,233 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:43:32,402 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:43:32,404 - root - INFO - HR Assistant started successfully
2025-07-10 21:43:46,974 - root - INFO - Starting HR Assistant...
2025-07-10 21:43:47,171 - root - INFO - Configuration validated successfully
2025-07-10 21:43:47,171 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:43:47,171 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:43:47,173 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:43:47,346 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:43:50,125 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:43:51,127 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:43:53,511 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:43:53,690 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:43:53,690 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:43:53,690 - root - INFO - Loading google LLM...
2025-07-10 21:43:56,765 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:43:58,463 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:43:59,692 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:44:01,859 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:44:02,258 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:44:03,728 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:44:04,230 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:44:05,675 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:44:06,135 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:44:07,444 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:44:07,942 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:44:09,424 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:44:10,257 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:44:10,277 - root - INFO - HR Assistant started successfully
2025-07-10 21:44:15,434 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 21:44:15,434 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 21:44:15,434 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 21:44:30,685 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 21:44:30,685 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-10 21:44:30,685 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-10 21:44:30,688 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-10 21:44:30,688 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-10 21:44:30,688 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-10 21:44:30,688 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-10 21:44:30,688 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-10 21:44:30,688 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-10 21:44:30,692 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-10 21:44:30,729 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-10 21:44:30,731 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-10 21:44:30,731 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-10 21:44:30,733 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-10 21:44:30,733 - ingestion_retrieval.ingestion - INFO - Generated 0 chunks for document: 14. Car & Fuel Policy.docx
2025-07-10 21:44:30,733 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 0
2025-07-10 21:44:30,733 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-10 21:44:30,733 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-10 21:44:30,733 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-10 21:44:30,738 - ingestion_retrieval.ingestion - INFO - Generated 47 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-10 21:44:30,738 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 47
2025-07-10 21:44:30,738 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-10 21:44:30,738 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-10 21:44:30,738 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-10 21:44:30,742 - ingestion_retrieval.ingestion - INFO - Generated 28 chunks for document: 9. QG Code of Conduct.docx
2025-07-10 21:44:30,743 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 28
2025-07-10 21:44:30,743 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 75
2025-07-10 21:44:30,743 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:44:33,926 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:44:35,258 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:44:57,379 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 21:44:59,587 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 21:44:59,587 - root - INFO - Successfully ingested documents. Chunks ingested: 75
2025-07-10 21:44:59,602 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 21:45:13,677 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:45:13,685 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:45:13,685 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:45:13,693 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:45:17,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:45:17,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:45:19,296 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:45:19,296 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 21:45:19,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:45:19,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:48:04,146 - root - INFO - Received question: Under what conditions can a Grade M-7 employee request early ownership of a company car, and what is the required payment formula?
2025-07-10 21:48:04,146 - root - INFO - Chain init: 0.00s
2025-07-10 21:48:05,477 - langchain.retrievers.multi_query - INFO - Generated queries: ['1. What are the eligibility criteria and payment calculation for a Grade M-7 employee to purchase their company car before the standard ownership transfer date?', '2. Company car early buyout policy: Grade M-7 employee requirements and cost calculation.', '3. What circumstances allow an M-7 grade employee to gain early title to a company vehicle, and how is the purchase price determined?']
2025-07-10 21:48:07,619 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:48:09,752 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:48:11,819 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:48:13,953 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:48:15,285 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:48:17,586 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:48:18,652 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:48:20,840 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:48:22,936 - root - INFO - Question processing: 18.79s
2025-07-10 21:48:22,936 - root - INFO - ⏱️ ask_hr took 18.79 seconds
2025-07-10 21:51:07,664 - root - INFO - Starting HR Assistant...
2025-07-10 21:51:07,851 - root - INFO - Configuration validated successfully
2025-07-10 21:51:07,851 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:51:07,853 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:51:07,853 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:51:08,060 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:51:12,540 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:51:14,930 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:51:17,653 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:51:17,834 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:51:17,834 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:51:17,834 - root - INFO - Loading google LLM...
2025-07-10 21:51:22,507 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:51:23,903 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:51:25,660 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:51:28,536 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:51:30,541 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:51:32,106 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:51:33,835 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:51:35,568 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:51:37,552 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:51:39,287 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:51:40,845 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:51:42,914 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:51:44,732 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:51:44,732 - root - INFO - HR Assistant started successfully
2025-07-10 21:51:56,199 - root - INFO - Starting HR Assistant...
2025-07-10 21:51:56,355 - root - INFO - Configuration validated successfully
2025-07-10 21:51:56,357 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:51:56,358 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:51:56,358 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:51:56,519 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:51:59,570 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:52:01,637 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:52:05,504 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:52:05,671 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:52:05,671 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:52:05,671 - root - INFO - Loading google LLM...
2025-07-10 21:52:09,910 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:52:11,476 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Evaluation and assessment methods.']
2025-07-10 21:52:15,709 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:52:17,383 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:52:18,142 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:52:19,383 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:52:20,468 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:52:21,999 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:52:21,999 - root - INFO - HR Assistant started successfully
2025-07-10 21:55:15,248 - root - INFO - Starting HR Assistant...
2025-07-10 21:55:15,439 - root - INFO - Configuration validated successfully
2025-07-10 21:55:15,441 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:55:15,442 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:55:15,442 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:55:15,629 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:55:19,597 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:55:21,820 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:55:25,638 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:55:25,863 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:55:25,865 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:55:25,865 - root - INFO - Loading google LLM...
2025-07-10 21:55:29,075 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:55:30,801 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:55:34,692 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:55:36,324 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:55:37,661 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:55:40,559 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:55:42,130 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:55:43,269 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:55:43,269 - root - INFO - HR Assistant started successfully
2025-07-10 21:55:54,476 - root - INFO - Starting HR Assistant...
2025-07-10 21:55:54,640 - root - INFO - Configuration validated successfully
2025-07-10 21:55:54,640 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 21:55:54,640 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 21:55:54,640 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 21:55:54,806 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 21:56:00,916 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 21:56:02,344 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 21:56:05,164 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 21:56:05,338 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 21:56:05,338 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 21:56:05,338 - root - INFO - Loading google LLM...
2025-07-10 21:56:09,448 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 21:56:10,649 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 21:56:13,552 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:56:14,788 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:56:16,029 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:56:17,177 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:56:18,825 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 21:56:19,825 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 21:56:19,825 - root - INFO - HR Assistant started successfully
2025-07-10 22:07:22,638 - root - INFO - Starting HR Assistant...
2025-07-10 22:07:22,807 - root - INFO - Configuration validated successfully
2025-07-10 22:07:22,807 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:07:22,807 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:07:22,817 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:07:23,013 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:07:27,493 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:07:29,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:07:32,291 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:07:32,494 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:07:32,496 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:07:32,498 - root - INFO - Loading google LLM...
2025-07-10 22:07:34,717 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:07:34,758 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: 1 validation error for MultiQueryRetriever
retriever
  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<ingestion_retrieval.retr...t at 0x00000225D79B5250>, input_type=get_hr_assistant_chain.<locals>.MetadataAwareRetriever]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 415, in get_hr_assistant_chain
    Mqr = MultiQueryRetriever.from_llm(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 86, in from_llm
    return cls(
           ^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for MultiQueryRetriever
retriever
  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<ingestion_retrieval.retr...t at 0x00000225D79B5250>, input_type=get_hr_assistant_chain.<locals>.MetadataAwareRetriever]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-07-10 22:07:34,772 - root - ERROR - Startup failed: 1 validation error for MultiQueryRetriever
retriever
  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<ingestion_retrieval.retr...t at 0x00000225D79B5250>, input_type=get_hr_assistant_chain.<locals>.MetadataAwareRetriever]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 186, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 415, in get_hr_assistant_chain
    Mqr = MultiQueryRetriever.from_llm(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 86, in from_llm
    return cls(
           ^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for MultiQueryRetriever
retriever
  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<ingestion_retrieval.retr...t at 0x00000225D79B5250>, input_type=get_hr_assistant_chain.<locals>.MetadataAwareRetriever]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-07-10 22:07:49,553 - root - INFO - Starting HR Assistant...
2025-07-10 22:07:49,752 - root - INFO - Configuration validated successfully
2025-07-10 22:07:49,752 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:07:49,752 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:07:49,752 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:07:49,942 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:07:53,427 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:07:54,779 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:07:56,736 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:07:56,930 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:07:56,930 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:07:56,930 - root - INFO - Loading google LLM...
2025-07-10 22:08:00,391 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:08:00,391 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: "MetadataAwareRetriever" object has no field "base_retriever"
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 411, in get_hr_assistant_chain
    self.base_retriever.search_kwargs["filter"] = filter_criteria
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 317, in __init__
    timings['load_llm'] = time.time() - llm_start
            ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 997, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 1044, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "MetadataAwareRetriever" object has no field "base_retriever"
2025-07-10 22:08:00,445 - root - ERROR - Startup failed: "MetadataAwareRetriever" object has no field "base_retriever"
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 186, in get_cached_hr_assistant_chain
    return True
       ^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 411, in get_hr_assistant_chain
    self.base_retriever.search_kwargs["filter"] = filter_criteria
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 317, in __init__
    timings['load_llm'] = time.time() - llm_start
            ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 997, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 1044, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "MetadataAwareRetriever" object has no field "base_retriever"
2025-07-10 22:08:15,106 - root - INFO - Starting HR Assistant...
2025-07-10 22:08:15,299 - root - INFO - Configuration validated successfully
2025-07-10 22:08:15,299 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:08:15,301 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:08:15,301 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:08:15,488 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:08:19,018 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:08:20,390 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:08:23,202 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:08:23,424 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:08:23,424 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:08:23,424 - root - INFO - Loading google LLM...
2025-07-10 22:08:27,803 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:08:27,805 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: "MetadataAwareRetriever" object has no field "base_retriever"
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 426, in get_hr_assistant_chain
    metadata_retriever = MetadataAwareRetriever(base_retriever, llm)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 326, in __init__
    self.base_retriever = base_retriever
    ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 997, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 1044, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "MetadataAwareRetriever" object has no field "base_retriever"
2025-07-10 22:08:27,818 - root - ERROR - Startup failed: "MetadataAwareRetriever" object has no field "base_retriever"
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 195, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 426, in get_hr_assistant_chain
    metadata_retriever = MetadataAwareRetriever(base_retriever, llm)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 326, in __init__
    self.base_retriever = base_retriever
    ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 997, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 1044, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "MetadataAwareRetriever" object has no field "base_retriever"
2025-07-10 22:08:39,277 - root - INFO - Starting HR Assistant...
2025-07-10 22:08:39,424 - root - INFO - Configuration validated successfully
2025-07-10 22:08:39,424 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:08:39,424 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:08:39,424 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:08:39,586 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:08:42,343 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:08:43,398 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:08:45,472 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:08:45,629 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:08:45,629 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:08:45,629 - root - INFO - Loading google LLM...
2025-07-10 22:08:49,199 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:08:49,199 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: "MetadataAwareRetriever" object has no field "base_retriever"
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 426, in get_hr_assistant_chain
    metadata_retriever = MetadataAwareRetriever(base_retriever, llm)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 326, in __init__
    self.base_retriever = base_retriever
    ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 997, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 1044, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "MetadataAwareRetriever" object has no field "base_retriever"
2025-07-10 22:08:49,199 - root - ERROR - Startup failed: "MetadataAwareRetriever" object has no field "base_retriever"
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 195, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 426, in get_hr_assistant_chain
    metadata_retriever = MetadataAwareRetriever(base_retriever, llm)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 326, in __init__
    self.base_retriever = base_retriever
    ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 997, in __setattr__
    elif (setattr_handler := self._setattr_handler(name, value)) is not None:
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\pydantic\main.py", line 1044, in _setattr_handler
    raise ValueError(f'"{cls.__name__}" object has no field "{name}"')
ValueError: "MetadataAwareRetriever" object has no field "base_retriever"
2025-07-10 22:10:53,216 - root - INFO - Starting HR Assistant...
2025-07-10 22:10:53,410 - root - INFO - Configuration validated successfully
2025-07-10 22:10:53,412 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:10:53,414 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:10:53,414 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:10:53,652 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:10:57,781 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:10:59,855 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:11:04,154 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:11:04,346 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:11:04,348 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:11:04,348 - root - INFO - Loading google LLM...
2025-07-10 22:11:08,533 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:11:10,005 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 22:11:13,828 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:15,276 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:16,200 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:17,124 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:18,309 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:19,532 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:11:19,546 - root - INFO - HR Assistant started successfully
2025-07-10 22:11:32,412 - root - INFO - Starting HR Assistant...
2025-07-10 22:11:32,606 - root - INFO - Configuration validated successfully
2025-07-10 22:11:32,608 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:11:32,608 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:11:32,608 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:11:32,790 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:11:36,437 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:11:37,779 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:11:40,400 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:11:40,562 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:11:40,564 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:11:40,564 - root - INFO - Loading google LLM...
2025-07-10 22:11:44,037 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:11:45,104 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 22:11:47,731 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:48,875 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:50,112 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:51,381 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:53,366 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:11:55,923 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:11:55,923 - root - INFO - HR Assistant started successfully
2025-07-10 22:13:07,185 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:13:08,663 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:13:09,074 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:13:12,125 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:13:12,945 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:13:20,197 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:13:21,504 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:13:21,933 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:13:26,247 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:13:28,100 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:13:28,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:13:39,487 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 22:13:39,487 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:13:39,487 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:13:39,648 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 0 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:13:39,648 - ingestion_retrieval.ingestion - ERROR - No documents found in directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:13:39,648 - root - INFO - Successfully ingested documents. Chunks ingested: 0
2025-07-10 22:13:39,648 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:14:35,236 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-10 22:14:35,236 - root - INFO - Successfully uploaded 3 files to Data
2025-07-10 22:14:37,836 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 22:14:37,837 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:14:37,837 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-10 22:14:49,437 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-10 22:14:49,443 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-10 22:14:49,443 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-10 22:14:49,443 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-10 22:14:49,445 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-10 22:14:49,445 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-10 22:14:49,446 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-10 22:14:49,613 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:14:49,613 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:15:31,820 - ingestion_retrieval.ingestion - INFO - Generated 99 chunks for document: 14. Car & Fuel Policy.docx
2025-07-10 22:15:31,822 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 99
2025-07-10 22:15:31,822 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-10 22:15:31,822 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-10 22:15:31,822 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-10 22:15:31,987 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:15:31,987 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:16:15,272 - ingestion_retrieval.ingestion - INFO - Generated 68 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-10 22:16:15,272 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 68
2025-07-10 22:16:15,272 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-10 22:16:15,272 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-10 22:16:15,272 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-10 22:16:15,441 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:16:15,441 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:16:43,186 - ingestion_retrieval.ingestion - INFO - Generated 41 chunks for document: 9. QG Code of Conduct.docx
2025-07-10 22:16:43,186 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 41
2025-07-10 22:16:43,186 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 208
2025-07-10 22:16:43,186 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:16:46,050 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:16:47,964 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:17:07,244 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:17:16,964 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:17:27,806 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:17:30,548 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:17:30,551 - root - INFO - Successfully ingested documents. Chunks ingested: 208
2025-07-10 22:17:30,551 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:18:08,172 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:18:08,172 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:18:09,576 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:18:09,577 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:18:10,138 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:18:10,138 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:18:10,150 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:18:10,150 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:18:10,151 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:18:10,151 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:18:42,151 - root - INFO - Received question: Under what conditions can a Grade M-7 employee request early ownership of a company car, and what is the required payment formula?
2025-07-10 22:18:42,151 - root - INFO - Chain init: 0.00s
2025-07-10 22:18:44,873 - langchain.retrievers.multi_query - INFO - Generated queries: ['What are the eligibility criteria for a Grade M-7 employee to acquire company car ownership prematurely, and how is the buyout price calculated?', 'What is the company policy regarding early transfer of ownership for company cars assigned to Grade M-7 employees, specifically addressing the conditions and payment structure?', 'Can an employee at the M-7 grade level request to purchase their company vehicle before the standard ownership transfer period, and if so, what factors determine the cost?']
2025-07-10 22:18:49,161 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 400 Bad Request"
2025-07-10 22:18:49,161 - root - ERROR - Error: Unexpected Response: 400 (Bad Request)
Raw response content:
b'{"status":{"error":"Bad request: Index required but not found for \\"topic\\" of one of the following types: [text, text]. Help: Create an index for this key or use a different filter."},"time":0.000 ...'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 407, in ask_hr
    result = qa_chain.invoke({"input": request.question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 460, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 172, in _get_relevant_documents
    documents = self.retrieve_documents(queries, run_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 210, in retrieve_documents
    docs = self.retriever.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1079, in _get_relevant_documents
    docs = self.vectorstore.similarity_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 468, in similarity_search
    results = self.similarity_search_with_score(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 512, in similarity_search_with_score
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 593, in query_points
    return self._client.query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 673, in query_points
    query_result = self.http.search_api.query_points(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\search_api.py", line 783, in query_points
    return self._build_for_query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\search_api.py", line 181, in _build_for_query_points
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 400 (Bad Request)
Raw response content:
b'{"status":{"error":"Bad request: Index required but not found for \\"topic\\" of one of the following types: [text, text]. Help: Create an index for this key or use a different filter."},"time":0.000 ...'
2025-07-10 22:19:38,783 - root - INFO - Starting HR Assistant...
2025-07-10 22:19:38,967 - root - INFO - Configuration validated successfully
2025-07-10 22:19:38,983 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:19:38,983 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:19:38,983 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:19:39,184 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:19:43,372 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:19:45,001 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:19:47,267 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:19:47,453 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:19:47,453 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:19:47,454 - root - INFO - Loading google LLM...
2025-07-10 22:19:51,018 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:19:52,245 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 22:19:55,154 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:19:56,772 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:19:58,664 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:00,067 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:01,046 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:02,379 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:20:02,379 - root - INFO - HR Assistant started successfully
2025-07-10 22:20:13,466 - root - INFO - Starting HR Assistant...
2025-07-10 22:20:13,636 - root - INFO - Configuration validated successfully
2025-07-10 22:20:13,636 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:20:13,636 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:20:13,636 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:20:13,801 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:20:16,623 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:20:18,347 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:20:21,384 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:20:21,552 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:20:21,552 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:20:21,552 - root - INFO - Loading google LLM...
2025-07-10 22:20:24,957 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:20:26,077 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 22:20:28,552 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:29,863 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:31,265 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:32,260 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:33,556 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:20:34,731 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:20:34,732 - root - INFO - HR Assistant started successfully
2025-07-10 22:21:02,089 - root - INFO - Received question: Under what conditions can a Grade M-7 employee request early ownership of a company car, and what is the required payment formula?
2025-07-10 22:21:02,089 - root - INFO - Chain init: 0.00s
2025-07-10 22:21:03,688 - langchain.retrievers.multi_query - INFO - Generated queries: ['1. What are the eligibility criteria and payment calculation for a Grade M-7 employee to purchase their company car before the standard ownership transfer date?', '2. Company car early buyout policy: Grade M-7 employee requirements and cost calculation.', '3. Under what circumstances and at what price can an employee at the M-7 level obtain ownership of their assigned company vehicle prematurely?']
2025-07-10 22:21:07,256 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:21:08,190 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:21:10,297 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:21:11,272 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:21:13,344 - root - INFO - Question processing: 11.25s
2025-07-10 22:21:13,344 - root - INFO - ⏱️ ask_hr took 11.25 seconds
2025-07-10 22:33:46,182 - root - INFO - Starting HR Assistant...
2025-07-10 22:33:46,359 - root - INFO - Configuration validated successfully
2025-07-10 22:33:46,359 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:33:46,359 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:33:46,359 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:33:46,535 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:33:49,938 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:33:52,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:33:57,286 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:33:57,461 - root - INFO - [LLM CONFIG] model_priority=2, groq_api_key=set, google_api_key=set
2025-07-10 22:33:57,462 - root - INFO - [LLM CONFIG] Selecting Google as provider
2025-07-10 22:33:57,462 - root - INFO - Loading google LLM...
2025-07-10 22:34:00,021 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:34:01,410 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are 3 different versions of the user question "test", designed to retrieve relevant documents from a vector database by offering alternative perspectives:', '1. What is a test?', '2. Examples of tests and their purpose.', '3. Different types of testing methodologies.']
2025-07-10 22:34:02,746 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:34:03,587 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:34:04,413 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:34:05,081 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:34:05,804 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:34:06,732 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:34:06,732 - root - INFO - HR Assistant started successfully
2025-07-10 22:36:18,075 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:36:20,801 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:36:21,459 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:36:25,913 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:36:27,898 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:36:40,769 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:36:43,317 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:36:44,232 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:36:49,360 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:36:51,309 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:36:52,262 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:37:06,135 - root - INFO - Starting HR Assistant...
2025-07-10 22:37:06,313 - root - INFO - Configuration validated successfully
2025-07-10 22:37:06,313 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:37:06,313 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:37:06,313 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:37:06,481 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:37:10,609 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:37:12,751 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:37:16,168 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:37:16,323 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:37:16,339 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:37:16,339 - root - INFO - Loading groq LLM...
2025-07-10 22:37:18,935 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:37:19,008 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:37:19,674 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:37:19,686 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test" to retrieve relevant documents from a vector database:', 'What is the purpose of a test?', 'Can I take a sample test online?', 'What are the different types of tests used in education?', 'These alternative questions provide different perspectives on the original question, which can help overcome the limitations of distance-based similarity search by capturing various aspects of the concept of "test".']
2025-07-10 22:37:24,534 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:37:26,000 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:37:27,505 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:37:28,556 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:37:29,556 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:37:30,147 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:37:30,775 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:37:30,778 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:37:30,778 - root - INFO - HR Assistant started successfully
2025-07-10 22:37:44,802 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-10 22:37:44,818 - root - INFO - Successfully uploaded 3 files to Data
2025-07-10 22:37:48,573 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:37:48,573 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:37:52,532 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 22:37:52,534 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:37:52,534 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:38:04,943 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:38:04,944 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-10 22:38:04,944 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-10 22:38:04,945 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-10 22:38:04,945 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-10 22:38:04,947 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-10 22:38:04,947 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-10 22:38:04,948 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-10 22:38:04,949 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-10 22:38:04,950 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-10 22:38:04,950 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-10 22:38:04,950 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-10 22:38:04,951 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-10 22:38:04,951 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-10 22:38:05,122 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:38:05,123 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:38:09,702 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 503 Service Unavailable"
2025-07-10 22:38:09,702 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 0.391638 seconds
2025-07-10 22:38:10,301 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-07-10 22:38:10,303 - ingestion_retrieval.ingestion - INFO - Generated 99 chunks for document: 14. Car & Fuel Policy.docx
2025-07-10 22:38:10,303 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 99
2025-07-10 22:38:10,303 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-10 22:38:10,303 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-10 22:38:10,303 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-10 22:38:10,484 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:38:10,486 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:38:15,536 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-07-10 22:38:15,540 - ingestion_retrieval.ingestion - INFO - Generated 68 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-10 22:38:15,540 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 68
2025-07-10 22:38:15,541 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-10 22:38:15,541 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-10 22:38:15,543 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-10 22:38:15,711 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:38:15,718 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:38:31,116 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:38:31,130 - ingestion_retrieval.ingestion - INFO - Generated 39 chunks for document: 9. QG Code of Conduct.docx
2025-07-10 22:38:31,132 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 39
2025-07-10 22:38:31,132 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 206
2025-07-10 22:38:31,132 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:38:34,965 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:38:37,690 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:38:45,302 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:38:56,642 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:39:06,525 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:39:09,743 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:39:09,743 - root - INFO - Successfully ingested documents. Chunks ingested: 206
2025-07-10 22:39:09,762 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:39:31,825 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:39:31,828 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:39:33,799 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:39:33,799 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:39:34,542 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:39:34,542 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:39:34,588 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:39:34,588 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:39:34,588 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:39:34,588 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:39:52,530 - root - INFO - Received question: Under what conditions can a Grade M-7 employee request early ownership of a company car, and what is the required payment formula?
2025-07-10 22:39:52,532 - root - INFO - Chain init: 0.00s
2025-07-10 22:39:55,096 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:39:55,102 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the eligibility criteria for a Grade M-7 employee to acquire a company car ahead of schedule, and how is the purchase price calculated?', 'Can a Grade M-7 staff member accelerate their company car ownership, and what is the formula used to determine the required payment or compensation?', 'What are the rules and regulations governing early company car ownership for Grade M-7 employees, including the necessary payment structure and calculation methodology?']
2025-07-10 22:39:59,502 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:40:01,026 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:40:01,741 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:40:03,174 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:40:04,118 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:40:05,927 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:40:05,970 - root - INFO - Question processing: 13.44s
2025-07-10 22:40:05,970 - root - INFO - ⏱️ ask_hr took 13.44 seconds
2025-07-10 22:44:05,968 - root - INFO - Received question: What are the reporting obligations and possible consequences if a Grade M-7 employee fails to report the theft of their company car on time?
2025-07-10 22:44:05,969 - root - INFO - Chain init: 0.00s
2025-07-10 22:44:07,146 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:44:07,163 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the disclosure requirements and potential penalties for a Grade M-7 employee who delays reporting the theft of their company vehicle?', 'What happens if a Grade M-7 employee fails to notify the company promptly about the theft of their assigned car, and what are the reporting obligations in such a scenario?', 'What are the consequences of non-compliance with company policies for a Grade M-7 employee who does not report the theft of their company car in a timely manner, and what are the necessary steps to take in such a situation?']
2025-07-10 22:44:11,625 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:44:12,218 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:44:12,792 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:44:13,444 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:44:14,016 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:44:15,547 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:44:15,555 - root - INFO - Question processing: 9.59s
2025-07-10 22:44:15,555 - root - INFO - ⏱️ ask_hr took 9.59 seconds
2025-07-10 22:46:50,695 - root - INFO - Starting HR Assistant...
2025-07-10 22:46:50,862 - root - INFO - Configuration validated successfully
2025-07-10 22:46:50,862 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:46:50,862 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:46:50,862 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:46:51,023 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:46:55,108 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:46:58,863 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:47:02,232 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:47:02,401 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:47:02,401 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:47:02,401 - root - INFO - Loading groq LLM...
2025-07-10 22:47:05,145 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:47:05,168 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:47:06,804 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:47:06,816 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test":', 'What is the purpose of a test?', 'How do I prepare for a test?', 'What are the benefits of testing in a particular context?', "These rephrased questions can help retrieve relevant documents from a vector database by capturing different aspects of the original question, such as the purpose, preparation, and benefits of testing. This can overcome some of the limitations of distance-based similarity search by providing a more comprehensive understanding of the user's query."]
2025-07-10 22:47:12,829 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:47:13,915 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:47:14,932 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:47:15,616 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:47:16,464 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:47:17,284 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:47:19,141 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:47:19,145 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:47:19,146 - root - INFO - HR Assistant started successfully
2025-07-10 22:47:35,969 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:47:39,705 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:47:41,541 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:47:45,373 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:47:46,716 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:47:54,280 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:47:56,351 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:47:56,921 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:48:01,833 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:48:05,866 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:48:06,450 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:48:17,450 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-10 22:48:17,497 - root - INFO - Successfully uploaded 3 files to Data
2025-07-10 22:48:18,440 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-10 22:48:18,440 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:48:18,440 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:48:31,035 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:48:31,035 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-10 22:48:31,035 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-10 22:48:31,037 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-10 22:48:31,037 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-10 22:48:31,037 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-10 22:48:31,037 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-10 22:48:31,039 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-10 22:48:31,039 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-10 22:48:31,039 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-10 22:48:31,039 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-10 22:48:31,041 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-10 22:48:31,041 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-10 22:48:31,042 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-10 22:48:31,192 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:48:31,192 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:48:32,542 - ingestion_retrieval.ingestion - INFO - Generated 99 chunks for document: 14. Car & Fuel Policy.docx
2025-07-10 22:48:32,542 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 99
2025-07-10 22:48:32,542 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-10 22:48:32,542 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-10 22:48:32,542 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-10 22:48:32,709 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:48:32,709 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:48:34,178 - ingestion_retrieval.ingestion - INFO - Generated 68 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-10 22:48:34,179 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 68
2025-07-10 22:48:34,179 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-10 22:48:34,180 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-10 22:48:34,181 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-10 22:48:34,411 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:48:34,413 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:48:35,813 - ingestion_retrieval.ingestion - INFO - Generated 39 chunks for document: 9. QG Code of Conduct.docx
2025-07-10 22:48:35,813 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 39
2025-07-10 22:48:35,813 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 206
2025-07-10 22:48:35,813 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:48:39,170 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:48:40,837 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:48:51,010 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:48:54,867 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:49:00,807 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:49:03,651 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-10 22:49:03,655 - root - INFO - Successfully ingested documents. Chunks ingested: 206
2025-07-10 22:49:03,657 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-10 22:49:31,180 - root - INFO - Starting HR Assistant...
2025-07-10 22:49:31,341 - root - INFO - Configuration validated successfully
2025-07-10 22:49:31,341 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:49:31,341 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:49:31,341 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:49:31,540 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:49:35,279 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:49:37,919 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:49:43,052 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:49:43,222 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:49:43,223 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:49:43,223 - root - INFO - Loading groq LLM...
2025-07-10 22:49:48,034 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:49:48,052 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:49:51,849 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:49:51,849 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test":', 'What is the purpose of a test?', 'How do I prepare for a test?', 'What are the benefits of testing in a particular context?', "These rephrased questions can help retrieve relevant documents from a vector database by capturing different aspects of the original question, such as the purpose, preparation, and benefits of testing. This can overcome some of the limitations of distance-based similarity search by providing a more comprehensive understanding of the user's query."]
2025-07-10 22:49:53,432 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:49:55,713 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:49:57,144 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:49:58,708 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:49:59,129 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:50:00,382 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:50:00,829 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:50:02,062 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:50:02,519 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:50:03,740 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:50:04,898 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:50:04,904 - root - INFO - Optimized RAG pipeline pre-warmed successfully
2025-07-10 22:50:04,905 - root - INFO - HR Assistant started successfully
2025-07-10 22:50:17,308 - root - INFO - Starting HR Assistant...
2025-07-10 22:50:17,507 - root - INFO - Configuration validated successfully
2025-07-10 22:50:17,507 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:50:17,507 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:50:17,507 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:50:17,674 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:50:21,823 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:50:24,144 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:50:25,470 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:50:25,678 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:50:25,678 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:50:25,678 - root - INFO - Loading groq LLM...
2025-07-10 22:50:32,443 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:50:32,457 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:50:34,160 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:50:34,163 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question "test" to retrieve relevant documents from a vector database:', 'What is the purpose of a test?', 'Can I take a sample test online?', 'What are the different types of tests used in education?', 'These alternative questions provide different perspectives on the original question, which can help overcome the limitations of distance-based similarity search by capturing various aspects of the concept of "test".']
2025-07-10 22:50:37,543 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:50:44,707 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:50:47,871 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:50:49,124 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:50:50,169 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:50:56,107 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:51:06,462 - root - ERROR - Startup failed: The read operation timed out
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 127, in __iter__
    for part in self._httpcore_stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 407, in __iter__
    raise exc from None
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 403, in __iter__
    for part in self._stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 342, in __iter__
    raise exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 334, in __iter__
    for chunk in self._connection._receive_response_body(**kwargs):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 203, in _receive_response_body
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 126, in read
    with map_exceptions(exc_map):
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 134, in send_inner
    response = self._client.send(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 928, in send
    raise exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 922, in send
    response.read()
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_models.py", line 881, in read
    self._content = b"".join(self.iter_bytes())
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_models.py", line 897, in iter_bytes
    for raw_bytes in self.iter_raw():
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_models.py", line 951, in iter_raw
    for raw_stream_bytes in self.stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 153, in __iter__
    for chunk in self._stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 126, in __iter__
    with map_httpcore_exceptions():
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 129, in startup_event
    qa_chain.invoke({"input": "test"})
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 334, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 172, in _get_relevant_documents
    documents = self.retrieve_documents(queries, run_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 210, in retrieve_documents
    docs = self.retriever.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 651, in max_marginal_relevance_search
    return self.max_marginal_relevance_search_by_vector(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 683, in max_marginal_relevance_search_by_vector
    results = self.max_marginal_relevance_search_with_score_by_vector(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 716, in max_marginal_relevance_search_with_score_by_vector
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 593, in query_points
    return self._client.query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 673, in query_points
    query_result = self.http.search_api.query_points(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\search_api.py", line 783, in query_points
    return self._build_for_query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\search_api.py", line 181, in _build_for_query_points
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 112, in send
    response = self.middleware(request, self.send_inner)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 250, in __call__
    return call_next(request)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 136, in send_inner
    raise ResponseHandlingException(e)
qdrant_client.http.exceptions.ResponseHandlingException: The read operation timed out
2025-07-10 22:52:24,481 - root - INFO - Starting HR Assistant...
2025-07-10 22:52:24,660 - root - INFO - Configuration validated successfully
2025-07-10 22:52:24,661 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:52:24,662 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:52:24,662 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:52:24,836 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:52:28,775 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:52:29,391 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:52:30,682 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:52:30,860 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:52:30,861 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:52:30,863 - root - INFO - Loading groq LLM...
2025-07-10 22:52:34,815 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:52:34,834 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:52:40,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:52:41,238 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:52:42,248 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:52:42,248 - root - WARNING - Could not test pipeline during startup: string indices must be integers, not 'str'
2025-07-10 22:52:42,248 - root - INFO - Pipeline will be tested on first query
2025-07-10 22:52:42,248 - root - INFO - HR Assistant started successfully
2025-07-10 22:52:42,248 - root - INFO - HR Assistant started successfully
2025-07-10 22:52:53,619 - root - INFO - Starting HR Assistant...
2025-07-10 22:52:53,791 - root - INFO - Configuration validated successfully
2025-07-10 22:52:53,792 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:52:53,792 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:52:53,793 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:52:53,962 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:52:56,841 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:52:58,275 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:53:00,791 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:53:00,956 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:53:00,957 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:53:00,957 - root - INFO - Loading groq LLM...
2025-07-10 22:53:04,076 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:53:04,093 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:53:06,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:53:07,244 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:53:07,516 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:53:07,519 - root - WARNING - Could not test pipeline during startup: string indices must be integers, not 'str'
2025-07-10 22:53:07,520 - root - INFO - Pipeline will be tested on first query
2025-07-10 22:53:07,520 - root - INFO - HR Assistant started successfully
2025-07-10 22:53:09,927 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:53:11,468 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:53:12,360 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:53:12,360 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:53:12,375 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:53:12,394 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:53:12,394 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:53:15,655 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:53:17,400 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 22:53:17,917 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:53:28,328 - root - INFO - Received question: What are the reporting obligations and possible consequences if a Grade M-7 employee fails to report the theft of their company car on time?
2025-07-10 22:53:28,329 - root - INFO - Chain init: 0.00s
2025-07-10 22:53:30,977 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:53:30,977 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the disclosure requirements and potential penalties for a Grade M-7 employee who delays reporting the theft of their company vehicle?', 'What happens if a Grade M-7 employee fails to notify their employer promptly about the theft of their company car, and what are the reporting obligations in such a scenario?', 'What are the consequences of non-compliance with company policies for a Grade M-7 employee who does not report the theft of their company car in a timely manner, and what are the necessary steps to take in such a situation?', 'These alternative questions can help retrieve relevant documents from a vector database by providing different perspectives on the original question, which can overcome some of the limitations of distance-based similarity search.']
2025-07-10 22:53:32,564 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:53:35,030 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:53:40,544 - root - ERROR - Error: The read operation timed out
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 127, in __iter__
    for part in self._httpcore_stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 407, in __iter__
    raise exc from None
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 403, in __iter__
    for part in self._stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 342, in __iter__
    raise exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 334, in __iter__
    for chunk in self._connection._receive_response_body(**kwargs):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 203, in _receive_response_body
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 126, in read
    with map_exceptions(exc_map):
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 134, in send_inner
    response = self._client.send(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 928, in send
    raise exc
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 922, in send
    response.read()
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_models.py", line 881, in read
    self._content = b"".join(self.iter_bytes())
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_models.py", line 897, in iter_bytes
    for raw_bytes in self.iter_raw():
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_models.py", line 951, in iter_raw
    for raw_stream_bytes in self.stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_client.py", line 153, in __iter__
    for chunk in self._stream:
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 126, in __iter__
    with map_httpcore_exceptions():
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 155, in __exit__
    self.gen.throw(value)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 428, in ask_hr
    result = qa_chain.invoke({"input": request.question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4772, in invoke
    return self._call_with_config(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 4630, in _invoke
    output = call_func_with_variable_args(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 334, in chain_with_memory
    output = retrieval_chain.invoke(chain_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3045, in invoke
    input_ = context.run(step.invoke, input_, config, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 511, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 1940, in _call_with_config
    context.run(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\config.py", line 428, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\passthrough.py", line 497, in _invoke
    **self.mapper.invoke(
      ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3774, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
                   ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3758, in _invoke_step
    return context.run(
           ^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 5431, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\runnables\base.py", line 3047, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 172, in _get_relevant_documents
    documents = self.retrieve_documents(queries, run_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\retrievers\multi_query.py", line 210, in retrieve_documents
    docs = self.retriever.invoke(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\retrievers.py", line 259, in invoke
    result = self._get_relevant_documents(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\vectorstores\base.py", line 1088, in _get_relevant_documents
    docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 651, in max_marginal_relevance_search
    return self.max_marginal_relevance_search_by_vector(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 683, in max_marginal_relevance_search_by_vector
    results = self.max_marginal_relevance_search_with_score_by_vector(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 716, in max_marginal_relevance_search_with_score_by_vector
    results = self.client.query_points(
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 593, in query_points
    return self._client.query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 673, in query_points
    query_result = self.http.search_api.query_points(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\search_api.py", line 783, in query_points
    return self._build_for_query_points(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\search_api.py", line 181, in _build_for_query_points
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 112, in send
    response = self.middleware(request, self.send_inner)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 250, in __call__
    return call_next(request)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 136, in send_inner
    raise ResponseHandlingException(e)
qdrant_client.http.exceptions.ResponseHandlingException: The read operation timed out
2025-07-10 22:54:25,814 - root - INFO - Starting HR Assistant...
2025-07-10 22:54:26,003 - root - INFO - Configuration validated successfully
2025-07-10 22:54:26,004 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:54:26,004 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:54:26,005 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:54:26,191 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:54:30,079 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:54:31,326 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:54:33,276 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:54:33,446 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:54:33,446 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:54:33,446 - root - INFO - Loading groq LLM...
2025-07-10 22:54:36,379 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:54:36,411 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:54:36,416 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-10 22:54:36,416 - root - INFO - HR Assistant started successfully
2025-07-10 22:54:56,197 - root - INFO - Received question: What are the reporting obligations and possible consequences if a Grade M-7 employee fails to report the theft of their company car on time?
2025-07-10 22:54:56,198 - root - INFO - Chain init: 0.00s
2025-07-10 22:54:58,255 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:54:58,255 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the disclosure requirements and potential penalties for a Grade M-7 employee who delays reporting the theft of their company vehicle?', 'What happens if a Grade M-7 employee fails to notify the company promptly about the theft of their assigned car, and what are the reporting responsibilities in such a scenario?', 'What are the consequences of not reporting a stolen company car in a timely manner for a Grade M-7 employee, and what are the mandatory reporting obligations in this situation?']
2025-07-10 22:54:59,412 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:55:02,354 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:55:07,274 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:55:08,543 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:55:10,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:55:11,455 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:55:12,721 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:55:14,051 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:55:18,954 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:55:18,962 - root - INFO - Question processing: 22.76s
2025-07-10 22:55:18,964 - root - INFO - ⏱️ ask_hr took 22.77 seconds
2025-07-10 22:57:12,036 - root - INFO - Starting HR Assistant...
2025-07-10 22:57:12,204 - root - INFO - Configuration validated successfully
2025-07-10 22:57:12,204 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 22:57:12,204 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 22:57:12,206 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 22:57:12,374 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 22:57:18,626 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 22:57:20,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:57:23,751 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 22:57:23,922 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 22:57:23,926 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 22:57:23,926 - root - INFO - Loading groq LLM...
2025-07-10 22:57:28,086 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:57:28,119 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 22:57:28,123 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-10 22:57:28,124 - root - INFO - HR Assistant started successfully
2025-07-10 22:58:52,178 - root - INFO - Received question: What are the reporting obligations and possible consequences if a Grade M-7 employee fails to report the theft of their company car on time?
2025-07-10 22:58:52,179 - root - INFO - Chain init: 0.00s
2025-07-10 22:58:54,495 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:58:54,502 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the disclosure requirements and potential penalties for a Grade M-7 employee who delays reporting the theft of their company vehicle?', 'What happens if a Grade M-7 employee fails to notify the company promptly about the theft of their assigned vehicle, and what are the reporting obligations in such a scenario?', 'What are the consequences of non-compliance with company policies for a Grade M-7 employee who does not report the theft of their company car in a timely manner, and what are the necessary steps to take in such a situation?', 'These alternative questions can help retrieve relevant documents from a vector database by providing different perspectives on the original question, which can overcome some of the limitations of distance-based similarity search.']
2025-07-10 22:58:56,456 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:58:58,900 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:58:59,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:59:01,051 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:59:01,608 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:59:03,002 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:59:03,405 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:59:04,696 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:59:05,484 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 22:59:06,660 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 22:59:08,511 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 22:59:08,526 - root - INFO - Question processing: 16.35s
2025-07-10 22:59:08,526 - root - INFO - ⏱️ ask_hr took 16.35 seconds
2025-07-10 23:03:34,068 - root - INFO - Received question: What is the required procedure if an employee forgets to mark attendance using their card?
2025-07-10 23:03:34,068 - root - INFO - Chain init: 0.00s
2025-07-10 23:03:36,737 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:03:36,750 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What happens if an employee fails to swipe their attendance card?', 'What are the steps to take when an employee forgets to clock in using their ID card?', 'How does an organization handle instances where employees neglect to record their attendance using their designated cards?', 'These alternative questions can help overcome the limitations of distance-based similarity search by providing different perspectives on the original question, increasing the chances of retrieving relevant documents from the vector database.']
2025-07-10 23:03:39,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:03:41,035 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:03:41,608 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:03:42,776 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:03:43,026 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:03:44,361 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:03:44,562 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:03:45,708 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:03:45,937 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:03:47,131 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:03:48,111 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:03:48,115 - root - INFO - Question processing: 14.05s
2025-07-10 23:03:48,115 - root - INFO - ⏱️ ask_hr took 14.05 seconds
2025-07-10 23:04:36,800 - root - INFO - Received question: How many times per year can an employee take leave on a Friday adjacent to the weekend without losing their Saturday holiday?
2025-07-10 23:04:36,801 - root - INFO - Chain init: 0.00s
2025-07-10 23:04:38,729 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:04:38,744 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What is the maximum number of Friday leaves an employee can take annually without affecting their weekend holidays, specifically Saturdays?', 'How often can an employee take a Friday off before and after a weekend without impacting their Saturday holiday entitlement throughout the year?', "What are the rules governing Friday leaves adjacent to weekends, and how do they impact an employee's Saturday holiday accrual over a 12-month period?"]
2025-07-10 23:04:39,346 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:04:41,131 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:04:41,708 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:04:42,846 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:04:43,412 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:04:44,562 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:04:45,129 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:04:46,246 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:04:47,208 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:04:47,214 - root - INFO - Question processing: 10.41s
2025-07-10 23:04:47,216 - root - INFO - ⏱️ ask_hr took 10.42 seconds
2025-07-10 23:05:50,383 - root - INFO - Received question: Can an employee be terminated without prior warning? If yes, under what conditions?
2025-07-10 23:05:50,383 - root - INFO - Chain init: 0.00s
2025-07-10 23:05:52,569 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:05:52,585 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'Can an employer legally fire an employee without giving them advance notice, and what are the exceptions to this rule?', 'Under what circumstances can an employee be terminated immediately, and are there any laws or regulations that protect employees from sudden dismissal?', 'What are the grounds for immediate termination of employment, and are there any situations in which an employer can fire an employee without providing prior warning or notice?']
2025-07-10 23:05:53,540 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:05:55,225 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:05:55,796 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:05:56,926 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:05:57,498 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:05:58,675 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:05:59,242 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:06:00,442 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:06:01,777 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:06:01,777 - root - INFO - Question processing: 11.39s
2025-07-10 23:06:01,777 - root - INFO - ⏱️ ask_hr took 11.39 seconds
2025-07-10 23:07:20,355 - root - INFO - Received question: What happens to a company car if a Grade M-7 employee leaves before the car reaches maturity? What payment or refund rules apply?
2025-07-10 23:07:20,355 - root - INFO - Chain init: 0.00s
2025-07-10 23:07:21,998 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:07:22,001 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', "What are the company's policies regarding vehicle ownership and reimbursement when an employee in the M-7 grade level terminates their employment before the vehicle reaches its designated lifespan?", 'How do companies handle the disposition of company-provided vehicles when an M-7 level employee departs prematurely, and what are the associated financial implications or refunds for the employee?', 'What are the rules and regulations governing company car ownership and potential refunds or payments when an employee at the M-7 grade level leaves the company before the vehicle has reached its full depreciation or maturity date?']
2025-07-10 23:07:24,578 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:07:27,096 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:07:27,995 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:07:29,131 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:07:29,529 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:07:30,712 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:07:31,099 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:07:32,229 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:07:33,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:07:33,736 - root - INFO - Question processing: 13.38s
2025-07-10 23:07:33,736 - root - INFO - ⏱️ ask_hr took 13.38 seconds
2025-07-10 23:08:35,487 - root - INFO - Received question: What should an employee do if they become aware of another employee violating the company's policy, and what disciplinary outcomes could follow?
2025-07-10 23:08:35,487 - root - INFO - Chain init: 0.00s
2025-07-10 23:08:37,005 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:08:37,021 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the steps an employee should take if they witness a colleague breaching company policy, and what consequences might the violating employee face?', 'How should an employee report and address a policy violation by a fellow worker, and what disciplinary actions could the company take in response?', 'What is the proper protocol for an employee to follow if they discover a coworker is not adhering to company policies, and what penalties or sanctions might be imposed on the non-compliant employee?']
2025-07-10 23:08:39,158 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:08:41,720 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:08:42,746 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:08:43,883 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:08:44,255 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:08:45,570 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:08:45,987 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:08:47,373 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:08:49,311 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:08:49,325 - root - INFO - Question processing: 13.84s
2025-07-10 23:08:49,325 - root - INFO - ⏱️ ask_hr took 13.84 seconds
2025-07-10 23:09:28,595 - root - INFO - Received question: Can a Grade M-7 employee request early ownership of a company car? If so, under what conditions, and how is the payment calculated?
2025-07-10 23:09:28,595 - root - INFO - Chain init: 0.00s
2025-07-10 23:09:30,091 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:09:30,094 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'Can a Grade M-7 staff member be eligible for premature vehicle allocation, and what are the prerequisites and cost implications involved?', 'What are the rules and regulations governing early car ownership for employees at the M-7 level, including any necessary approvals and payment structures?', 'Are there any provisions that allow Grade M-7 personnel to acquire company vehicles ahead of schedule, and if so, what are the associated terms, conditions, and financial arrangements?']
2025-07-10 23:09:32,581 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:09:34,899 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:09:35,481 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:09:36,633 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:09:36,866 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:09:37,999 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:09:38,216 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:09:39,372 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:09:41,001 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:09:41,007 - root - INFO - Question processing: 12.41s
2025-07-10 23:09:41,007 - root - INFO - ⏱️ ask_hr took 12.41 seconds
2025-07-10 23:14:32,480 - root - INFO - Starting HR Assistant...
2025-07-10 23:14:32,647 - root - INFO - Configuration validated successfully
2025-07-10 23:14:32,648 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-10 23:14:32,648 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-10 23:14:32,648 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-10 23:14:32,797 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-10 23:14:36,381 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 23:14:37,818 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:14:41,166 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-10 23:14:41,369 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-10 23:14:41,369 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-10 23:14:41,381 - root - INFO - Loading groq LLM...
2025-07-10 23:14:45,228 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:14:45,235 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-10 23:14:45,255 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-10 23:14:45,255 - root - INFO - HR Assistant started successfully
2025-07-10 23:15:56,289 - root - INFO - Received question: What is the required procedure if an employee forgets to mark attendance using their card?
2025-07-10 23:15:56,289 - root - INFO - Chain init: 0.00s
2025-07-10 23:15:59,384 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:15:59,387 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What happens if an employee fails to swipe their attendance card?', 'What are the steps to take when an employee forgets to clock in using their ID card?', 'How does an organization handle instances where employees neglect to record their attendance using their designated cards?', 'These alternative questions can help overcome the limitations of distance-based similarity search by providing different perspectives on the original question, increasing the chances of retrieving relevant documents from the vector database.']
2025-07-10 23:16:00,503 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:16:02,303 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:16:04,395 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:16:06,086 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:16:07,319 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:16:08,803 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:16:09,719 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:16:11,355 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:16:12,131 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:16:14,135 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:16:16,985 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:16:16,985 - root - INFO - Question processing: 20.70s
2025-07-10 23:16:16,985 - root - INFO - ⏱️ ask_hr took 20.70 seconds
2025-07-10 23:16:51,913 - root - INFO - Received question: How many times per year can an employee take leave on a Friday adjacent 'the weekend without losing their Saturday holiday?
2025-07-10 23:16:51,913 - root - INFO - Chain init: 0.00s
2025-07-10 23:16:54,102 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:16:54,104 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What is the maximum number of Friday leaves an employee can take annually without sacrificing their Saturday holiday?', 'How often can an employee take a Friday off before and after a weekend without impacting their Saturday holiday entitlement?', "What are the rules governing Friday leaves adjacent to weekends, and how do they affect an employee's Saturday holiday accrual?", 'These alternative questions aim to capture different aspects of the original question, such as the frequency of Friday leaves, the impact on Saturday holiday entitlement, and the rules governing Friday leaves adjacent to weekends. By generating multiple perspectives on the user question, we can increase the chances of retrieving relevant documents from the vector database.']
2025-07-10 23:16:56,241 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:16:59,216 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:17:01,197 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:17:03,249 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:17:04,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:17:06,397 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:17:07,468 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:17:09,849 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:17:11,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:17:13,320 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-10 23:17:15,714 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-10 23:17:15,722 - root - INFO - Question processing: 23.81s
2025-07-10 23:17:15,722 - root - INFO - ⏱️ ask_hr took 23.81 seconds
2025-07-10 23:29:18,137 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 23:29:19,707 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 23:29:20,344 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:29:24,317 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 23:29:25,416 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 23:29:30,434 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 23:29:32,440 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 23:29:33,186 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-10 23:29:37,436 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-10 23:29:39,960 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-10 23:29:40,300 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:23:23,535 - root - INFO - Starting HR Assistant...
2025-07-11 02:23:23,698 - root - INFO - Configuration validated successfully
2025-07-11 02:23:23,698 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-11 02:23:23,698 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-11 02:23:23,698 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-11 02:23:23,877 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-11 02:23:28,119 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-11 02:23:30,297 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:23:33,991 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-11 02:23:34,176 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-11 02:23:34,176 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-11 02:23:34,176 - root - INFO - Loading groq LLM...
2025-07-11 02:23:38,854 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:23:38,922 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-11 02:23:38,929 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-11 02:23:38,929 - root - INFO - HR Assistant started successfully
2025-07-11 02:26:15,820 - root - INFO - Starting HR Assistant...
2025-07-11 02:26:15,987 - root - INFO - Configuration validated successfully
2025-07-11 02:26:15,987 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-11 02:26:15,987 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-11 02:26:15,987 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-11 02:26:16,157 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-11 02:26:29,365 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-11 02:26:32,182 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:26:35,219 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-11 02:26:35,393 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-11 02:26:35,395 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-11 02:26:35,395 - root - INFO - Loading groq LLM...
2025-07-11 02:26:40,182 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:26:40,210 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-11 02:26:40,219 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-11 02:26:40,221 - root - INFO - HR Assistant started successfully
2025-07-11 02:27:41,961 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-11 02:27:41,961 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:41,961 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:42,145 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 0 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:42,145 - ingestion_retrieval.ingestion - ERROR - No documents found in directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:42,145 - root - INFO - Successfully ingested documents. Chunks ingested: 0
2025-07-11 02:27:42,145 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:43,399 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-11 02:27:43,402 - root - INFO - Successfully uploaded 3 files to Data
2025-07-11 02:27:44,469 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-11 02:27:44,469 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:44,469 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-11 02:27:57,089 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-11 02:27:57,293 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-11 02:27:57,294 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-11 02:27:57,294 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-11 02:27:57,294 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-11 02:27:57,295 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-11 02:27:57,459 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-11 02:27:57,459 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-11 02:27:57,459 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-11 02:27:57,459 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-11 02:27:57,459 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-11 02:27:57,637 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-11 02:27:57,637 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-11 02:27:57,637 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-11 02:27:57,640 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-11 02:28:01,843 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-11 02:28:04,163 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:28:11,164 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-11 02:28:16,055 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-11 02:28:20,813 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-11 02:28:25,145 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-11 02:28:25,145 - root - INFO - Successfully ingested documents. Chunks ingested: 248
2025-07-11 02:28:25,145 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-11 02:29:07,110 - root - INFO - Received question: What is the required procedure if an employee forgets to mark attendance using their card?
2025-07-11 02:29:07,111 - root - INFO - Chain init: 0.00s
2025-07-11 02:29:08,956 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:29:08,965 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What happens if an employee fails to swipe their attendance card?', 'What are the protocols in place when an employee neglects to record their attendance?', 'How does the company handle instances where an employee forgets to clock in using their attendance card?', 'These alternative questions provide different perspectives on the original question, which can help overcome the limitations of distance-based similarity search by capturing various nuances and phrasings that may be used in relevant documents.']
2025-07-11 02:29:09,576 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:29:12,349 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:29:14,162 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:29:15,363 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:29:15,749 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:29:16,956 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:29:17,380 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:29:18,571 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:29:18,971 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:29:20,107 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:29:23,351 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:29:23,395 - root - INFO - Question processing: 16.28s
2025-07-11 02:29:23,396 - root - INFO - ⏱️ ask_hr took 16.29 seconds
2025-07-11 02:30:37,785 - root - INFO - Received question: How many times per year can an employee take leave on a Friday adjacent to the weekend without losing their Saturday holiday?
2025-07-11 02:30:37,786 - root - INFO - Chain init: 0.00s
2025-07-11 02:30:40,669 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:30:40,669 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What is the maximum number of Friday leaves an employee can take annually without affecting their weekend holidays, specifically Saturdays?', "How often can an employee schedule a Friday off without impacting their Saturday holiday entitlement, and what are the company's policies around this?", "What are the rules and restrictions on taking Fridays off adjacent to weekends, and how do these affect an employee's Saturday holiday accrual and usage throughout the year?"]
2025-07-11 02:30:41,908 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:30:43,219 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:30:43,812 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:30:44,993 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:30:46,850 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:30:49,698 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:30:50,463 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:30:51,642 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:30:52,742 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:30:52,754 - root - INFO - Question processing: 14.97s
2025-07-11 02:30:52,754 - root - INFO - ⏱️ ask_hr took 14.97 seconds
2025-07-11 02:31:43,354 - root - INFO - Received question: Can an employee be terminated without prior warning? If yes, under what conditions?
2025-07-11 02:31:43,355 - root - INFO - Chain init: 0.00s
2025-07-11 02:31:45,644 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:31:45,644 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'Can an employer legally fire an employee without giving them advance notice, and what are the exceptions to this rule?', 'Under what circumstances can an employee be terminated immediately, without prior warning or notice, and what are the legal implications of such an action?', 'What are the grounds for immediate termination of employment without prior warning, and how do labor laws and regulations govern such situations?', 'These alternative questions can help retrieve relevant documents that may not be captured by a simple distance-based similarity search, and provide the user with a more comprehensive understanding of the topic.']
2025-07-11 02:31:48,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:31:50,299 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:31:50,867 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:31:51,981 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:31:52,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:31:53,530 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:31:53,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:31:54,892 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:31:55,088 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:31:56,233 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:31:57,100 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:31:57,105 - root - INFO - Question processing: 13.75s
2025-07-11 02:31:57,106 - root - INFO - ⏱️ ask_hr took 13.75 seconds
2025-07-11 02:32:48,807 - root - INFO - Received question: What happens to a company car if a Grade M-7 employee leaves before the car reaches maturity? What payment or refund rules apply?
2025-07-11 02:32:48,808 - root - INFO - Chain init: 0.00s
2025-07-11 02:32:51,984 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:32:52,002 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', "What are the company's policies regarding vehicle ownership and reimbursement when an employee, specifically a Grade M-7, terminates their employment before the vehicle reaches its designated maturity date?", "How do companies handle the disposition of company-provided vehicles when an employee, particularly those at the M-7 level, leaves the organization prior to the vehicle's maturity, and what are the associated financial implications?", 'What are the rules and regulations governing company car ownership and reimbursement when an M-7 employee departs the company before the vehicle reaches its maturity, and are there any refunds or payments due to the employee or the company in such scenarios?']
2025-07-11 02:32:55,187 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:32:56,836 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:32:57,519 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:32:58,652 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:32:58,854 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:32:59,986 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:33:00,185 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:33:01,340 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:33:02,585 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:33:02,593 - root - INFO - Question processing: 13.79s
2025-07-11 02:33:02,593 - root - INFO - ⏱️ ask_hr took 13.79 seconds
2025-07-11 02:33:59,008 - root - INFO - Received question: What should an employee do if they become aware of another employee violating the company's policy, and what disciplinary outcomes could follow?
2025-07-11 02:33:59,010 - root - INFO - Chain init: 0.00s
2025-07-11 02:34:02,164 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:34:02,182 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the steps an employee should take if they witness a colleague breaching company rules, and what consequences might the violating employee face?', 'How should an employee report and address a policy violation by a coworker, and what disciplinary actions could the company take in response?', 'What is the proper protocol for an employee to follow if they discover a fellow employee is not complying with company policies, and what penalties or sanctions might be imposed as a result?', "These alternative questions offer different perspectives on the original question, which can help overcome the limitations of distance-based similarity search by capturing various aspects of the user's inquiry."]
2025-07-11 02:34:02,764 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:34:04,047 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:34:05,878 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:34:07,782 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:34:08,181 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:34:09,298 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:34:09,665 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:34:10,932 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:34:11,297 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:34:12,434 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:34:13,398 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:34:13,414 - root - INFO - Question processing: 14.40s
2025-07-11 02:34:13,414 - root - INFO - ⏱️ ask_hr took 14.41 seconds
2025-07-11 02:35:03,628 - root - INFO - Received question: Can a Grade M-7 employee request early ownership of a company car? If so, under what conditions, and how is the payment calculated?
2025-07-11 02:35:03,629 - root - INFO - Chain init: 0.00s
2025-07-11 02:35:05,417 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:35:05,418 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'Can a Grade M-7 staff member be eligible for premature vehicle allocation, and what are the prerequisites and cost implications involved?', 'What are the rules and regulations governing early car ownership for employees at the M-7 level, including any necessary approvals and payment structures?', 'Are there any provisions that allow Grade M-7 personnel to acquire company vehicles ahead of schedule, and if so, what are the conditional requirements and financial arrangements that apply?']
2025-07-11 02:35:06,024 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:35:07,394 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:35:07,977 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:35:09,138 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:35:09,742 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:35:10,976 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:35:11,561 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:35:12,689 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-11 02:35:14,143 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:35:14,143 - root - INFO - Question processing: 10.51s
2025-07-11 02:35:14,143 - root - INFO - ⏱️ ask_hr took 10.52 seconds
2025-07-11 02:41:04,090 - root - INFO - Starting HR Assistant...
2025-07-11 02:41:04,268 - root - INFO - Configuration validated successfully
2025-07-11 02:41:04,268 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-11 02:41:04,268 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-11 02:41:04,268 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-11 02:41:04,446 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-11 02:41:07,169 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-11 02:41:07,935 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-11 02:41:09,434 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-11 02:41:09,770 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-11 02:41:09,770 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-11 02:41:09,770 - root - INFO - Loading groq LLM...
2025-07-11 02:41:12,033 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-11 02:41:12,065 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-11 02:41:12,065 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-11 02:41:12,065 - root - INFO - HR Assistant started successfully
2025-07-13 21:52:44,435 - root - INFO - Starting HR Assistant...
2025-07-13 21:52:44,609 - root - INFO - Configuration validated successfully
2025-07-13 21:52:44,609 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:52:44,610 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 21:52:44,611 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 21:52:44,611 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 21:52:44,812 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 21:52:44,812 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 21:52:44,812 - root - INFO - Loading groq LLM...
2025-07-13 21:52:50,117 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 21:52:50,282 - ingestion_retrieval.retrieval - ERROR - Error creating hybrid retriever: 'EmbeddingConfig' object has no attribute 'model_name'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 267, in get_hybrid_retriever
    model=config.embedding.model_name,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmbeddingConfig' object has no attribute 'model_name'
2025-07-13 21:52:50,322 - root - ERROR - Startup failed: 'EmbeddingConfig' object has no attribute 'model_name'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 210, in get_cached_hr_assistant_chain
    return get_hybrid_retriever(qdrant_url, qdrant_api, collection_name, k, alpha)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 267, in get_hybrid_retriever
    model=config.embedding.model_name,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'EmbeddingConfig' object has no attribute 'model_name'
2025-07-13 21:53:01,000 - root - INFO - Starting HR Assistant...
2025-07-13 21:53:01,159 - root - INFO - Configuration validated successfully
2025-07-13 21:53:01,160 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:53:01,160 - root - ERROR - Startup failed: name 'get_cached_hr_assistant_chain' is not defined
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'get_cached_hr_assistant_chain' is not defined
2025-07-13 21:53:11,205 - root - INFO - Starting HR Assistant...
2025-07-13 21:53:11,359 - root - INFO - Configuration validated successfully
2025-07-13 21:53:11,359 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:53:11,359 - root - ERROR - Startup failed: name 'get_cached_hr_assistant_chain' is not defined
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'get_cached_hr_assistant_chain' is not defined
2025-07-13 21:54:18,634 - root - INFO - Starting HR Assistant...
2025-07-13 21:54:18,787 - root - INFO - Configuration validated successfully
2025-07-13 21:54:18,787 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:54:18,787 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 21:54:18,787 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 21:54:18,940 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 21:54:21,453 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 21:54:22,453 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 21:54:24,821 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 21:54:24,971 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 21:54:24,971 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 21:54:24,974 - root - INFO - Loading groq LLM...
2025-07-13 21:54:27,562 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-07-13 21:54:27,562 - ingestion_retrieval.retrieval - WARNING - LLM warm-up failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 311, in get_hr_assistant_chain
    _ = llm.invoke(" ")  # Warm first-token latency
        ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 368, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.AuthenticationError: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
2025-07-13 21:54:27,690 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 21:54:27,690 - root - INFO - HR Assistant started successfully
2025-07-13 21:56:27,367 - root - INFO - Starting HR Assistant...
2025-07-13 21:56:27,512 - root - INFO - Configuration validated successfully
2025-07-13 21:56:27,517 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:56:27,517 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 21:56:27,517 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 21:56:27,669 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 21:56:30,170 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 21:56:31,076 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 21:56:32,926 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 21:56:33,084 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 21:56:33,084 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 21:56:33,084 - root - INFO - Loading groq LLM...
2025-07-13 21:56:35,741 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-07-13 21:56:35,742 - ingestion_retrieval.retrieval - WARNING - LLM warm-up failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 311, in get_hr_assistant_chain
    _ = llm.invoke(" ")  # Warm first-token latency
        ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 368, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.AuthenticationError: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
2025-07-13 21:56:35,753 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 21:56:35,753 - root - INFO - HR Assistant started successfully
2025-07-13 21:56:51,138 - root - INFO - Starting HR Assistant...
2025-07-13 21:56:51,307 - root - INFO - Configuration validated successfully
2025-07-13 21:56:51,307 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:56:51,308 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 21:56:51,308 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 21:56:51,462 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 21:56:53,803 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 21:56:54,628 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 21:56:56,274 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 21:56:56,477 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 21:56:56,477 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 21:56:56,477 - root - INFO - Loading groq LLM...
2025-07-13 21:56:58,986 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-07-13 21:56:58,989 - ingestion_retrieval.retrieval - WARNING - LLM warm-up failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 372, in get_hr_assistant_chain
    _ = llm.invoke(" ")  # Warm first-token latency
        ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 368, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.AuthenticationError: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
2025-07-13 21:56:58,999 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 21:56:59,000 - root - INFO - HR Assistant started successfully
2025-07-13 21:58:39,749 - root - INFO - Starting HR Assistant...
2025-07-13 21:58:39,918 - root - INFO - Configuration validated successfully
2025-07-13 21:58:39,918 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:58:39,918 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 21:58:39,918 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 21:58:39,918 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 21:58:44,535 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 21:58:45,305 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 21:58:45,309 - root - ERROR - Startup failed: Existing Qdrant collection HRDOCS is built with unnamed dense vector. If you want to reuse it, set `vector_name` to ''(empty string).If you want to recreate the collection, set `force_recreate` to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 226, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1060, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1098, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection HRDOCS is built with unnamed dense vector. If you want to reuse it, set `vector_name` to ''(empty string).If you want to recreate the collection, set `force_recreate` to `True`.
2025-07-13 21:59:26,996 - root - INFO - Starting HR Assistant...
2025-07-13 21:59:27,155 - root - INFO - Configuration validated successfully
2025-07-13 21:59:27,155 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 21:59:27,155 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 21:59:27,155 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 21:59:27,159 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 21:59:31,921 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 21:59:33,021 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 21:59:33,300 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 21:59:33,301 - root - ERROR - Startup failed: Existing Qdrant collection HRDOCS is built with unnamed dense vector. If you want to reuse it, set `vector_name` to ''(empty string).If you want to recreate the collection, set `force_recreate` to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 230, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1060, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1098, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection HRDOCS is built with unnamed dense vector. If you want to reuse it, set `vector_name` to ''(empty string).If you want to recreate the collection, set `force_recreate` to `True`.
2025-07-13 22:00:44,022 - root - INFO - Starting HR Assistant...
2025-07-13 22:00:44,174 - root - INFO - Configuration validated successfully
2025-07-13 22:00:44,174 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:00:44,175 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:00:44,175 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:00:44,175 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:00:52,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:00:59,537 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:00:59,543 - root - ERROR - Startup failed: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 230, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
TypeError: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
2025-07-13 22:01:57,612 - root - INFO - Starting HR Assistant...
2025-07-13 22:01:57,773 - root - INFO - Configuration validated successfully
2025-07-13 22:01:57,773 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:01:57,774 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:01:57,775 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:01:57,775 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:02:02,600 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:02:04,092 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:02:04,284 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:02:04,286 - root - ERROR - Startup failed: Existing Qdrant collection HRDOCS is built with unnamed dense vector. If you want to reuse it, set `vector_name` to ''(empty string).If you want to recreate the collection, set `force_recreate` to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 230, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1060, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1098, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection HRDOCS is built with unnamed dense vector. If you want to reuse it, set `vector_name` to ''(empty string).If you want to recreate the collection, set `force_recreate` to `True`.
2025-07-13 22:02:24,663 - root - INFO - Starting HR Assistant...
2025-07-13 22:02:24,871 - root - INFO - Configuration validated successfully
2025-07-13 22:02:24,871 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:02:24,874 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:02:24,874 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:02:24,874 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:02:30,060 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:02:31,378 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:02:31,378 - root - ERROR - Startup failed: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.531e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 226, in get_hr_assistant_chain
    collection_info = _qdrant_client.get_collection(collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.531e-6}'
2025-07-13 22:04:16,635 - root - INFO - Starting HR Assistant...
2025-07-13 22:04:16,792 - root - INFO - Configuration validated successfully
2025-07-13 22:04:16,794 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:04:16,794 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:04:16,794 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:04:16,794 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:04:21,020 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:04:21,842 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:04:21,846 - root - ERROR - Startup failed: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.655e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 227, in get_hr_assistant_chain
    collection_info = _qdrant_client.get_collection(collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.655e-6}'
2025-07-13 22:06:32,085 - root - INFO - Starting HR Assistant...
2025-07-13 22:06:32,250 - root - INFO - Configuration validated successfully
2025-07-13 22:06:32,250 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:06:32,252 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:06:32,252 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:06:32,252 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:06:37,761 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:06:38,544 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:06:39,159 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:06:39,162 - root - ERROR - Startup failed: Existing Qdrant collection HRDOCS does not contain dense vector named dense. Did you mean one of the existing vectors: ? If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 252, in get_hr_assistant_chain
    _vector_store = QdrantVectorStore(
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1060, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1082, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection HRDOCS does not contain dense vector named dense. Did you mean one of the existing vectors: ? If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-13 22:14:47,427 - root - INFO - Starting HR Assistant...
2025-07-13 22:14:47,588 - root - INFO - Configuration validated successfully
2025-07-13 22:14:47,588 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:14:47,589 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:14:47,589 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:14:47,589 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:14:53,041 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:14:53,634 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:14:53,896 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:14:56,423 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:14:56,593 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:14:56,600 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:14:56,782 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:14:56,782 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:14:56,783 - root - INFO - Loading groq LLM...
2025-07-13 22:14:58,986 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-07-13 22:14:58,989 - ingestion_retrieval.retrieval - WARNING - LLM warm-up failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 377, in get_hr_assistant_chain
    _ = llm.invoke(" ")  # Warm first-token latency
        ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 368, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.AuthenticationError: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
2025-07-13 22:14:59,111 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:14:59,111 - root - INFO - HR Assistant started successfully
2025-07-13 22:16:55,298 - root - INFO - Starting HR Assistant...
2025-07-13 22:16:55,447 - root - INFO - Configuration validated successfully
2025-07-13 22:16:55,447 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:16:55,447 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:16:55,447 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:16:55,447 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:17:00,439 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:17:01,508 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:17:01,920 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:17:03,255 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:17:03,419 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:17:03,423 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:17:03,581 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:17:03,581 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:17:03,581 - ingestion_retrieval.retrieval - INFO - Attempting to initialize groq LLM...
2025-07-13 22:17:06,599 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-07-13 22:17:06,603 - ingestion_retrieval.retrieval - WARNING - Failed to initialize Groq LLM, falling back to Google: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
2025-07-13 22:17:06,603 - ingestion_retrieval.retrieval - ERROR - Critical error initializing LLM: Failed to initialize any LLM provider
2025-07-13 22:17:06,605 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Failed to initialize any LLM provider
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 390, in get_hr_assistant_chain
    # 5. Create QA Chain with optimized prompt
              ^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 176, in load_llm
RuntimeError: Failed to initialize any LLM provider
2025-07-13 22:17:06,644 - root - ERROR - Startup failed: Failed to initialize any LLM provider
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 216, in get_cached_hr_assistant_chain
    google_api_key=config.embedding.google_api_key
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 390, in get_hr_assistant_chain
    # 5. Create QA Chain with optimized prompt
              ^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 176, in load_llm
RuntimeError: Failed to initialize any LLM provider
2025-07-13 22:17:19,946 - root - INFO - Starting HR Assistant...
2025-07-13 22:17:20,105 - root - INFO - Configuration validated successfully
2025-07-13 22:17:20,107 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:17:20,107 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:17:20,107 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:17:20,107 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:17:24,566 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:17:25,178 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:17:25,360 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:17:27,778 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:17:27,933 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:17:27,940 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:17:28,092 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:17:28,092 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:17:28,095 - root - INFO - Loading groq LLM...
2025-07-13 22:17:30,082 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-07-13 22:17:30,084 - ingestion_retrieval.retrieval - WARNING - LLM warm-up failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 377, in get_hr_assistant_chain
    _ = llm.invoke(" ")  # Warm first-token latency
        ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 378, in invoke
    self.generate_prompt(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 963, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 782, in generate
    self._generate_with_cache(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1028, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_groq\chat_models.py", line 536, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\resources\chat\completions.py", line 368, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1232, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\groq\_base_client.py", line 1034, in request
    raise self._make_status_error_from_response(err.response) from None
groq.AuthenticationError: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}
2025-07-13 22:17:30,095 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:17:30,095 - root - INFO - HR Assistant started successfully
2025-07-13 22:19:19,082 - root - INFO - Starting HR Assistant...
2025-07-13 22:19:19,229 - root - INFO - Configuration validated successfully
2025-07-13 22:19:19,230 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:19:19,230 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:19:19,231 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:19:19,232 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:19:24,951 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:19:25,932 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:19:26,114 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:19:28,579 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:19:28,743 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:19:28,753 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:19:28,914 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:19:28,914 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:19:28,915 - root - INFO - Loading groq LLM...
2025-07-13 22:19:31,556 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:19:31,624 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:19:31,629 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:19:31,629 - root - INFO - HR Assistant started successfully
2025-07-13 22:20:00,138 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:20:00,139 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:20:01,109 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:20:01,109 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:20:01,355 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:20:01,356 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:20:01,366 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:20:01,366 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:20:01,366 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:20:01,366 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:20:14,718 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-13 22:20:14,729 - root - INFO - Successfully uploaded 3 files to Data
2025-07-13 22:20:21,011 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:20:21,011 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:20:21,014 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:20:31,755 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:20:31,757 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:20:31,757 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:20:31,758 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:20:31,758 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:20:31,759 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:20:31,759 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:20:31,760 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:20:31,760 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:20:31,760 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:20:31,760 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:20:31,760 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:20:31,760 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:20:31,763 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:20:31,920 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:20:31,920 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-13 22:20:31,920 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:20:31,924 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:20:31,925 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:20:32,076 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:20:32,077 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-13 22:20:32,077 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:20:32,077 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:20:32,077 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:20:32,231 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:20:32,232 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-13 22:20:32,232 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-13 22:20:32,232 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:20:34,943 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:20:37,465 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:20:37,467 - ingestion_retrieval.ingestion - INFO - Using existing collection: HRDOCS
2025-07-13 22:20:37,765 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:20:37,805 - root - ERROR - Error during document ingestion: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 331, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 228, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 219, in sync_ingest
    vectordatabase = QdrantVectorStore(
                     ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1082, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-13 22:20:37,842 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-13 22:22:36,341 - root - INFO - Starting HR Assistant...
2025-07-13 22:22:36,481 - root - INFO - Configuration validated successfully
2025-07-13 22:22:36,481 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:22:36,481 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:22:36,481 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:22:36,486 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:22:41,446 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:22:43,035 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:22:43,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:22:45,778 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:22:45,916 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:22:45,925 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:22:46,069 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:22:46,069 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:22:46,069 - root - INFO - Loading groq LLM...
2025-07-13 22:22:48,625 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:22:48,656 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:22:48,663 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:22:48,665 - root - INFO - HR Assistant started successfully
2025-07-13 22:23:13,285 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:23:13,286 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:23:13,286 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:23:20,733 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:23:20,733 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:23:20,733 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:23:20,735 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:23:20,735 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:23:20,735 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:23:20,735 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:23:20,737 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:23:20,737 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:23:20,737 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:23:20,737 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:23:20,737 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:23:20,737 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:23:20,737 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:23:20,904 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:23:20,906 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-13 22:23:20,906 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:23:20,906 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:23:20,906 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:23:21,071 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:23:21,073 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-13 22:23:21,073 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:23:21,073 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:23:21,073 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:23:21,225 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:23:21,225 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-13 22:23:21,225 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-13 22:23:21,225 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:23:23,408 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:23:27,615 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:23:27,617 - ingestion_retrieval.ingestion - INFO - Using existing collection HRDOCS with vector_name=''
2025-07-13 22:23:27,619 - root - ERROR - Error during document ingestion: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 331, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 270, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 253, in sync_ingest
    vectordatabase = QdrantVectorStore(
                     ^^^^^^^^^^^^^^^^^^
TypeError: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
2025-07-13 22:23:27,622 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
2025-07-13 22:23:57,964 - root - INFO - Starting HR Assistant...
2025-07-13 22:23:58,147 - root - INFO - Configuration validated successfully
2025-07-13 22:23:58,148 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:23:58,148 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:23:58,148 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:23:58,149 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:24:04,137 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:24:05,147 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:24:05,342 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:24:08,008 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:24:08,164 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:24:08,172 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:24:08,328 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:24:08,328 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:24:08,328 - root - INFO - Loading groq LLM...
2025-07-13 22:24:11,434 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:24:11,456 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:24:11,461 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:24:11,461 - root - INFO - HR Assistant started successfully
2025-07-13 22:25:06,162 - root - INFO - Starting HR Assistant...
2025-07-13 22:25:06,322 - root - INFO - Configuration validated successfully
2025-07-13 22:25:06,322 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:25:06,322 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:25:06,322 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:25:06,322 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:25:11,815 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:25:12,703 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:25:13,034 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:25:15,088 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:25:15,248 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:25:15,255 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:25:15,446 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:25:15,448 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:25:15,448 - root - INFO - Loading groq LLM...
2025-07-13 22:25:25,299 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:25:25,321 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:25:25,327 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:25:25,329 - root - INFO - HR Assistant started successfully
2025-07-13 22:25:37,575 - root - INFO - Starting HR Assistant...
2025-07-13 22:25:37,725 - root - INFO - Configuration validated successfully
2025-07-13 22:25:37,726 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:25:37,726 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:25:37,726 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:25:37,726 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:25:44,651 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:25:45,613 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:25:46,039 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:25:48,339 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:25:48,498 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:25:48,506 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:25:48,660 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:25:48,663 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:25:48,663 - root - INFO - Loading groq LLM...
2025-07-13 22:25:50,948 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:25:50,969 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:25:50,974 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:25:50,974 - root - INFO - HR Assistant started successfully
2025-07-13 22:26:10,064 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:26:11,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:26:11,479 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:26:14,752 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:26:15,687 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:26:20,629 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:26:21,387 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:27:27,585 - root - INFO - Starting HR Assistant...
2025-07-13 22:27:27,787 - root - INFO - Configuration validated successfully
2025-07-13 22:27:27,787 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:27:27,791 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:27:27,792 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:27:27,792 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:27:32,625 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:27:33,984 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:27:33,984 - root - ERROR - Startup failed: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.802e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 227, in get_hr_assistant_chain
    collection_info = _qdrant_client.get_collection(collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.802e-6}'
2025-07-13 22:28:30,643 - root - INFO - Starting HR Assistant...
2025-07-13 22:28:30,801 - root - INFO - Configuration validated successfully
2025-07-13 22:28:30,801 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:28:30,801 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:28:30,801 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:28:30,801 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:28:36,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:28:36,956 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:28:37,166 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:28:39,913 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:28:40,087 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:28:40,094 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:28:40,254 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:28:40,254 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:28:40,254 - root - INFO - Loading groq LLM...
2025-07-13 22:28:43,220 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:28:43,239 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:28:43,244 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:28:43,246 - root - INFO - HR Assistant started successfully
2025-07-13 22:28:43,342 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:28:43,344 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:28:43,344 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:28:43,346 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:28:45,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:28:45,928 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:28:46,387 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:28:46,681 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:28:46,681 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:28:46,861 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:28:54,887 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:28:54,887 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:28:54,889 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:29:05,430 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:29:05,430 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:29:05,430 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:29:05,433 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:29:05,433 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:29:05,434 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:29:05,434 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:29:05,434 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:29:05,435 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:29:05,435 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:29:05,436 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:29:05,436 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:29:05,436 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:29:05,436 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:29:05,601 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:29:05,601 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-13 22:29:05,601 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:29:05,601 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:29:05,605 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:29:05,764 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:29:05,765 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-13 22:29:05,765 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:29:05,766 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:29:05,766 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:29:05,924 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:29:05,925 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-13 22:29:05,926 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-13 22:29:05,926 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:29:08,157 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:29:12,163 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:29:12,165 - ingestion_retrieval.ingestion - INFO - Using existing collection HRDOCS with vector_name=''
2025-07-13 22:29:12,165 - root - ERROR - Error during document ingestion: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 331, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 270, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 253, in sync_ingest
    vectordatabase = QdrantVectorStore(
                     ^^^^^^^^^^^^^^^^^^
TypeError: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
2025-07-13 22:29:12,165 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: QdrantVectorStore.__init__() got an unexpected keyword argument 'force_recreate'
2025-07-13 22:29:57,964 - root - INFO - Starting HR Assistant...
2025-07-13 22:29:58,122 - root - INFO - Configuration validated successfully
2025-07-13 22:29:58,126 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:29:58,127 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:29:58,127 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:29:58,127 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:30:04,191 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:30:04,882 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:30:05,181 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:30:07,650 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:30:07,813 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:30:07,845 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:30:08,003 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:30:08,003 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:30:08,006 - root - INFO - Loading groq LLM...
2025-07-13 22:30:10,870 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:30:10,889 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:30:10,895 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:30:10,898 - root - INFO - HR Assistant started successfully
2025-07-13 22:30:52,894 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:30:52,895 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:30:52,895 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:31:00,506 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:31:00,506 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:31:00,506 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:31:00,506 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:31:00,509 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:31:00,679 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:31:00,680 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-13 22:31:00,680 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:31:00,681 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:31:00,681 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:31:00,838 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:31:00,838 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-13 22:31:00,838 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:31:00,838 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:31:00,840 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:31:00,987 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:31:00,987 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-13 22:31:00,989 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-13 22:31:00,989 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:31:03,359 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:31:07,296 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:31:07,296 - ingestion_retrieval.ingestion - INFO - Using existing collection HRDOCS with vector_name=''
2025-07-13 22:31:07,833 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:31:07,835 - root - ERROR - Error during document ingestion: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 331, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 269, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 253, in sync_ingest
    vectordatabase = QdrantVectorStore(
                     ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1082, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-13 22:31:07,835 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-13 22:31:26,356 - root - INFO - Collection management action: delete
2025-07-13 22:31:26,356 - root - INFO - Deleting collection: HRDOCS
2025-07-13 22:31:28,478 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:31:29,355 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:31:29,643 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:31:33,380 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:31:34,302 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:31:36,855 - root - INFO - Collection management action: create
2025-07-13 22:31:36,855 - root - INFO - Creating collection: HRDOCS
2025-07-13 22:31:38,681 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:31:39,487 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:31:39,492 - root - ERROR - Error in collection management: Error checking/creating collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.482e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 300, in check_or_create_qdrant_collection
    client.get_collection(collection_name)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.482e-6}'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 370, in collection_manage
    result = check_or_create_qdrant_collection(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 325, in check_or_create_qdrant_collection
    raise Exception(f"Error checking/creating collection: {str(e)}")
Exception: Error checking/creating collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.482e-6}'
2025-07-13 22:32:05,573 - root - INFO - Collection management action: create
2025-07-13 22:32:05,575 - root - INFO - Creating collection: HRDOCS
2025-07-13 22:32:07,927 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:32:08,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:32:08,790 - root - ERROR - Error in collection management: Error checking/creating collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.802e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 300, in check_or_create_qdrant_collection
    client.get_collection(collection_name)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.802e-6}'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 370, in collection_manage
    result = check_or_create_qdrant_collection(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 325, in check_or_create_qdrant_collection
    raise Exception(f"Error checking/creating collection: {str(e)}")
Exception: Error checking/creating collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.802e-6}'
2025-07-13 22:33:03,047 - root - INFO - Starting HR Assistant...
2025-07-13 22:33:03,199 - root - INFO - Configuration validated successfully
2025-07-13 22:33:03,199 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:33:03,202 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:33:03,202 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:33:03,202 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:33:11,292 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:33:12,557 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:33:12,561 - root - ERROR - Startup failed: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.491e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 197, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 227, in get_hr_assistant_chain
    collection_info = _qdrant_client.get_collection(collection_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.491e-6}'
2025-07-13 22:34:21,465 - root - INFO - Starting HR Assistant...
2025-07-13 22:34:21,620 - root - INFO - Configuration validated successfully
2025-07-13 22:34:21,620 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:34:21,621 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:34:21,622 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:34:21,622 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:34:26,967 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:34:27,817 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:34:28,137 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:34:30,374 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:34:30,528 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:34:30,536 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:34:30,691 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:34:30,691 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:34:30,691 - root - INFO - Loading groq LLM...
2025-07-13 22:34:33,862 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:34:33,890 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:34:33,895 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:34:33,896 - root - INFO - HR Assistant started successfully
2025-07-13 22:34:49,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:34:49,320 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:34:49,991 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:34:49,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:34:50,170 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:34:50,179 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:34:50,179 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:34:50,179 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:34:50,179 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:34:50,306 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:34:57,511 - root - INFO - Collection management action: create
2025-07-13 22:34:57,512 - root - INFO - Creating collection: HRDOCS
2025-07-13 22:34:59,362 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:35:00,417 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:35:00,419 - root - INFO - Collection operation result: Collection 'HRDOCS' already exists
2025-07-13 22:35:03,962 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:35:04,682 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:35:04,931 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:35:23,767 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:35:23,767 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:35:23,767 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:35:33,166 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:35:33,166 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:35:33,166 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:35:33,166 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:35:33,166 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:35:33,166 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:35:33,166 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:35:33,172 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:35:33,172 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:35:33,173 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:35:33,173 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:35:33,173 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:35:33,174 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:35:33,175 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:35:33,335 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:35:33,337 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-13 22:35:33,337 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:35:33,337 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:35:33,337 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:35:33,499 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:35:33,502 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-13 22:35:33,502 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:35:33,502 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:35:33,502 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:35:33,659 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:35:33,659 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-13 22:35:33,659 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-13 22:35:33,659 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:35:37,460 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:35:41,162 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:35:41,165 - ingestion_retrieval.ingestion - INFO - Using existing collection HRDOCS with vector_name=''
2025-07-13 22:35:41,508 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:35:41,512 - root - ERROR - Error during document ingestion: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 331, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 269, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 253, in sync_ingest
    vectordatabase = QdrantVectorStore(
                     ^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 213, in __init__
    self._validate_collection_config(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1050, in _validate_collection_config
    cls._validate_collection_for_dense(
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain_qdrant\qdrant.py", line 1082, in _validate_collection_for_dense
    raise QdrantVectorStoreError(
langchain_qdrant.qdrant.QdrantVectorStoreError: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-13 22:35:41,589 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: Existing Qdrant collection HRDOCS does not contain dense vector named . Did you mean one of the existing vectors: dense? If you want to recreate the collection, set `force_recreate` parameter to `True`.
2025-07-13 22:37:09,497 - root - INFO - Starting HR Assistant...
2025-07-13 22:37:09,653 - root - INFO - Configuration validated successfully
2025-07-13 22:37:09,653 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:37:09,653 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:37:09,653 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:37:09,653 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:37:13,763 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:37:14,894 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:37:15,138 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:37:17,580 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:37:17,748 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:37:17,754 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:37:17,912 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:37:17,944 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:37:17,945 - root - INFO - Loading groq LLM...
2025-07-13 22:37:20,014 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:37:20,037 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:37:20,043 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:37:20,043 - root - INFO - HR Assistant started successfully
2025-07-13 22:38:08,704 - root - INFO - Starting HR Assistant...
2025-07-13 22:38:08,860 - root - INFO - Configuration validated successfully
2025-07-13 22:38:08,861 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:38:08,861 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:38:08,861 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:38:08,861 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:38:12,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:38:13,583 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:13,913 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:15,185 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:15,355 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:38:15,363 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:38:15,515 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:38:15,515 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:38:15,515 - root - INFO - Loading groq LLM...
2025-07-13 22:38:18,033 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:38:18,054 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:38:18,059 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:38:18,059 - root - INFO - HR Assistant started successfully
2025-07-13 22:38:29,408 - root - INFO - Starting HR Assistant...
2025-07-13 22:38:29,564 - root - INFO - Configuration validated successfully
2025-07-13 22:38:29,566 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:38:29,566 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:38:29,566 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:38:29,566 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:38:34,067 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:38:34,810 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:35,038 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:36,517 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:36,677 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:38:36,685 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:38:36,862 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:38:36,867 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:38:36,867 - root - INFO - Loading groq LLM...
2025-07-13 22:38:38,916 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:38:38,934 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:38:38,940 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:38:38,940 - root - INFO - HR Assistant started successfully
2025-07-13 22:38:52,916 - root - INFO - Starting HR Assistant...
2025-07-13 22:38:53,075 - root - INFO - Configuration validated successfully
2025-07-13 22:38:53,075 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:38:53,076 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:38:53,076 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:38:53,076 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:38:56,829 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:38:57,372 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:57,555 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:58,366 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:38:58,520 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:38:58,525 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:38:58,675 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:38:58,676 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:38:58,676 - root - INFO - Loading groq LLM...
2025-07-13 22:39:00,773 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:39:00,796 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:39:00,796 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:39:00,796 - root - INFO - HR Assistant started successfully
2025-07-13 22:39:20,828 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:39:20,830 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:39:21,811 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:39:21,813 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:39:22,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:39:22,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:39:22,175 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:39:22,176 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:39:22,177 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:39:22,178 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:39:32,087 - root - INFO - Collection management action: delete
2025-07-13 22:39:32,088 - root - INFO - Deleting collection: HRDOCS
2025-07-13 22:39:34,179 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:39:35,179 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:39:35,500 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:39:38,256 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:39:39,260 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:39:40,525 - root - INFO - Collection management action: create
2025-07-13 22:39:40,525 - root - INFO - Creating collection: HRDOCS
2025-07-13 22:39:42,266 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:39:43,330 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:39:43,330 - root - ERROR - Error in collection management: name 'logger' is not defined
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 328, in check_or_create_qdrant_collection
    client.get_collection(collection_name=collection_name)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.482e-6}'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 367, in check_or_create_qdrant_collection
    logger.error(f"Unexpected error checking collection: {e}")
    ^^^^^^
NameError: name 'logger' is not defined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 370, in collection_manage
    result = check_or_create_qdrant_collection(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 376, in check_or_create_qdrant_collection
    logger.error(error_msg)
    ^^^^^^
NameError: name 'logger' is not defined
2025-07-13 22:40:23,552 - root - INFO - Starting HR Assistant...
2025-07-13 22:40:23,724 - root - INFO - Configuration validated successfully
2025-07-13 22:40:23,724 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:40:23,724 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:40:23,724 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:40:23,724 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:40:27,768 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:40:28,450 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:40:28,862 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:40:30,666 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:40:30,852 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:40:30,861 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:40:31,025 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:40:31,025 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:40:31,028 - root - INFO - Loading groq LLM...
2025-07-13 22:40:33,067 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:40:33,087 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:40:33,092 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:40:33,093 - root - INFO - HR Assistant started successfully
2025-07-13 22:40:43,561 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:40:43,564 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:40:44,700 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:40:44,704 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:40:44,886 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:40:44,886 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:40:44,895 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:40:44,895 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:40:44,895 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:40:44,898 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:40:48,776 - root - INFO - Collection management action: delete
2025-07-13 22:40:48,777 - root - INFO - Deleting collection: HRDOCS
2025-07-13 22:40:50,650 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:40:51,478 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:40:51,685 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:40:54,532 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:40:55,515 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:40:56,861 - root - INFO - Collection management action: create
2025-07-13 22:40:56,861 - root - INFO - Creating collection: HRDOCS
2025-07-13 22:40:58,509 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:40:59,567 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 404 Not Found"
2025-07-13 22:40:59,567 - ingestion_retrieval.ingestion - ERROR - Unexpected error checking collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.236e-6}'
2025-07-13 22:40:59,571 - ingestion_retrieval.ingestion - ERROR - Error in check_or_create_qdrant_collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.236e-6}'
2025-07-13 22:40:59,571 - root - ERROR - Error in collection management: Error in check_or_create_qdrant_collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.236e-6}'
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 331, in check_or_create_qdrant_collection
    client.get_collection(collection_name=collection_name)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_client.py", line 2223, in get_collection
    return self._client.get_collection(collection_name=collection_name, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\qdrant_remote.py", line 2584, in get_collection
    result: Optional[types.CollectionInfo] = self.http.collections_api.get_collection(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 320, in get_collection
    return self._build_for_get_collection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api\collections_api.py", line 144, in _build_for_get_collection
    return self.api_client.request(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 95, in request
    return self.send(request, type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\qdrant_client\http\api_client.py", line 130, in send
    raise UnexpectedResponse.for_response(response)
qdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.236e-6}'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 370, in collection_manage
    result = check_or_create_qdrant_collection(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 380, in check_or_create_qdrant_collection
    raise Exception(error_msg)
Exception: Error in check_or_create_qdrant_collection: Unexpected Response: 404 (Not Found)
Raw response content:
b'{"status":{"error":"Not found: Collection `HRDOCS` doesn\'t exist!"},"time":6.236e-6}'
2025-07-13 22:42:28,890 - root - INFO - Starting HR Assistant...
2025-07-13 22:42:29,023 - root - INFO - Configuration validated successfully
2025-07-13 22:42:29,024 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:42:29,024 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:42:29,025 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:42:29,025 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:42:32,425 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:42:33,438 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:42:33,666 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:42:34,834 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:42:34,968 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:42:34,973 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:42:35,104 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:42:35,104 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:42:35,104 - root - INFO - Loading groq LLM...
2025-07-13 22:42:37,615 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:42:37,635 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:42:37,642 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:42:37,642 - root - INFO - HR Assistant started successfully
2025-07-13 22:42:45,403 - root - INFO - Collection management action: delete
2025-07-13 22:42:45,403 - root - INFO - Deleting collection: HRDOCS
2025-07-13 22:42:47,435 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:42:48,683 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:42:48,900 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:42:51,556 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:42:52,348 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:42:55,585 - root - INFO - Collection management action: create
2025-07-13 22:42:55,585 - root - INFO - Creating collection: HRDOCS
2025-07-13 22:42:57,797 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:42:58,864 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:42:58,865 - ingestion_retrieval.ingestion - INFO - Deleted existing collection: HRDOCS
2025-07-13 22:42:59,352 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:42:59,355 - ingestion_retrieval.ingestion - INFO - Successfully created collection: HRDOCS
2025-07-13 22:42:59,355 - root - INFO - Collection operation result: Created new collection 'HRDOCS' with dense and sparse vectors
2025-07-13 22:43:03,013 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:43:03,669 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:43:03,873 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:43:05,779 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:43:05,780 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:43:05,780 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:43:13,146 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:43:13,147 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:43:13,148 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:43:13,148 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:43:13,148 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:43:13,149 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:43:13,149 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:43:13,150 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:43:13,150 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:43:13,151 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:43:13,151 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:43:13,151 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:43:13,152 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:43:13,152 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:43:13,308 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:43:13,308 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-13 22:43:13,308 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:43:13,310 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:43:13,310 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:43:13,458 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:43:13,460 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-13 22:43:13,460 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:43:13,460 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:43:13,460 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:43:13,610 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:43:13,610 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-13 22:43:13,610 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-13 22:43:13,610 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:43:15,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:43:17,962 - root - ERROR - Error during document ingestion: name 'recreate_collection' is not defined
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 331, in _process_ingestion
    result = await ingest_documents_to_qdrant_async(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 289, in ingest_documents_to_qdrant_async
    return await asyncio.get_event_loop().run_in_executor(executor, sync_ingest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\ingestion.py", line 248, in sync_ingest
    if recreate_collection:
       ^^^^^^^^^^^^^^^^^^^
NameError: name 'recreate_collection' is not defined
2025-07-13 22:43:17,973 - root - WARNING - HTTPException in ingest_api: Error during document ingestion: name 'recreate_collection' is not defined
2025-07-13 22:44:39,675 - root - INFO - Starting HR Assistant...
2025-07-13 22:44:39,839 - root - INFO - Configuration validated successfully
2025-07-13 22:44:39,840 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:44:39,841 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:44:39,841 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:44:39,841 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:44:44,945 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:44:45,936 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:44:46,260 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:44:47,838 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:44:47,999 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:44:48,007 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:44:48,160 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:44:48,161 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:44:48,161 - root - INFO - Loading groq LLM...
2025-07-13 22:44:50,636 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:44:50,660 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:44:50,664 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:44:50,664 - root - INFO - HR Assistant started successfully
2025-07-13 22:44:56,191 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:44:56,191 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:44:56,191 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:45:04,097 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:45:04,100 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:45:04,100 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:45:04,100 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:45:04,102 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:45:04,102 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:45:04,102 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:45:04,103 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:45:04,103 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:45:04,103 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:45:04,103 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:45:04,103 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:45:04,105 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:45:04,105 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:45:04,262 - ingestion_retrieval.ingestion - INFO - Generated 120 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:45:04,262 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 120
2025-07-13 22:45:04,262 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:45:04,262 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:45:04,262 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:45:04,418 - ingestion_retrieval.ingestion - INFO - Generated 80 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:45:04,418 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 80
2025-07-13 22:45:04,418 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:45:04,420 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:45:04,420 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:45:04,570 - ingestion_retrieval.ingestion - INFO - Generated 48 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:45:04,570 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 48
2025-07-13 22:45:04,570 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 248
2025-07-13 22:45:04,570 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:45:06,780 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:45:08,747 - ingestion_retrieval.ingestion - INFO - Initializing QdrantVectorStore with vector_name='dense', sparse_vector_name='sparse'
2025-07-13 22:45:09,570 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:11,329 - ingestion_retrieval.ingestion - INFO - Adding 248 chunks to the vector store...
2025-07-13 22:45:14,461 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-13 22:45:16,634 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-13 22:45:18,986 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-13 22:45:20,881 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-13 22:45:20,884 - ingestion_retrieval.ingestion - INFO - Successfully added documents to the vector store
2025-07-13 22:45:20,885 - root - INFO - Successfully ingested documents. Chunks ingested: 248
2025-07-13 22:45:20,888 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:45:31,837 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:45:31,968 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:45:32,622 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:45:33,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:33,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:45:33,018 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:45:33,018 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:45:33,018 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:45:33,018 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:45:33,300 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:44,221 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-13 22:45:44,223 - root - INFO - Chain init: 0.00s
2025-07-13 22:45:45,516 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:45:45,521 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', "What are the company's policies regarding employee leave of absence?", "Can I find information on the M7 company's vacation and time-off policies?", 'What are the rules and regulations surrounding employee leave at M7 corporation?', 'These alternative questions can help retrieve relevant documents from a vector database by providing different perspectives on the original question, which can overcome some of the limitations of distance-based similarity search.']
2025-07-13 22:45:46,548 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:48,121 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:45:48,780 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:50,514 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:45:51,253 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:52,407 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:45:52,965 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:54,225 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:45:54,943 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:56,299 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:45:56,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:45:57,842 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:45:58,924 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:45:58,965 - root - INFO - Question processing: 14.74s
2025-07-13 22:45:58,965 - root - INFO - ⏱️ ask_hr took 14.74 seconds
2025-07-13 22:46:50,298 - root - INFO - Received question: What are the monthly car fuel consumption limits for employees of QADRI-Group by grade, and who bears the fuel cost for official and personal use within the city?
2025-07-13 22:46:50,298 - root - INFO - Chain init: 0.00s
2025-07-13 22:46:51,426 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:46:51,430 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'What are the fuel expense policies for QADRI-Group employees, including monthly limits and cost allocation for official and personal vehicle use within the city, categorized by job grade?', "Can you provide the QADRI-Group's vehicle fuel reimbursement guidelines, including monthly consumption limits, cost-sharing arrangements, and eligibility criteria for employees of different grades, specifically for official and personal use within urban areas?", "What are the QADRI-Group's policies and procedures for employee vehicle fuel expenses, including monthly allowances, cost responsibility, and reimbursement processes for official and personal use within the city, differentiated by employee grade level?"]
2025-07-13 22:46:52,603 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:46:54,224 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:46:54,882 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:46:56,009 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:46:56,554 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:46:57,727 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:46:58,305 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:46:59,494 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:47:00,059 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:47:01,400 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:47:02,686 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:47:02,692 - root - INFO - Question processing: 12.39s
2025-07-13 22:47:02,692 - root - INFO - ⏱️ ask_hr took 12.39 seconds
2025-07-13 22:48:12,911 - root - INFO - Received question: LEAVE ALLOWED PER YEAR TO M7 EMPLOY
2025-07-13 22:48:12,911 - root - INFO - Chain init: 0.00s
2025-07-13 22:48:14,618 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:48:14,619 - langchain.retrievers.multi_query - INFO - Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'How many paid leaves are M7 employees entitled to annually?', 'What is the annual leave policy for M7 staff members?', 'What is the maximum number of days of leave allowed for M7 employees per year?', 'These alternative questions can help overcome the limitations of distance-based similarity search by providing different perspectives on the original question, increasing the chances of retrieving relevant documents from the vector database.']
2025-07-13 22:48:15,953 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:48:17,603 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:48:18,274 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:48:19,602 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:48:19,989 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:48:21,190 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:48:21,584 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:48:22,761 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:48:23,170 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:48:24,306 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:48:24,699 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:48:25,835 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:48:26,956 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:48:26,963 - root - INFO - Question processing: 14.05s
2025-07-13 22:48:26,963 - root - INFO - ⏱️ ask_hr took 14.05 seconds
2025-07-13 22:49:23,845 - root - INFO - Starting HR Assistant...
2025-07-13 22:49:23,973 - root - INFO - Configuration validated successfully
2025-07-13 22:49:23,973 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:49:23,977 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:49:23,977 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:49:23,977 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:49:28,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:49:28,818 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:49:29,005 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:49:30,751 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:49:30,911 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:49:30,919 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:49:31,135 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:49:31,135 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:49:31,135 - root - INFO - Loading groq LLM...
2025-07-13 22:49:33,647 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:49:33,683 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:49:33,687 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:49:33,687 - root - INFO - HR Assistant started successfully
2025-07-13 22:50:23,755 - root - INFO - Received question: LEAVE ALLOWED PER YEAR TO M7 EMPLOY
2025-07-13 22:50:23,763 - root - INFO - Chain init: 0.00s
2025-07-13 22:50:25,049 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:50:26,803 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:50:28,351 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:50:28,373 - root - INFO - Question processing: 4.61s
2025-07-13 22:50:28,373 - root - INFO - ⏱️ ask_hr took 4.62 seconds
2025-07-13 22:51:04,708 - root - INFO - Received question: m7  leave policy
2025-07-13 22:51:04,708 - root - INFO - Chain init: 0.00s
2025-07-13 22:51:05,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:51:07,009 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:51:08,172 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:51:08,177 - root - INFO - Question processing: 3.47s
2025-07-13 22:51:08,178 - root - INFO - ⏱️ ask_hr took 3.47 seconds
2025-07-13 22:51:43,147 - root - INFO - Received question: What are the monthly car fuel consumption limits for employees of QADRI-Group by grade, and who bears the fuel cost for official and personal use within the city?
2025-07-13 22:51:43,148 - root - INFO - Chain init: 0.00s
2025-07-13 22:51:43,878 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:51:45,305 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:51:46,548 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:51:46,555 - root - INFO - Question processing: 3.41s
2025-07-13 22:51:46,555 - root - INFO - ⏱️ ask_hr took 3.41 seconds
2025-07-13 22:52:07,086 - root - INFO - Received question: Under what conditions can a Grade M-7 employee request early ownership of a company car, and how is the payment calculated?
2025-07-13 22:52:07,087 - root - INFO - Chain init: 0.00s
2025-07-13 22:52:09,036 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:52:10,872 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:52:13,503 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:52:13,508 - root - INFO - Question processing: 6.42s
2025-07-13 22:52:13,508 - root - INFO - ⏱️ ask_hr took 6.42 seconds
2025-07-13 22:54:54,788 - root - INFO - Starting HR Assistant...
2025-07-13 22:54:54,953 - root - INFO - Configuration validated successfully
2025-07-13 22:54:54,954 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 22:54:54,954 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 22:54:54,955 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 22:54:54,955 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 22:55:00,510 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:55:01,964 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:55:02,287 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:55:04,762 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:55:04,957 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:55:04,963 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 22:55:05,124 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:55:05,126 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:55:05,126 - root - INFO - Loading groq LLM...
2025-07-13 22:55:09,596 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:55:09,619 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 22:55:09,660 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 22:55:09,662 - root - INFO - HR Assistant started successfully
2025-07-13 22:55:33,326 - root - INFO - Collection management action: delete
2025-07-13 22:55:33,326 - root - INFO - Deleting collection: HRDOCS
2025-07-13 22:55:35,636 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:55:36,708 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:55:37,283 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:55:40,530 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:55:43,271 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:56:16,725 - root - INFO - Collection management action: create
2025-07-13 22:56:16,725 - root - INFO - Creating collection: HRDOCS
2025-07-13 22:56:18,780 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:56:19,683 - httpx - INFO - HTTP Request: DELETE https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:56:19,686 - ingestion_retrieval.ingestion - INFO - Deleted existing collection: HRDOCS
2025-07-13 22:56:20,095 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:56:20,099 - ingestion_retrieval.ingestion - INFO - Successfully created collection: HRDOCS
2025-07-13 22:56:20,099 - root - INFO - Collection operation result: Created new collection 'HRDOCS' with dense and sparse vectors
2025-07-13 22:56:23,287 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:56:24,063 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:56:24,313 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:56:30,034 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:56:30,034 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:30,035 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:30,183 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 0 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:30,183 - ingestion_retrieval.ingestion - ERROR - No documents found in directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:30,184 - root - INFO - Successfully ingested documents. Chunks ingested: 0
2025-07-13 22:56:30,185 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:33,323 - root - INFO - Upload request received - folder: Data, file count: 3
2025-07-13 22:56:33,332 - root - INFO - Successfully uploaded 3 files to Data
2025-07-13 22:56:34,295 - root - INFO - Ingest request received - path: uploaded_folders\Data
2025-07-13 22:56:34,296 - root - INFO - Processing directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:34,296 - root - INFO - Starting document ingestion from: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:43,360 - ingestion_retrieval.ingestion - INFO - DirectoryLoader found 3 documents in C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:56:43,360 - ingestion_retrieval.ingestion - INFO - Document 1: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\14. Car & Fuel Policy.docx
2025-07-13 22:56:43,361 - ingestion_retrieval.ingestion - INFO -   Content length: 40391
2025-07-13 22:56:43,361 - ingestion_retrieval.ingestion - INFO -   Content preview: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Suppor...
2025-07-13 22:56:43,361 - ingestion_retrieval.ingestion - INFO - Document 2: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\3. Attendance Policy 3.0.docx
2025-07-13 22:56:43,361 - ingestion_retrieval.ingestion - INFO -   Content length: 31026
2025-07-13 22:56:43,361 - ingestion_retrieval.ingestion - INFO -   Content preview: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resou...
2025-07-13 22:56:43,365 - ingestion_retrieval.ingestion - INFO - Document 3: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data\Data\9. QG Code of Conduct.docx
2025-07-13 22:56:43,365 - ingestion_retrieval.ingestion - INFO -   Content length: 17144
2025-07-13 22:56:43,365 - ingestion_retrieval.ingestion - INFO -   Content preview: Introduction

This code is intended to provide with a clear overview of the obligations that each em...
2025-07-13 22:56:43,367 - ingestion_retrieval.ingestion - INFO - Processing 3 documents for chunking...
2025-07-13 22:56:43,367 - ingestion_retrieval.ingestion - INFO - Processing document 1/3: 14. Car & Fuel Policy.docx
2025-07-13 22:56:43,367 - ingestion_retrieval.ingestion - INFO - Document content length: 40391 characters
2025-07-13 22:56:43,367 - ingestion_retrieval.ingestion - INFO - First 200 characters: CAR POLICY QADRI GROUP

CAR POLICY

Information: Author: Head HR & OD Reviewed By: Group Head Support Services Custodian: Human Resources Department Last Modification Date: 20th September, 2023 Versio...
2025-07-13 22:56:43,519 - ingestion_retrieval.ingestion - INFO - Generated 52 chunks for document: 14. Car & Fuel Policy.docx
2025-07-13 22:56:43,519 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 52
2025-07-13 22:56:43,519 - ingestion_retrieval.ingestion - INFO - Processing document 2/3: 3. Attendance Policy 3.0.docx
2025-07-13 22:56:43,521 - ingestion_retrieval.ingestion - INFO - Document content length: 31026 characters
2025-07-13 22:56:43,521 - ingestion_retrieval.ingestion - INFO - First 200 characters: Attendance Policy QADRI GROUP

Attendance Policy

Author Group Head HR & Admin Custodian Human Resource Department Effective Date 1st Mar  2025 Version 3.0



Objective

Attendance is considered as co...
2025-07-13 22:56:43,673 - ingestion_retrieval.ingestion - INFO - Generated 41 chunks for document: 3. Attendance Policy 3.0.docx
2025-07-13 22:56:43,676 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 41
2025-07-13 22:56:43,677 - ingestion_retrieval.ingestion - INFO - Processing document 3/3: 9. QG Code of Conduct.docx
2025-07-13 22:56:43,677 - ingestion_retrieval.ingestion - INFO - Document content length: 17144 characters
2025-07-13 22:56:43,678 - ingestion_retrieval.ingestion - INFO - First 200 characters: Introduction

This code is intended to provide with a clear overview of the obligations that each employee in Qadri Group needs to understand. It captures not only the values we live by, but also the ...
2025-07-13 22:56:43,832 - ingestion_retrieval.ingestion - INFO - Generated 22 chunks for document: 9. QG Code of Conduct.docx
2025-07-13 22:56:43,832 - ingestion_retrieval.ingestion - INFO - Valid chunks (non-empty): 22
2025-07-13 22:56:43,832 - ingestion_retrieval.ingestion - INFO - Total chunks generated: 115
2025-07-13 22:56:43,833 - ingestion_retrieval.ingestion - INFO - Using Google embedding model: models/embedding-001
2025-07-13 22:56:45,797 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:56:48,193 - ingestion_retrieval.ingestion - INFO - Initializing QdrantVectorStore with vector_name='dense', sparse_vector_name='sparse'
2025-07-13 22:56:49,103 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:56:50,846 - ingestion_retrieval.ingestion - INFO - Adding 115 chunks to the vector store...
2025-07-13 22:56:55,395 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-13 22:56:58,244 - httpx - INFO - HTTP Request: PUT https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points?wait=true "HTTP/1.1 200 OK"
2025-07-13 22:56:58,247 - ingestion_retrieval.ingestion - INFO - Successfully added documents to the vector store
2025-07-13 22:56:58,247 - root - INFO - Successfully ingested documents. Chunks ingested: 115
2025-07-13 22:56:58,250 - root - INFO - Successfully removed files from directory: C:\Users\user\Desktop\QD_HR_ASSISTANT\uploaded_folders\Data
2025-07-13 22:57:06,715 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:57:06,719 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 22:57:07,951 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:57:07,951 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-13 22:57:08,191 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:57:08,192 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:57:08,240 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:57:08,240 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:57:08,240 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 22:57:08,242 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 22:57:22,742 - root - INFO - Received question: m7 leave policy
2025-07-13 22:57:22,742 - root - INFO - Chain init: 0.00s
2025-07-13 22:57:24,267 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:57:25,936 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:57:28,133 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:57:28,138 - root - INFO - Question processing: 5.40s
2025-07-13 22:57:28,138 - root - INFO - ⏱️ ask_hr took 5.40 seconds
2025-07-13 22:57:51,185 - root - INFO - Received question: Under what conditions can a Grade M-7 employee request early ownership of a company car, and how is the payment calculated?
2025-07-13 22:57:51,186 - root - INFO - Chain init: 0.00s
2025-07-13 22:57:52,048 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:57:53,696 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:57:55,493 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:57:55,498 - root - INFO - Question processing: 4.31s
2025-07-13 22:57:55,498 - root - INFO - ⏱️ ask_hr took 4.31 seconds
2025-07-13 22:59:24,882 - root - INFO - Received question: fuel limits policy
2025-07-13 22:59:24,884 - root - INFO - Chain init: 0.00s
2025-07-13 22:59:26,439 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:59:29,773 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:59:32,156 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:59:32,162 - root - INFO - Question processing: 7.28s
2025-07-13 22:59:32,163 - root - INFO - ⏱️ ask_hr took 7.28 seconds
2025-07-13 22:59:46,631 - root - INFO - Received question: fuel limit
2025-07-13 22:59:46,631 - root - INFO - Chain init: 0.00s
2025-07-13 22:59:47,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 22:59:49,151 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 22:59:53,641 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 22:59:53,648 - root - INFO - Question processing: 7.02s
2025-07-13 22:59:53,648 - root - INFO - ⏱️ ask_hr took 7.02 seconds
2025-07-13 23:01:53,343 - root - INFO - Starting HR Assistant...
2025-07-13 23:01:53,495 - root - INFO - Configuration validated successfully
2025-07-13 23:01:53,495 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 23:01:53,495 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 23:01:53,495 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 23:01:53,495 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 23:01:58,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 23:01:59,060 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:01:59,291 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:02:00,284 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:02:00,446 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 23:02:00,452 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 23:02:00,613 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 23:02:00,613 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 23:02:00,613 - root - INFO - Loading groq LLM...
2025-07-13 23:02:03,673 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:02:03,695 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 23:02:03,702 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 23:02:03,702 - root - INFO - HR Assistant started successfully
2025-07-13 23:11:38,624 - root - INFO - Received question: if i do 5 excess leave and my slary is 200k then how much deduction
2025-07-13 23:11:38,636 - root - INFO - Chain init: 0.00s
2025-07-13 23:11:40,245 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:11:42,555 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:11:44,375 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:11:44,431 - root - INFO - Question processing: 5.80s
2025-07-13 23:11:44,431 - root - INFO - ⏱️ ask_hr took 5.81 seconds
2025-07-13 23:13:55,384 - root - INFO - Received question: What is the official process an employee must follow if they forget to mark attendance with their card, and what are the consequences of repeated manual attendance?
2025-07-13 23:13:55,385 - root - INFO - Chain init: 0.00s
2025-07-13 23:13:56,115 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:13:57,657 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:13:59,291 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:13:59,297 - root - INFO - Question processing: 3.91s
2025-07-13 23:13:59,297 - root - INFO - ⏱️ ask_hr took 3.91 seconds
2025-07-13 23:14:32,070 - root - INFO - Received question: How does the company handle leaves taken on Fridays or Mondays adjacent to weekends? Are there any exceptions to this rule?
2025-07-13 23:14:32,070 - root - INFO - Chain init: 0.00s
2025-07-13 23:14:32,977 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:14:34,681 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:14:36,465 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:14:36,471 - root - INFO - Question processing: 4.40s
2025-07-13 23:14:36,472 - root - INFO - ⏱️ ask_hr took 4.40 seconds
2025-07-13 23:23:58,340 - root - INFO - Received question: If a company car is damaged due to an employee's negligence, who bears the cost, and how is the repair payment calculated?
2025-07-13 23:23:58,340 - root - INFO - Chain init: 0.00s
2025-07-13 23:24:00,032 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:24:02,228 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:24:04,448 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:24:04,451 - root - INFO - Question processing: 6.11s
2025-07-13 23:24:04,451 - root - INFO - ⏱️ ask_hr took 6.11 seconds
2025-07-13 23:24:40,856 - root - INFO - Received question: What happens to the company car if a Grade M-7 employee leaves before the car reaches maturity? What are the possible payment or refund scenarios?
2025-07-13 23:24:40,856 - root - INFO - Chain init: 0.00s
2025-07-13 23:24:41,636 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:24:43,054 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:24:45,374 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:24:45,381 - root - INFO - Question processing: 4.52s
2025-07-13 23:24:45,381 - root - INFO - ⏱️ ask_hr took 4.52 seconds
2025-07-13 23:25:01,868 - root - INFO - Received question: What disciplinary actions may follow if an employee is found marking attendance on behalf of another employee?
2025-07-13 23:25:01,868 - root - INFO - Chain init: 0.00s
2025-07-13 23:25:02,861 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:25:04,541 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:25:06,310 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:25:06,352 - root - INFO - Question processing: 4.48s
2025-07-13 23:25:06,352 - root - INFO - ⏱️ ask_hr took 4.48 seconds
2025-07-13 23:26:04,974 - root - INFO - Received question: Can an employee accept a gift from a vendor? What should they do if they receive such a gift?
2025-07-13 23:26:04,979 - root - INFO - Chain init: 0.00s
2025-07-13 23:26:06,189 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:26:07,939 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:26:09,745 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:26:09,753 - root - INFO - Question processing: 4.77s
2025-07-13 23:26:09,753 - root - INFO - ⏱️ ask_hr took 4.78 seconds
2025-07-13 23:30:19,276 - root - INFO - Starting HR Assistant...
2025-07-13 23:30:19,434 - root - INFO - Configuration validated successfully
2025-07-13 23:30:19,435 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 23:30:19,435 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 23:30:19,435 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 23:30:19,435 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 23:30:24,208 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 23:30:24,768 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:30:24,953 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:30:26,379 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:30:26,545 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 23:30:26,556 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 23:30:26,713 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 23:30:26,715 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 23:30:26,716 - root - INFO - Loading groq LLM...
2025-07-13 23:30:29,694 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:30:29,712 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 23:30:29,719 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 23:30:29,719 - root - INFO - HR Assistant started successfully
2025-07-13 23:31:03,574 - root - INFO - Received question: Can an employee accept a gift from a vendor? What should they do if they receive such a gift?
2025-07-13 23:31:03,575 - root - INFO - Chain init: 0.00s
2025-07-13 23:31:04,168 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:31:05,597 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:31:06,840 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:31:06,845 - root - INFO - Question processing: 3.27s
2025-07-13 23:31:06,846 - root - INFO - ⏱️ ask_hr took 3.27 seconds
2025-07-13 23:36:28,046 - root - INFO - Received question: How does the company handle leaves taken on Fridays or Mondays adjacent to weekends? Are there any exceptions to this rule?
2025-07-13 23:36:28,081 - root - INFO - Chain init: 0.00s
2025-07-13 23:36:29,985 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:36:32,330 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:36:33,823 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:36:33,890 - root - INFO - Question processing: 5.81s
2025-07-13 23:36:33,892 - root - INFO - ⏱️ ask_hr took 5.86 seconds
2025-07-13 23:38:27,128 - root - INFO - Received question: If a Grade M-7 employee’s company car is stolen, and the incident is reported late, what financial or disciplinary consequences might they face?
2025-07-13 23:38:27,131 - root - INFO - Chain init: 0.00s
2025-07-13 23:38:28,354 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:38:30,188 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:38:32,455 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:38:32,474 - root - INFO - Question processing: 5.34s
2025-07-13 23:38:32,476 - root - INFO - ⏱️ ask_hr took 5.35 seconds
2025-07-13 23:48:15,904 - root - INFO - Received question: m7 leave policy
2025-07-13 23:48:15,905 - root - INFO - Chain init: 0.00s
2025-07-13 23:48:19,407 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:48:22,070 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:48:23,552 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:48:23,567 - root - INFO - Question processing: 7.66s
2025-07-13 23:48:23,568 - root - INFO - ⏱️ ask_hr took 7.66 seconds
2025-07-13 23:49:20,571 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-13 23:49:20,573 - root - INFO - Chain init: 0.00s
2025-07-13 23:49:21,651 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:49:23,795 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:49:26,105 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:49:26,151 - root - INFO - Question processing: 5.58s
2025-07-13 23:49:26,151 - root - INFO - ⏱️ ask_hr took 5.58 seconds
2025-07-13 23:51:54,456 - root - INFO - Received question: M7 LEAVE POLICY
2025-07-13 23:51:54,456 - root - INFO - Chain init: 0.00s
2025-07-13 23:51:55,176 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:51:56,695 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:51:59,604 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:51:59,619 - root - INFO - Question processing: 5.12s
2025-07-13 23:51:59,621 - root - INFO - ⏱️ ask_hr took 5.17 seconds
2025-07-13 23:53:04,098 - root - INFO - Starting HR Assistant...
2025-07-13 23:53:04,284 - root - INFO - Configuration validated successfully
2025-07-13 23:53:04,284 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 23:53:04,284 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 23:53:04,286 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 23:53:04,287 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 23:53:09,412 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 23:53:10,465 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:53:10,684 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:53:13,262 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:53:13,467 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 23:53:13,475 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 23:53:13,655 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 23:53:13,656 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 23:53:13,657 - root - INFO - Loading groq LLM...
2025-07-13 23:53:16,204 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:53:16,237 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 23:53:16,244 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 23:53:16,248 - root - INFO - HR Assistant started successfully
2025-07-13 23:53:45,207 - root - INFO - Starting HR Assistant...
2025-07-13 23:53:45,392 - root - INFO - Configuration validated successfully
2025-07-13 23:53:45,392 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-13 23:53:45,392 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-13 23:53:45,394 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-13 23:53:45,394 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-13 23:53:50,414 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-13 23:53:52,289 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:53:52,739 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:53:54,807 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:53:54,958 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-13 23:53:54,965 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-13 23:53:55,115 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-13 23:53:55,115 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-13 23:53:55,115 - root - INFO - Loading groq LLM...
2025-07-13 23:53:57,538 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:53:57,556 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-13 23:53:57,563 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-13 23:53:57,563 - root - INFO - HR Assistant started successfully
2025-07-13 23:54:21,503 - root - INFO - Received question: EXCESS LEAVE FORMULA
2025-07-13 23:54:21,503 - root - INFO - Chain init: 0.00s
2025-07-13 23:54:23,253 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-13 23:54:24,818 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-13 23:54:26,876 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-13 23:54:26,887 - root - INFO - Question processing: 5.38s
2025-07-13 23:54:26,887 - root - INFO - ⏱️ ask_hr took 5.38 seconds
2025-07-14 06:57:03,894 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:03,973 - root - INFO - Configuration validated successfully
2025-07-14 06:57:03,974 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:03,975 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:03,978 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:03,980 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:04,867 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:04,871 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:04,974 - root - INFO - Configuration validated successfully
2025-07-14 06:57:04,984 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:04,985 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:04,988 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:05,064 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:05,080 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:05,174 - root - INFO - Configuration validated successfully
2025-07-14 06:57:05,183 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:05,264 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:05,274 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:05,364 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:05,669 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:05,771 - root - INFO - Configuration validated successfully
2025-07-14 06:57:05,775 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:05,780 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:05,865 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:05,871 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:06,070 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:06,265 - root - INFO - Configuration validated successfully
2025-07-14 06:57:06,275 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:06,181 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:06,277 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:06,376 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:06,378 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:06,469 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:06,385 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:06,671 - root - INFO - Configuration validated successfully
2025-07-14 06:57:06,675 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:06,774 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:06,868 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:06,873 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:06,877 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:07,068 - root - INFO - Configuration validated successfully
2025-07-14 06:57:07,164 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:07,265 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:07,082 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:07,364 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:07,476 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:07,571 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:07,779 - root - INFO - Configuration validated successfully
2025-07-14 06:57:07,782 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:07,865 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:07,867 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:07,869 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:08,273 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:08,374 - root - INFO - Configuration validated successfully
2025-07-14 06:57:08,378 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:08,472 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:08,367 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:08,478 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:08,583 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:08,666 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:08,874 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:09,073 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:09,169 - root - INFO - Configuration validated successfully
2025-07-14 06:57:09,179 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:09,264 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:09,273 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:09,276 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:09,567 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:09,865 - root - INFO - Configuration validated successfully
2025-07-14 06:57:09,965 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:09,974 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:09,982 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:09,983 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:10,165 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:11,171 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:10,988 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:11,368 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:11,473 - root - INFO - Configuration validated successfully
2025-07-14 06:57:11,576 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:11,665 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:11,765 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:11,768 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:12,768 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:13,267 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:13,576 - root - INFO - Configuration validated successfully
2025-07-14 06:57:13,671 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:13,766 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:13,864 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:13,965 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:13,871 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:14,186 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:14,667 - root - INFO - Configuration validated successfully
2025-07-14 06:57:14,764 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:14,770 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:15,065 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:15,075 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:15,266 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:15,567 - root - INFO - Configuration validated successfully
2025-07-14 06:57:15,580 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:15,670 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:15,765 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:15,865 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:16,579 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:16,675 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:16,879 - root - INFO - Configuration validated successfully
2025-07-14 06:57:16,964 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:16,972 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:16,975 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:16,966 - root - INFO - Configuration validated successfully
2025-07-14 06:57:17,165 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:17,168 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:17,079 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:17,179 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:17,278 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:18,169 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:18,975 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:19,066 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:19,065 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:20,673 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:52,582 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:52,601 - root - INFO - Configuration validated successfully
2025-07-14 06:57:52,602 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:52,608 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:52,611 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:52,614 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:52,793 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:52,892 - root - INFO - Configuration validated successfully
2025-07-14 06:57:52,903 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:52,981 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:52,989 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:52,995 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:53,286 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:53,378 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:53,480 - root - INFO - Configuration validated successfully
2025-07-14 06:57:53,494 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:53,583 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:53,586 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:53,587 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:53,676 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:53,693 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:53,876 - root - INFO - Configuration validated successfully
2025-07-14 06:57:53,976 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:53,993 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:53,982 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:54,075 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:54,080 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:54,180 - root - INFO - Configuration validated successfully
2025-07-14 06:57:54,185 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:54,276 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:54,280 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:54,289 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:54,375 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:54,382 - root - INFO - Configuration validated successfully
2025-07-14 06:57:54,479 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:54,485 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:54,576 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:54,582 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:54,683 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:54,784 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:54,881 - root - INFO - Configuration validated successfully
2025-07-14 06:57:54,977 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:54,982 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:54,993 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:55,084 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:55,177 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:55,583 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:56,076 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:56,182 - root - INFO - Configuration validated successfully
2025-07-14 06:57:56,276 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:56,281 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:56,379 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:56,384 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:56,279 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:56,578 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:56,584 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:56,680 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:56,685 - root - INFO - Configuration validated successfully
2025-07-14 06:57:56,779 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:56,678 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:56,785 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:56,785 - root - INFO - Configuration validated successfully
2025-07-14 06:57:56,976 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:56,879 - root - INFO - Configuration validated successfully
2025-07-14 06:57:56,976 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:57,081 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:57,075 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:57,182 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:57,282 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:57,384 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:57,486 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:57,579 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:57,680 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:57,791 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:58,080 - root - INFO - Configuration validated successfully
2025-07-14 06:57:58,176 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:58,181 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:58,276 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:58,280 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:58,279 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:58,485 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:58,584 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:58,687 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:58,783 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:58,882 - root - INFO - Configuration validated successfully
2025-07-14 06:57:58,975 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:58,989 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:58,992 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:58,993 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:59,186 - root - INFO - Configuration validated successfully
2025-07-14 06:57:59,200 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:59,277 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:59,375 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:59,475 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:57:59,697 - root - INFO - Starting HR Assistant...
2025-07-14 06:57:59,685 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:57:59,877 - root - INFO - Configuration validated successfully
2025-07-14 06:57:59,888 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:57:59,892 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:57:59,897 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:57:59,976 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:58:00,076 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:58:00,778 - root - INFO - Starting HR Assistant...
2025-07-14 06:58:00,795 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:58:00,877 - root - INFO - Configuration validated successfully
2025-07-14 06:58:00,878 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:58:00,878 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:58:00,881 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:58:00,885 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:58:00,885 - root - INFO - Starting HR Assistant...
2025-07-14 06:58:00,890 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:58:00,902 - root - INFO - Configuration validated successfully
2025-07-14 06:58:00,903 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 06:58:00,904 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 06:58:00,905 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 06:58:00,901 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:58:00,906 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 06:58:00,996 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 06:58:01,016 - root - ERROR - Startup failed: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
Traceback (most recent call last):
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 42, in __init__
    from fastembed import SparseTextEmbedding  # type: ignore
ModuleNotFoundError: No module named 'fastembed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/main.py", line 123, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
  File "/app/ingestion_retrieval/retrieval.py", line 222, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
  File "/app/ingestion_retrieval/retrieval.py", line 245, in get_hr_assistant_chain
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")
  File "/opt/venv/lib/python3.10/site-packages/langchain_qdrant/fastembed_sparse.py", line 44, in __init__
    raise ValueError(
ValueError: The 'fastembed' package is not installed. Please install it with `pip install fastembed` or `pip install fastembed-gpu`.
2025-07-14 07:10:18,437 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:18,549 - root - INFO - Configuration validated successfully
2025-07-14 07:10:18,552 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:18,633 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:18,642 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:18,649 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:19,634 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:19,946 - root - INFO - Configuration validated successfully
2025-07-14 07:10:20,049 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:20,134 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:20,144 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:20,232 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:20,233 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:20,536 - root - INFO - Configuration validated successfully
2025-07-14 07:10:20,550 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:20,632 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:20,640 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:20,644 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:21,935 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:22,237 - root - INFO - Configuration validated successfully
2025-07-14 07:10:22,243 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:22,333 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:22,347 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:22,433 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:23,236 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:23,539 - root - INFO - Configuration validated successfully
2025-07-14 07:10:23,639 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:23,736 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:23,739 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:23,832 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:23,833 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:24,329 - root - INFO - Configuration validated successfully
2025-07-14 07:10:24,245 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:24,426 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:24,538 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:24,626 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:24,638 - root - INFO - Configuration validated successfully
2025-07-14 07:10:24,643 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:24,726 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:24,730 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:24,926 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:24,938 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:26,644 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:26,726 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:26,838 - root - INFO - Configuration validated successfully
2025-07-14 07:10:26,840 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:26,842 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:26,925 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:26,934 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:26,940 - root - INFO - Configuration validated successfully
2025-07-14 07:10:27,026 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:27,033 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:27,039 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:27,043 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:28,034 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:28,742 - root - INFO - Configuration validated successfully
2025-07-14 07:10:28,744 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:28,839 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:28,925 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:28,928 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:29,136 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:29,243 - root - INFO - Configuration validated successfully
2025-07-14 07:10:29,328 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:29,337 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:29,425 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:29,525 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:29,933 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:30,338 - root - INFO - Configuration validated successfully
2025-07-14 07:10:30,348 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:30,539 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:30,627 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:30,636 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:30,735 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:31,026 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:31,042 - root - INFO - Configuration validated successfully
2025-07-14 07:10:31,044 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:31,033 - root - INFO - Configuration validated successfully
2025-07-14 07:10:31,127 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:31,129 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:31,130 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:31,131 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:31,125 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:31,225 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:31,229 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:32,231 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:32,826 - root - INFO - Configuration validated successfully
2025-07-14 07:10:32,848 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:32,853 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:32,930 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:33,027 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:34,436 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:35,028 - root - INFO - Configuration validated successfully
2025-07-14 07:10:35,037 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:35,044 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:35,132 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:35,135 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:10:37,529 - root - INFO - Starting HR Assistant...
2025-07-14 07:10:37,832 - root - INFO - Configuration validated successfully
2025-07-14 07:10:37,841 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 07:10:37,844 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:10:37,845 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 07:10:37,848 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 07:11:04,535 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:04,540 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:05,132 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:05,224 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:05,626 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:05,638 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:06,168 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:06,168 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:06,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:06,229 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:06,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:06,727 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:06,815 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:06,824 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:06,823 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:06,824 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,184 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,224 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,227 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,227 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,426 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:07,544 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:07,648 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:07,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:08,216 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:08,516 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:08,825 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:08,857 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:08,859 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:08,874 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:08,894 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:08,925 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:08,940 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:08,943 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:08,970 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:08,971 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:09,024 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,031 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:09,033 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:09,034 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:09,035 - root - INFO - Loading groq LLM...
2025-07-14 07:11:09,037 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:09,041 - root - INFO - Loading groq LLM...
2025-07-14 07:11:09,044 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,046 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:09,056 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:09,060 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:09,068 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:09,069 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:09,123 - root - INFO - Loading groq LLM...
2025-07-14 07:11:09,130 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:09,146 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:09,149 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:09,151 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,152 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:09,228 - root - INFO - Loading groq LLM...
2025-07-14 07:11:09,325 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,427 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,523 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,626 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,726 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,733 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:09,734 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:09,827 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,828 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,830 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,926 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,832 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:09,924 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:09,933 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:11:10,027 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:10,029 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:10,124 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:10,124 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:10,232 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:10,144 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:10,324 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:10,326 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:10,326 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:10,330 - root - INFO - Loading groq LLM...
2025-07-14 07:11:10,332 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:10,335 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:10,337 - root - INFO - Loading groq LLM...
2025-07-14 07:11:10,342 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:10,423 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:10,426 - root - INFO - Loading groq LLM...
2025-07-14 07:11:10,427 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:10,432 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:10,433 - root - INFO - Loading groq LLM...
2025-07-14 07:11:10,524 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:10,528 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:10,627 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:10,530 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:10,827 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:10,926 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:11,127 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:11,327 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:11,340 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:11,625 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:11,727 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:11,832 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:11,836 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:11,841 - root - INFO - Loading groq LLM...
2025-07-14 07:11:11,924 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:11,932 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:12,132 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:12,228 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:12,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:12,337 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:12,530 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:12,630 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:12,728 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:12,639 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:12,830 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:12,945 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:13,123 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:13,025 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:13,038 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:13,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:13,330 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:13,343 - root - INFO - Loading groq LLM...
2025-07-14 07:11:13,426 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:13,437 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:13,436 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:13,527 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:13,534 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:13,526 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:13,442 - root - INFO - Loading groq LLM...
2025-07-14 07:11:13,542 - root - INFO - Loading groq LLM...
2025-07-14 07:11:13,624 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:13,726 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:13,923 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:13,924 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:13,930 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:14,023 - root - INFO - Loading groq LLM...
2025-07-14 07:11:14,129 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:14,225 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:14,231 - root - INFO - Loading groq LLM...
2025-07-14 07:11:14,228 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:14,424 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:14,428 - root - INFO - Loading groq LLM...
2025-07-14 07:11:14,531 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:14,637 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:14,729 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:14,731 - root - INFO - Loading groq LLM...
2025-07-14 07:11:14,942 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:11:15,140 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 07:11:15,332 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 07:11:15,534 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:11:15,535 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:11:15,539 - root - INFO - Loading groq LLM...
2025-07-14 07:11:23,435 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:23,526 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:23,720 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:23,727 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:23,732 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:23,828 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:23,831 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:23,927 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,021 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,022 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,022 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:24,226 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:24,320 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,327 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:24,418 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:24,416 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,437 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,419 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:24,419 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:24,429 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:24,442 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,540 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:24,541 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:24,623 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,625 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:24,720 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:24,727 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,721 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:24,727 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,830 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:24,818 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:24,824 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:24,924 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:24,929 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:24,932 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,032 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,040 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:24,937 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:25,017 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,125 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,127 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,135 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,223 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:11:25,224 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,228 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,233 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,229 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,334 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,419 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,427 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,431 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,516 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,532 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,631 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,634 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,725 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,729 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,729 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,817 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,733 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,734 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,818 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,819 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 07:11:25,824 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,841 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,845 - root - INFO - HR Assistant started successfully
2025-07-14 07:11:25,917 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 07:11:25,920 - root - INFO - HR Assistant started successfully
2025-07-14 07:17:45,514 - root - INFO - Received question: m7 leave policy
2025-07-14 07:17:45,520 - root - INFO - Chain init: 0.00s
2025-07-14 07:17:48,670 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:17:53,294 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-14 07:17:54,722 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 07:17:54,746 - root - INFO - Question processing: 9.22s
2025-07-14 07:17:54,747 - root - INFO - ⏱️ ask_hr took 9.23 seconds
2025-07-14 07:18:33,361 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:18:33,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:18:33,960 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 07:18:33,961 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 07:18:34,157 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:18:34,186 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:18:34,225 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:18:34,228 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:18:34,230 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:18:34,232 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:18:47,054 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:18:47,057 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:18:47,060 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:18:47,062 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:18:49,611 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:18:51,466 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 07:18:51,942 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:18:54,094 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:18:55,495 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 07:18:55,694 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:49:38,894 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:49:38,918 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:49:38,919 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:49:38,921 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:49:41,979 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 07:49:41,987 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 07:49:42,746 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:49:43,014 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 07:49:44,070 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 07:49:44,301 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 07:49:44,301 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:49:44,804 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 07:49:46,439 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 07:49:46,443 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 09:40:15,321 - root - INFO - Received question: fuel limits
2025-07-14 09:40:15,356 - root - INFO - Chain init: 0.00s
2025-07-14 09:40:18,235 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 09:40:22,328 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-14 09:40:24,178 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 09:40:24,215 - root - INFO - Question processing: 8.86s
2025-07-14 09:40:24,217 - root - INFO - ⏱️ ask_hr took 8.90 seconds
2025-07-14 09:41:07,988 - root - INFO - Received question: fuel limits for all grade
2025-07-14 09:41:07,990 - root - INFO - Chain init: 0.00s
2025-07-14 09:41:08,996 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 09:41:11,318 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-14 09:41:14,801 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 09:41:14,811 - root - INFO - Question processing: 6.82s
2025-07-14 09:41:14,812 - root - INFO - ⏱️ ask_hr took 6.82 seconds
2025-07-14 12:03:53,190 - root - INFO - Received question: ANY LATEST NEWS
2025-07-14 12:03:53,234 - root - INFO - Chain init: 0.00s
2025-07-14 12:03:55,496 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 12:03:59,138 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-14 12:04:03,793 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 0.410183 seconds
2025-07-14 12:04:09,337 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 12:04:09,375 - root - INFO - Question processing: 16.14s
2025-07-14 12:04:09,380 - root - INFO - ⏱️ ask_hr took 16.19 seconds
2025-07-14 12:10:02,323 - root - INFO - Received question: FUEL LIMITS
2025-07-14 12:10:02,338 - root - INFO - Chain init: 0.00s
2025-07-14 12:10:04,363 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 12:10:07,811 - httpx - INFO - HTTP Request: POST https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS/points/query "HTTP/1.1 200 OK"
2025-07-14 12:10:11,137 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 12:10:11,195 - root - INFO - Question processing: 8.86s
2025-07-14 12:10:11,196 - root - INFO - ⏱️ ask_hr took 8.87 seconds
2025-07-14 20:53:02,655 - root - INFO - Starting HR Assistant...
2025-07-14 20:53:02,704 - root - INFO - Configuration validated successfully
2025-07-14 20:53:02,704 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 20:53:02,704 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 20:53:02,704 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 20:53:02,707 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 20:53:07,590 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 20:53:08,147 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:53:08,334 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:53:10,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:53:10,983 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 20:53:10,988 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 20:53:11,132 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 20:53:11,133 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 20:53:11,133 - root - INFO - Loading groq LLM...
2025-07-14 20:53:13,394 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 20:53:13,437 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 20:53:13,445 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 20:53:13,445 - root - INFO - HR Assistant started successfully
2025-07-14 20:53:39,318 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 20:53:39,333 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 20:53:39,847 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 20:53:39,864 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 20:53:40,062 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:53:40,062 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:53:40,074 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 20:53:40,074 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 20:53:40,074 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 20:53:40,074 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 20:53:44,723 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 20:53:44,723 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 20:57:48,822 - root - INFO - Starting HR Assistant...
2025-07-14 20:57:48,990 - root - INFO - Configuration validated successfully
2025-07-14 20:57:48,990 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 20:57:48,990 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 20:57:48,990 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 20:57:48,990 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 20:57:55,197 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 20:57:55,758 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:57:56,064 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:57:56,908 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 20:57:57,074 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 20:57:57,084 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 20:57:57,239 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 20:57:57,239 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 20:57:57,239 - root - INFO - Loading groq LLM...
2025-07-14 20:58:00,136 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 20:58:00,155 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 20:58:00,159 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 20:58:00,159 - root - INFO - HR Assistant started successfully
2025-07-14 21:21:00,097 - root - INFO - Starting HR Assistant...
2025-07-14 21:21:00,284 - root - INFO - Configuration validated successfully
2025-07-14 21:21:00,286 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 21:21:00,286 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 21:21:00,286 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 21:21:00,286 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 21:21:06,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:21:07,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:21:07,468 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:21:09,242 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:21:09,523 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 21:21:09,535 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 21:21:09,832 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:21:09,832 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:21:09,832 - root - INFO - Loading groq LLM...
2025-07-14 21:21:13,127 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 21:21:13,155 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 21:21:13,160 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 21:21:13,161 - root - INFO - HR Assistant started successfully
2025-07-15 04:24:03,919 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:04,038 - root - INFO - Configuration validated successfully
2025-07-15 04:24:04,116 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:04,126 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:04,129 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:04,132 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:04,143 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:04,315 - root - INFO - Configuration validated successfully
2025-07-15 04:24:04,322 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:04,323 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:04,325 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:04,336 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:04,416 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:04,521 - root - INFO - Configuration validated successfully
2025-07-15 04:24:04,530 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:04,618 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:04,715 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:04,731 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:04,923 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:05,036 - root - INFO - Configuration validated successfully
2025-07-15 04:24:05,127 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:05,129 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:05,215 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:05,230 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:05,415 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:05,520 - root - INFO - Configuration validated successfully
2025-07-15 04:24:05,528 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:05,532 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:05,616 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:05,620 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:06,024 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:06,128 - root - INFO - Configuration validated successfully
2025-07-15 04:24:06,129 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:06,131 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:06,133 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:06,234 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:06,740 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:06,818 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:06,918 - root - INFO - Configuration validated successfully
2025-07-15 04:24:07,015 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:07,018 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:07,029 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:07,026 - root - INFO - Configuration validated successfully
2025-07-15 04:24:07,121 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:07,031 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:07,126 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:07,225 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:07,228 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:07,524 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:07,633 - root - INFO - Configuration validated successfully
2025-07-15 04:24:07,715 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:07,632 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:07,720 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:07,815 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:07,826 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:07,834 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:07,916 - root - INFO - Configuration validated successfully
2025-07-15 04:24:07,923 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:07,925 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:07,927 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:08,015 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:08,415 - root - INFO - Configuration validated successfully
2025-07-15 04:24:08,422 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:08,423 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:08,425 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:08,427 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:09,015 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:09,221 - root - INFO - Configuration validated successfully
2025-07-15 04:24:09,227 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:09,316 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:09,326 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:09,330 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:11,020 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:11,315 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:11,326 - root - INFO - Configuration validated successfully
2025-07-15 04:24:11,416 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:11,428 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:11,431 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:11,519 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:11,525 - root - INFO - Configuration validated successfully
2025-07-15 04:24:11,528 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:11,620 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:11,629 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:11,631 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:12,426 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:12,629 - root - INFO - Configuration validated successfully
2025-07-15 04:24:12,715 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:12,719 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:12,727 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:12,730 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:14,734 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:15,221 - root - INFO - Configuration validated successfully
2025-07-15 04:24:15,228 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:15,315 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:15,318 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:15,327 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:18,831 - root - INFO - Starting HR Assistant...
2025-07-15 04:24:19,420 - root - INFO - Configuration validated successfully
2025-07-15 04:24:19,516 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:24:19,525 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:24:19,535 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:24:19,616 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:24:47,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:47,122 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:47,414 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:47,915 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:48,611 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:48,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:48,816 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:49,113 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:49,121 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:49,118 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:49,117 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:49,512 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:49,810 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:49,818 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:50,213 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,211 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,223 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,512 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,621 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:50,711 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,715 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,712 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:50,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:50,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,112 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:51,511 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,511 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,512 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:51,520 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:51,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,913 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:51,911 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,011 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,031 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:52,116 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:52,216 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:52,216 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:52,216 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:52,221 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,225 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:52,226 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:52,227 - root - INFO - Loading groq LLM...
2025-07-15 04:24:52,229 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:52,232 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:52,310 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,311 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:52,324 - root - INFO - Loading groq LLM...
2025-07-15 04:24:52,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,411 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,414 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:52,511 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,528 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:52,622 - root - INFO - Loading groq LLM...
2025-07-15 04:24:52,613 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:52,615 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,721 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:52,811 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,812 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:52,910 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:52,910 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:52,916 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:52,917 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:53,015 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:53,016 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:53,113 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:53,115 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:53,122 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:53,217 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:53,126 - root - INFO - Loading groq LLM...
2025-07-15 04:24:53,211 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:53,316 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:53,311 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:53,313 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:53,314 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:53,327 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:53,412 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:53,414 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:53,429 - root - INFO - Loading groq LLM...
2025-07-15 04:24:53,416 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:53,417 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:53,417 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:53,516 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:53,616 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:53,625 - root - INFO - Loading groq LLM...
2025-07-15 04:24:53,620 - root - INFO - Loading groq LLM...
2025-07-15 04:24:53,711 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:53,714 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:53,715 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:53,717 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:53,721 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:53,724 - root - INFO - Loading groq LLM...
2025-07-15 04:24:53,723 - root - INFO - Loading groq LLM...
2025-07-15 04:24:53,738 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:53,811 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:53,924 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:54,311 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:54,315 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:54,321 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:54,418 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:54,510 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:54,424 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:54,622 - root - INFO - Loading groq LLM...
2025-07-15 04:24:54,512 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:54,617 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:54,815 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:54,914 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:54,917 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:55,013 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:55,024 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:55,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:55,211 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:55,224 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:55,217 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:55,316 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:55,318 - root - INFO - Loading groq LLM...
2025-07-15 04:24:55,322 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:55,317 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:55,418 - root - INFO - Loading groq LLM...
2025-07-15 04:24:55,416 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:55,420 - root - INFO - Loading groq LLM...
2025-07-15 04:24:55,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:24:55,720 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:55,916 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:56,118 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:56,317 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:56,417 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:56,421 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:56,513 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:56,516 - root - INFO - Loading groq LLM...
2025-07-15 04:24:57,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:57,111 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:57,409 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:57,619 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:57,721 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:57,821 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:57,823 - root - INFO - Loading groq LLM...
2025-07-15 04:24:57,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:58,111 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:58,315 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:58,510 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:58,524 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:58,606 - root - INFO - Loading groq LLM...
2025-07-15 04:24:58,913 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:24:59,015 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:24:59,213 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:24:59,409 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:24:59,413 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:24:59,422 - root - INFO - Loading groq LLM...
2025-07-15 04:25:00,107 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,131 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,139 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,146 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,212 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,159 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,208 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,212 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,408 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,409 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,409 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,517 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,609 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,610 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:00,618 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,622 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,618 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:00,708 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:00,711 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:00,712 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,716 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,806 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,810 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:00,810 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,015 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,810 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,111 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:00,811 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:00,907 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:00,920 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,311 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,016 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,506 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:01,206 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,309 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,313 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:25:01,525 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:25:01,406 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,416 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:01,416 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,521 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,525 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:01,522 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,526 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:25:01,620 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,621 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:25:01,622 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,708 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,717 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:01,720 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,722 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,808 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,811 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:01,813 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:01,829 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:01,831 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:02,644 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:02,675 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:02,683 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:02,684 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:03,302 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:25:03,798 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:03,800 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:03,801 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:25:03,862 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:03,916 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:03,917 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:03,919 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:03,920 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:25:03,928 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:03,929 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:03,935 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:25:03,936 - root - INFO - HR Assistant started successfully
2025-07-15 04:25:04,378 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 04:25:04,577 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:25:05,334 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:25:05,891 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 04:25:06,079 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:28:38,813 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:38,814 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:38,814 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:38,906 - root - INFO - Configuration validated successfully
2025-07-15 04:28:38,908 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:38,910 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:38,911 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:38,911 - root - INFO - Configuration validated successfully
2025-07-15 04:28:38,913 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:38,915 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:38,997 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:39,000 - root - INFO - Configuration validated successfully
2025-07-15 04:28:39,004 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:39,005 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:39,008 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:39,009 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:39,013 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:39,015 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:39,305 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:39,406 - root - INFO - Configuration validated successfully
2025-07-15 04:28:39,413 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:39,498 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:39,507 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:39,510 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:39,512 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:39,605 - root - INFO - Configuration validated successfully
2025-07-15 04:28:39,613 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:39,615 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:39,616 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:39,697 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:39,900 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:40,005 - root - INFO - Configuration validated successfully
2025-07-15 04:28:40,010 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:40,097 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:40,099 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:40,102 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:40,203 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:40,298 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:40,304 - root - INFO - Configuration validated successfully
2025-07-15 04:28:40,313 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:40,397 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:40,401 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:40,405 - root - INFO - Configuration validated successfully
2025-07-15 04:28:40,404 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:40,406 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:40,412 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:40,413 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:40,413 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:40,605 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:40,809 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:40,806 - root - INFO - Configuration validated successfully
2025-07-15 04:28:40,912 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:40,914 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:40,916 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:41,003 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:41,114 - root - INFO - Configuration validated successfully
2025-07-15 04:28:41,198 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:41,301 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:41,397 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:41,403 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:41,412 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:41,505 - root - INFO - Configuration validated successfully
2025-07-15 04:28:41,510 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:41,515 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:41,597 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:41,697 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:41,907 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:42,099 - root - INFO - Configuration validated successfully
2025-07-15 04:28:42,101 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:42,108 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:42,112 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:42,116 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:42,706 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:42,801 - root - INFO - Configuration validated successfully
2025-07-15 04:28:42,808 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:42,901 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:42,904 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:42,909 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:43,500 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:43,415 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:43,700 - root - INFO - Configuration validated successfully
2025-07-15 04:28:43,714 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:43,707 - root - INFO - Configuration validated successfully
2025-07-15 04:28:43,812 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:43,798 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:43,901 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:43,898 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:43,907 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:43,919 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:44,005 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:44,199 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:44,505 - root - INFO - Configuration validated successfully
2025-07-15 04:28:44,597 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:44,509 - root - INFO - Starting HR Assistant...
2025-07-15 04:28:44,607 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:44,704 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:44,797 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:28:44,797 - root - INFO - Configuration validated successfully
2025-07-15 04:28:44,906 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:28:44,913 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:28:44,997 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:28:45,004 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:29:01,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:03,097 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:03,099 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:03,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:04,093 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:04,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:05,093 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:05,108 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:05,793 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:06,497 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:06,407 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:06,703 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:07,100 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:07,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,394 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,394 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,403 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:07,396 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,400 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,495 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:07,695 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:07,692 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:07,704 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:07,705 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:07,706 - root - INFO - Loading groq LLM...
2025-07-15 04:29:07,796 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:07,897 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,899 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:07,897 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:07,909 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:07,912 - root - INFO - Loading groq LLM...
2025-07-15 04:29:07,905 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:08,003 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,994 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:07,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:08,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:08,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:08,297 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:08,415 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:08,493 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:08,609 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:08,800 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:08,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:08,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:08,896 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:08,912 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:08,907 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:09,003 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:09,004 - root - INFO - Loading groq LLM...
2025-07-15 04:29:09,093 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,201 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,298 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,394 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:09,394 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,494 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:09,594 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,598 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:09,698 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:09,611 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,613 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,686 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,694 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,699 - root - INFO - Loading groq LLM...
2025-07-15 04:29:09,704 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,793 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:09,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:09,894 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:09,908 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:09,996 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:09,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:10,002 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:10,008 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:10,093 - root - INFO - Loading groq LLM...
2025-07-15 04:29:10,013 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:10,193 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:10,197 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:10,194 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:10,199 - root - INFO - Loading groq LLM...
2025-07-15 04:29:10,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:10,401 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:10,502 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:10,600 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:10,605 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:10,607 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:10,692 - root - INFO - Loading groq LLM...
2025-07-15 04:29:10,781 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:10,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:10,894 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:10,999 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:11,001 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:11,093 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,109 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:11,107 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:11,199 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:11,296 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:11,293 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,303 - root - INFO - Loading groq LLM...
2025-07-15 04:29:11,393 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,493 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,399 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:11,494 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,506 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:11,501 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:11,599 - root - INFO - Loading groq LLM...
2025-07-15 04:29:11,509 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:11,596 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,701 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:11,597 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:11,795 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,597 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:11,798 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:11,896 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:11,806 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:11,898 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:11,900 - root - INFO - Loading groq LLM...
2025-07-15 04:29:11,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:11,998 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:12,003 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:12,096 - root - INFO - Loading groq LLM...
2025-07-15 04:29:12,099 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:12,104 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:12,196 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:12,201 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:12,194 - root - INFO - Loading groq LLM...
2025-07-15 04:29:12,394 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:12,400 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:12,408 - root - INFO - Loading groq LLM...
2025-07-15 04:29:12,497 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:12,696 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:12,704 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:12,793 - root - INFO - Loading groq LLM...
2025-07-15 04:29:12,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:13,200 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:13,395 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:13,606 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:13,698 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:13,794 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:13,797 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:13,800 - root - INFO - Loading groq LLM...
2025-07-15 04:29:13,902 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:14,014 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:14,096 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:14,195 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:14,199 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:14,202 - root - INFO - Loading groq LLM...
2025-07-15 04:29:14,894 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:14,895 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:14,908 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:15,201 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:16,105 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:16,109 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:16,211 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:16,304 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:16,392 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:16,395 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:16,508 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:16,993 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:17,100 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:17,107 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:17,195 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:29:17,402 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:17,495 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:29:17,599 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:17,693 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:17,697 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:17,706 - root - INFO - Loading groq LLM...
2025-07-15 04:29:18,000 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:18,099 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:18,097 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:18,103 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:18,203 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:29:18,107 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:18,296 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:18,202 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:18,298 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:18,496 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:18,498 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:18,504 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:18,593 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:18,605 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:18,609 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:18,694 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:18,808 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:18,905 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:18,908 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:19,008 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:19,095 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:19,111 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:19,112 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:19,209 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:19,393 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:19,406 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:19,408 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:19,512 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:19,534 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:19,593 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:19,595 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:19,793 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:19,800 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:19,817 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:19,819 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:19,928 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:19,937 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:19,938 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:19,983 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:20,075 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:20,097 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:20,132 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:20,142 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:20,143 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:20,153 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:20,198 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:20,205 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:20,233 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:20,283 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:20,294 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:20,295 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:20,474 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:29:20,576 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 04:29:20,578 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:20,633 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:20,645 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:20,646 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:20,775 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:20,869 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:29:20,909 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:29:20,920 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:29:20,922 - root - INFO - HR Assistant started successfully
2025-07-15 04:29:21,027 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 04:29:21,211 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:29:34,628 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:29:34,630 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:29:43,216 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:29:43,217 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:33:16,100 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:16,107 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:16,211 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:16,406 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:16,701 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:17,498 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:49,686 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:49,791 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:50,087 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:50,481 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:50,882 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:51,075 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:51,192 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:51,385 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:51,478 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:51,772 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:52,275 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:52,491 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:52,587 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:52,689 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:52,794 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:52,974 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:53,388 - root - INFO - Starting HR Assistant...
2025-07-15 04:33:53,901 - root - INFO - Configuration validated successfully
2025-07-15 04:33:53,971 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:53,975 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:53,979 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:53,986 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:54,278 - root - INFO - Configuration validated successfully
2025-07-15 04:33:54,280 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:54,284 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:54,288 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:54,292 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:55,077 - root - INFO - Configuration validated successfully
2025-07-15 04:33:55,079 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:55,086 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:55,090 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:55,092 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:55,574 - root - INFO - Configuration validated successfully
2025-07-15 04:33:55,575 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:55,577 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:55,578 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:55,579 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:55,890 - root - INFO - Configuration validated successfully
2025-07-15 04:33:55,897 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:55,971 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:55,975 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:55,981 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:56,377 - root - INFO - Configuration validated successfully
2025-07-15 04:33:56,382 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:56,387 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:56,389 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:56,400 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:56,471 - root - INFO - Configuration validated successfully
2025-07-15 04:33:56,477 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:56,485 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:56,487 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:56,490 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:56,588 - root - INFO - Configuration validated successfully
2025-07-15 04:33:56,589 - root - INFO - Configuration validated successfully
2025-07-15 04:33:56,592 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:56,598 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:56,671 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:56,685 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:56,678 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:56,691 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:56,694 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:56,705 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:56,901 - root - INFO - Configuration validated successfully
2025-07-15 04:33:56,971 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:56,974 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:56,982 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:56,989 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:57,102 - root - INFO - Configuration validated successfully
2025-07-15 04:33:57,171 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:57,173 - root - INFO - Configuration validated successfully
2025-07-15 04:33:57,174 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:57,188 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:57,180 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:57,271 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:57,273 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:57,280 - root - INFO - Configuration validated successfully
2025-07-15 04:33:57,288 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:57,293 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:57,371 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:57,374 - root - INFO - Configuration validated successfully
2025-07-15 04:33:57,374 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:57,393 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:57,397 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:57,395 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:57,401 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:57,477 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:57,482 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:57,496 - root - INFO - Configuration validated successfully
2025-07-15 04:33:57,501 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:57,572 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:57,571 - root - INFO - Configuration validated successfully
2025-07-15 04:33:57,577 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:57,580 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:57,585 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:57,587 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:57,594 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:57,599 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:33:57,978 - root - INFO - Configuration validated successfully
2025-07-15 04:33:57,982 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 04:33:57,988 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 04:33:57,994 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 04:33:57,999 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 04:34:22,980 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:23,067 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:23,169 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:24,369 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:24,780 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:24,787 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:24,983 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:25,169 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:25,673 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:25,773 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:26,073 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:26,175 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:26,184 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:26,706 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:26,777 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:27,070 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:27,171 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:27,371 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:27,677 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:27,774 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:27,873 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,128 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,128 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,225 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,358 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,372 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,382 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,444 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,445 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,493 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,546 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,578 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,640 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,755 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:28,757 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:28,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,033 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,068 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,149 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,193 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,205 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,226 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,262 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,321 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,385 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,412 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,510 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,563 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,563 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,583 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,585 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,763 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,864 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:29,976 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:30,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:30,260 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:30,262 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:30,376 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:30,387 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:30,468 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:30,479 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:30,571 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:30,581 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:30,586 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:30,596 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:31,068 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:31,094 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:31,096 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:31,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:31,363 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:31,377 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:31,565 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:31,589 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:31,596 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:32,063 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:32,562 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:32,567 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:32,662 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:32,670 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:32,688 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:32,684 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:32,778 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:33,287 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:33,362 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:33,366 - root - INFO - Loading groq LLM...
2025-07-15 04:34:33,494 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:33,496 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:33,563 - root - INFO - Loading groq LLM...
2025-07-15 04:34:33,589 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:33,662 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:33,667 - root - INFO - Loading groq LLM...
2025-07-15 04:34:33,885 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:33,887 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:33,962 - root - INFO - Loading groq LLM...
2025-07-15 04:34:36,473 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:36,664 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:36,674 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:36,676 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:36,862 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:36,868 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:36,869 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:36,878 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:36,880 - root - INFO - Loading groq LLM...
2025-07-15 04:34:36,983 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:37,266 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:37,581 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:37,867 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:37,874 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:38,082 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:38,366 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:38,385 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:38,682 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:38,776 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:39,068 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:39,074 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:39,080 - root - INFO - Loading groq LLM...
2025-07-15 04:34:39,381 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 04:34:39,469 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 04:34:39,665 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:39,665 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:39,670 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:39,762 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:39,771 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:39,772 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:39,862 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:39,868 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:39,878 - root - INFO - Loading groq LLM...
2025-07-15 04:34:39,878 - root - INFO - Loading groq LLM...
2025-07-15 04:34:39,976 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:39,976 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:39,977 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:40,071 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:40,076 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:40,077 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:40,079 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:40,080 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:40,162 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:40,168 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:40,171 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:40,173 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:40,669 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:40,674 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:40,768 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:40,776 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:41,462 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:41,764 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:41,874 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:41,876 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:43,252 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:43,383 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:43,384 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:43,385 - root - INFO - Loading groq LLM...
2025-07-15 04:34:43,462 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:43,462 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:43,463 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:43,464 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:43,465 - root - INFO - Loading groq LLM...
2025-07-15 04:34:43,466 - root - INFO - Loading groq LLM...
2025-07-15 04:34:43,472 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:43,474 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:43,477 - root - INFO - Loading groq LLM...
2025-07-15 04:34:43,484 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:43,485 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:43,487 - root - INFO - Loading groq LLM...
2025-07-15 04:34:43,563 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 04:34:43,580 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:43,581 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:43,580 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:43,662 - root - INFO - Loading groq LLM...
2025-07-15 04:34:43,668 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:43,671 - root - INFO - Loading groq LLM...
2025-07-15 04:34:43,770 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:43,969 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:43,969 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 04:34:44,062 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:44,077 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:44,168 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:44,263 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 04:34:44,263 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:44,268 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:44,277 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:44,463 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 04:34:44,473 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:44,668 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:44,662 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:44,762 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:44,768 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:44,769 - root - INFO - Loading groq LLM...
2025-07-15 04:34:44,779 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:44,872 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:45,075 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:45,162 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 04:34:45,165 - root - INFO - Loading groq LLM...
2025-07-15 04:34:47,360 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,385 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,393 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,393 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,400 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,412 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,471 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,480 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,481 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,490 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,497 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,498 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,541 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,568 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,577 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,580 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,703 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,707 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,730 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,735 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,737 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,738 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,742 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,776 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,779 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,806 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,813 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,814 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,829 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,855 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,863 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,865 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:47,885 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 04:34:47,912 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 04:34:47,919 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 04:34:47,920 - root - INFO - HR Assistant started successfully
2025-07-15 04:34:53,538 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 04:34:53,539 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:51:45,304 - root - INFO - Starting HR Assistant...
2025-07-14 21:51:45,455 - root - INFO - Configuration validated successfully
2025-07-14 21:51:45,455 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 21:51:45,455 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 21:51:45,458 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 21:51:45,458 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 21:51:51,213 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:51:51,781 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:51:52,008 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:51:53,919 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:51:54,097 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 21:51:54,105 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 21:51:54,265 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:51:54,266 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:51:54,266 - root - INFO - Loading groq LLM...
2025-07-14 21:51:58,443 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 21:51:58,468 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 21:51:58,469 - ingestion_retrieval.retrieval - ERROR - Error in get_hr_assistant_chain: Prompt must accept context as an input variable. Received prompt with input variables: ["'hr_routing_prompt'"]
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 341, in get_hr_assistant_chain
    qa_chain = create_stuff_documents_chain(llm, hr_prompt)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\chains\combine_documents\stuff.py", line 79, in create_stuff_documents_chain
    _validate_prompt(prompt, document_variable_name)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\chains\combine_documents\base.py", line 27, in _validate_prompt
    raise ValueError(
ValueError: Prompt must accept context as an input variable. Received prompt with input variables: ["'hr_routing_prompt'"]
2025-07-14 21:51:58,471 - root - ERROR - Startup failed: Prompt must accept context as an input variable. Received prompt with input variables: ["'hr_routing_prompt'"]
Traceback (most recent call last):
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\main.py", line 126, in startup_event
    qa_chain = get_cached_hr_assistant_chain(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 152, in get_cached_hr_assistant_chain
    return get_hr_assistant_chain(qdrant_url, qdrant_api, collection_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\ingestion_retrieval\retrieval.py", line 341, in get_hr_assistant_chain
    qa_chain = create_stuff_documents_chain(llm, hr_prompt)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\chains\combine_documents\stuff.py", line 79, in create_stuff_documents_chain
    _validate_prompt(prompt, document_variable_name)
  File "C:\Users\user\Desktop\QD_HR_ASSISTANT\.venv\Lib\site-packages\langchain\chains\combine_documents\base.py", line 27, in _validate_prompt
    raise ValueError(
ValueError: Prompt must accept context as an input variable. Received prompt with input variables: ["'hr_routing_prompt'"]
2025-07-14 21:53:04,529 - root - INFO - Starting HR Assistant...
2025-07-14 21:53:04,689 - root - INFO - Configuration validated successfully
2025-07-14 21:53:04,692 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 21:53:04,692 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 21:53:04,692 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 21:53:04,692 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 21:53:08,632 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:53:09,185 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:53:09,369 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:53:11,780 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:53:12,016 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 21:53:12,024 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 21:53:12,174 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:53:12,174 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:53:12,176 - root - INFO - Loading groq LLM...
2025-07-14 21:53:15,401 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 21:53:15,420 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 21:53:15,460 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 21:53:15,460 - root - INFO - HR Assistant started successfully
2025-07-14 21:53:23,147 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:53:23,392 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:53:23,969 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:53:24,379 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:53:24,381 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:53:24,429 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:53:24,429 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:53:24,429 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:53:24,431 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:53:24,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:53:53,592 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:53:54,580 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:53:54,836 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:53:54,841 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:53:54,841 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:53:54,842 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:53:54,842 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:54:23,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:54:25,613 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:54:26,233 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:54:26,239 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:54:26,240 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:54:26,241 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:54:26,241 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:03,484 - root - INFO - Starting HR Assistant...
2025-07-14 21:55:03,648 - root - INFO - Configuration validated successfully
2025-07-14 21:55:03,648 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 21:55:03,650 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 21:55:03,650 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 21:55:03,651 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 21:55:08,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:55:09,068 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:55:09,377 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:55:12,690 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:55:12,843 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 21:55:12,851 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 21:55:13,004 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:55:13,040 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:13,041 - root - INFO - Loading groq LLM...
2025-07-14 21:55:16,989 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 21:55:17,008 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 21:55:17,012 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 21:55:17,013 - root - INFO - HR Assistant started successfully
2025-07-14 21:55:20,302 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:55:21,610 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:55:21,836 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:55:21,840 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:55:21,840 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:21,840 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:55:21,842 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:24,324 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:55:25,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:55:25,721 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:55:25,728 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:55:25,728 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:25,729 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:55:25,730 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:25,887 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:55:25,888 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:25,889 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:55:25,904 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:55:29,095 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:55:29,693 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:55:31,160 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:55:31,729 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:55:31,730 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:55:32,140 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:57:20,204 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:57:20,207 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:57:48,162 - root - INFO - Starting HR Assistant...
2025-07-14 21:57:48,307 - root - INFO - Configuration validated successfully
2025-07-14 21:57:48,313 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 21:57:48,313 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 21:57:48,313 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 21:57:48,313 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 21:57:53,476 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:57:55,938 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:57:56,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:57:58,782 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:57:58,944 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 21:57:58,951 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 21:57:59,111 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:57:59,111 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:57:59,111 - root - INFO - Loading groq LLM...
2025-07-14 21:58:01,915 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 21:58:01,931 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 21:58:01,936 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 21:58:01,936 - root - INFO - HR Assistant started successfully
2025-07-14 21:58:09,495 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:58:09,869 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 21:58:12,355 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:58:12,763 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:58:12,766 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 21:58:13,177 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 21:58:13,197 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:58:13,198 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:58:13,203 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:58:13,209 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 21:58:23,174 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 21:58:23,175 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:04:57,357 - root - INFO - Starting HR Assistant...
2025-07-14 22:04:57,519 - root - INFO - Configuration validated successfully
2025-07-14 22:04:57,519 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:04:57,519 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:04:57,520 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:04:57,520 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:05:03,132 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:05:03,868 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:05:04,279 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:05:06,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:05:06,576 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:05:06,585 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:05:06,743 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:05:06,743 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:05:06,745 - root - INFO - Loading groq LLM...
2025-07-14 22:05:10,207 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:05:10,271 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:05:10,276 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:05:10,276 - root - INFO - HR Assistant started successfully
2025-07-14 22:05:33,847 - root - INFO - Starting HR Assistant...
2025-07-14 22:05:34,018 - root - INFO - Configuration validated successfully
2025-07-14 22:05:34,018 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:05:34,023 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:05:34,023 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:05:34,023 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:05:39,674 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:05:40,987 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:05:41,398 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:05:44,440 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:05:44,616 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:05:44,623 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:05:44,775 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:05:44,779 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:05:44,779 - root - INFO - Loading groq LLM...
2025-07-14 22:05:48,143 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:05:48,162 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:05:48,169 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:05:48,169 - root - INFO - HR Assistant started successfully
2025-07-14 22:06:22,240 - root - INFO - Starting HR Assistant...
2025-07-14 22:06:22,398 - root - INFO - Configuration validated successfully
2025-07-14 22:06:22,398 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:06:22,398 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:06:22,398 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:06:22,398 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:06:27,418 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:06:28,080 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:06:28,269 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:06:29,968 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:06:30,132 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:06:30,140 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:06:30,290 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:06:30,290 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:06:30,290 - root - INFO - Loading groq LLM...
2025-07-14 22:06:33,020 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:06:33,042 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:06:33,044 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:06:33,047 - root - INFO - HR Assistant started successfully
2025-07-14 22:06:43,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:06:43,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:06:44,534 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:06:44,535 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:06:44,781 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:06:44,781 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:06:44,794 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:06:44,797 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:06:44,797 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:06:44,800 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:06:51,674 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:06:51,674 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:07:18,919 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:07:18,919 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:08:28,092 - root - INFO - Starting HR Assistant...
2025-07-14 22:08:28,247 - root - INFO - Configuration validated successfully
2025-07-14 22:08:28,248 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:08:28,248 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:08:28,248 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:08:28,248 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:08:33,023 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:08:34,010 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:08:34,257 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:08:36,479 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:08:36,632 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:08:36,639 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:08:36,803 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:08:36,803 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:08:36,804 - root - INFO - Loading groq LLM...
2025-07-14 22:08:39,868 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:08:39,882 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:08:39,889 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:08:39,890 - root - INFO - HR Assistant started successfully
2025-07-14 22:10:42,131 - root - INFO - Starting HR Assistant...
2025-07-14 22:10:42,275 - root - INFO - Configuration validated successfully
2025-07-14 22:10:42,277 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:10:42,277 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:10:42,277 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:10:42,277 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:10:46,409 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:10:47,151 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:10:47,398 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:10:49,207 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:10:49,371 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:10:49,378 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:10:49,529 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:10:49,531 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:10:49,531 - root - INFO - Loading groq LLM...
2025-07-14 22:10:52,174 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:10:52,192 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:10:52,197 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:10:52,198 - root - INFO - HR Assistant started successfully
2025-07-14 22:11:38,688 - root - INFO - Starting HR Assistant...
2025-07-14 22:11:38,844 - root - INFO - Configuration validated successfully
2025-07-14 22:11:38,844 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:11:38,845 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:11:38,845 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:11:38,845 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:11:43,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:11:44,508 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:11:44,840 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:11:46,408 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:11:46,592 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:11:46,597 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:11:46,768 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:11:46,770 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:11:46,770 - root - INFO - Loading groq LLM...
2025-07-14 22:11:49,398 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:11:49,418 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:11:49,421 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:11:49,424 - root - INFO - HR Assistant started successfully
2025-07-14 22:12:01,507 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:12:01,507 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:12:02,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:12:02,167 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:12:02,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:12:02,412 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:12:02,421 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:12:02,421 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:12:02,423 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:12:02,423 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:12:12,729 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:12:12,729 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:12:25,743 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:12:25,743 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:12:30,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:12:31,302 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:12:31,504 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:12:31,512 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:12:31,516 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:12:31,516 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:12:31,516 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:13:01,667 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:13:02,317 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:13:02,500 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:13:02,506 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:13:02,506 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:13:02,507 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:13:02,507 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:13:32,097 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:13:32,986 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:13:33,204 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:13:33,210 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:13:33,213 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:13:33,213 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:13:33,214 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:13:53,625 - root - INFO - Starting HR Assistant...
2025-07-14 22:13:53,781 - root - INFO - Configuration validated successfully
2025-07-14 22:13:53,782 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:13:53,782 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:13:53,782 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:13:53,782 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:13:57,988 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:13:58,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:13:58,974 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:14:00,392 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:14:00,554 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:14:00,559 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:14:00,718 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:00,721 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:00,721 - root - INFO - Loading groq LLM...
2025-07-14 22:14:03,092 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:14:03,109 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:14:03,115 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:14:03,115 - root - INFO - HR Assistant started successfully
2025-07-14 22:14:05,489 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:14:06,278 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:14:06,469 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:14:06,472 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:06,474 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:06,474 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:06,474 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:23,426 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:14:23,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:14:24,422 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:14:24,854 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:14:24,857 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:14:24,862 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:24,863 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:24,863 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:24,863 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:25,095 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:14:27,529 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:27,529 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:39,384 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:39,386 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:54,084 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:14:54,978 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:14:55,314 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:14:55,319 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:55,319 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:14:55,319 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:14:55,319 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:23,847 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:15:24,974 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:15:25,339 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:15:25,346 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:15:25,347 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:25,347 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:15:25,348 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:38,154 - root - INFO - Starting HR Assistant...
2025-07-14 22:15:38,335 - root - INFO - Configuration validated successfully
2025-07-14 22:15:38,335 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:15:38,335 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:15:38,335 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:15:38,338 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:15:42,642 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:15:43,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:15:44,308 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:15:45,997 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:15:46,163 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:15:46,169 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:15:46,324 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:15:46,324 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:46,324 - root - INFO - Loading groq LLM...
2025-07-14 22:15:49,432 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:15:49,452 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:15:49,456 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:15:49,458 - root - INFO - HR Assistant started successfully
2025-07-14 22:15:53,322 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:15:54,174 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:15:54,498 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:15:54,504 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:15:54,504 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:54,504 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:15:54,505 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:58,185 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:15:58,597 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:15:59,358 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:15:59,624 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:15:59,626 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:15:59,632 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:15:59,632 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:59,634 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:15:59,634 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:15:59,948 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:16:28,716 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:16:29,479 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:16:29,792 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:16:29,798 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:16:29,800 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:16:29,801 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:16:29,801 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:16:54,285 - root - INFO - Starting HR Assistant...
2025-07-14 22:16:54,438 - root - INFO - Configuration validated successfully
2025-07-14 22:16:54,438 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:16:54,438 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:16:54,440 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:16:54,440 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:16:58,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:16:59,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:16:59,792 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:17:02,244 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:17:02,443 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:17:02,452 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:17:02,607 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:17:02,608 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:17:02,608 - root - INFO - Loading groq LLM...
2025-07-14 22:17:05,716 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:17:05,736 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:17:05,740 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:17:05,742 - root - INFO - HR Assistant started successfully
2025-07-14 22:17:18,676 - root - INFO - Starting HR Assistant...
2025-07-14 22:17:18,829 - root - INFO - Configuration validated successfully
2025-07-14 22:17:18,831 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:17:18,831 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:17:18,831 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:17:18,831 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:17:23,361 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:17:24,435 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:17:24,679 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:17:26,982 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:17:27,147 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:17:27,155 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:17:27,313 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:17:27,313 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:17:27,313 - root - INFO - Loading groq LLM...
2025-07-14 22:17:30,011 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:17:30,024 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:17:30,032 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:17:30,032 - root - INFO - HR Assistant started successfully
2025-07-14 22:17:30,191 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:17:30,192 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:17:41,348 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:17:42,287 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:17:42,529 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:17:42,546 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:17:42,547 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:17:42,548 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:17:42,548 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:17:44,669 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:17:45,656 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:17:45,982 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:18:09,242 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:18:09,243 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:18:09,243 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:18:09,243 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:18:11,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:18:12,189 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:18:12,438 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:18:25,194 - root - INFO - Starting HR Assistant...
2025-07-14 22:18:25,352 - root - INFO - Configuration validated successfully
2025-07-14 22:18:25,354 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:18:25,354 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:18:25,354 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:18:25,354 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:18:30,405 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:18:31,142 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:18:31,490 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:18:32,964 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:18:33,147 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:18:33,157 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:18:33,332 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:18:33,334 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:18:33,335 - root - INFO - Loading groq LLM...
2025-07-14 22:18:36,045 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:18:36,059 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:18:36,067 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:18:36,068 - root - INFO - HR Assistant started successfully
2025-07-14 22:18:39,247 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:18:39,247 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:18:39,247 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:18:39,247 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:18:42,311 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:18:43,551 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:18:43,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:18:59,774 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:19:00,431 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:19:00,679 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:19:00,702 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:19:00,702 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:19:00,702 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:19:00,704 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:19:03,240 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:19:03,900 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:19:04,085 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:19:43,792 - root - INFO - Starting HR Assistant...
2025-07-14 22:19:43,947 - root - INFO - Configuration validated successfully
2025-07-14 22:19:43,948 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:19:43,948 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:19:43,949 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:19:43,949 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:19:48,750 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:19:49,732 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:19:49,914 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:19:52,201 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:19:52,385 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:19:52,393 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:19:52,575 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:19:52,575 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:19:52,575 - root - INFO - Loading groq LLM...
2025-07-14 22:19:55,510 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:19:55,530 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:19:55,534 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:19:55,534 - root - INFO - HR Assistant started successfully
2025-07-14 22:20:14,001 - root - INFO - Starting HR Assistant...
2025-07-14 22:20:14,153 - root - INFO - Configuration validated successfully
2025-07-14 22:20:14,154 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:20:14,154 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:20:14,154 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:20:14,154 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:20:19,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:20:19,919 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:20:20,278 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:20:22,029 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:20:22,186 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:20:22,193 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:20:22,353 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:20:22,353 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:20:22,354 - root - INFO - Loading groq LLM...
2025-07-14 22:20:26,074 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:20:26,096 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:20:26,099 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:20:26,099 - root - INFO - HR Assistant started successfully
2025-07-14 22:20:28,888 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:20:28,889 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:20:29,710 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:20:29,710 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:20:30,054 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:20:30,056 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:20:30,066 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:20:30,066 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:20:30,067 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:20:30,067 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:20:43,151 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:20:43,152 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:20:44,061 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:20:44,061 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:20:44,400 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:20:44,401 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:20:44,404 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:20:44,404 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:20:44,407 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:20:44,407 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:21:38,539 - root - INFO - Starting HR Assistant...
2025-07-14 22:21:38,707 - root - INFO - Configuration validated successfully
2025-07-14 22:21:38,708 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:21:38,708 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:21:38,708 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:21:38,708 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:21:44,151 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:21:45,309 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:21:45,635 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:21:47,690 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:21:47,850 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:21:47,858 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:21:48,011 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:21:48,011 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:21:48,011 - root - INFO - Loading groq LLM...
2025-07-14 22:21:50,735 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:21:50,755 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:21:50,760 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:21:50,760 - root - INFO - HR Assistant started successfully
2025-07-14 22:22:02,336 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:22:03,404 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:22:03,653 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:22:03,661 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:22:03,665 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:22:03,667 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:22:03,668 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:22:05,872 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:22:06,457 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:22:06,859 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:22:22,448 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:22:22,456 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:22:23,435 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:22:23,435 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:22:23,683 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:22:23,684 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:22:23,689 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:22:23,691 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:22:23,691 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:22:23,691 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:24:40,858 - root - INFO - Starting HR Assistant...
2025-07-14 22:24:41,018 - root - INFO - Configuration validated successfully
2025-07-14 22:24:41,018 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:24:41,018 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:24:41,021 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:24:41,021 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:24:46,041 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:24:46,819 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:24:47,142 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:24:49,795 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:24:49,945 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:24:49,952 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:24:50,105 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:24:50,107 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:24:50,107 - root - INFO - Loading groq LLM...
2025-07-14 22:24:52,841 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:24:52,858 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:24:52,863 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:24:52,863 - root - INFO - HR Assistant started successfully
2025-07-14 22:24:57,518 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:24:57,532 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:24:57,532 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:24:57,532 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:25:00,362 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:25:00,690 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:25:01,101 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:25:01,526 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:25:01,527 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:25:01,939 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:25:10,982 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:25:10,982 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:28:25,469 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:28:25,471 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:28:57,469 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:28:57,469 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:28:57,469 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:28:57,469 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:28:59,790 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:29:00,419 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:29:00,669 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:29:29,616 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:29:30,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:29:30,844 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:29:30,851 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:29:30,851 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:29:30,851 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:29:30,851 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:32:25,962 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:32:25,962 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:32:45,632 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-14 22:32:45,635 - admin_interface - INFO - Successfully updated .env file
2025-07-14 22:32:51,987 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:32:51,987 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:37:58,107 - root - INFO - Starting HR Assistant...
2025-07-14 22:37:58,259 - root - INFO - Configuration validated successfully
2025-07-14 22:37:58,263 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:37:58,263 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:37:58,263 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:37:58,263 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:38:03,312 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:38:04,135 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:38:04,549 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:38:07,008 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:38:07,166 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:38:07,174 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:38:07,329 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:38:07,331 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:38:07,331 - root - INFO - Loading groq LLM...
2025-07-14 22:38:10,152 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:38:10,172 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:38:10,175 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:38:10,175 - root - INFO - HR Assistant started successfully
2025-07-14 22:40:29,482 - root - INFO - Starting HR Assistant...
2025-07-14 22:40:29,670 - root - INFO - Configuration validated successfully
2025-07-14 22:40:29,672 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:40:29,672 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:40:29,672 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:40:29,672 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:40:35,165 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:40:36,182 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:40:36,374 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:40:38,268 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:40:38,432 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:40:38,440 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:40:38,604 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:40:38,606 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:40:38,606 - root - INFO - Loading groq LLM...
2025-07-14 22:40:41,023 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:40:41,040 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:40:41,044 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:40:41,044 - root - INFO - HR Assistant started successfully
2025-07-14 22:41:13,617 - root - INFO - Starting HR Assistant...
2025-07-14 22:41:13,795 - root - INFO - Configuration validated successfully
2025-07-14 22:41:13,795 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:41:13,795 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:41:13,795 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:41:13,797 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:41:18,247 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:41:19,248 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:41:19,565 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:41:20,694 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:41:20,889 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:41:20,898 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:41:21,057 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:41:21,057 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:41:21,057 - root - INFO - Loading groq LLM...
2025-07-14 22:41:23,245 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:41:23,263 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:41:23,268 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:41:23,268 - root - INFO - HR Assistant started successfully
2025-07-14 22:41:43,375 - root - INFO - Starting HR Assistant...
2025-07-14 22:41:43,557 - root - INFO - Configuration validated successfully
2025-07-14 22:41:43,557 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:41:43,557 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:41:43,557 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:41:43,557 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:41:47,900 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:41:48,837 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:41:49,356 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:41:50,903 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:41:51,068 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:41:51,076 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:41:51,241 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:41:51,242 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:41:51,242 - root - INFO - Loading groq LLM...
2025-07-14 22:41:54,835 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:41:54,855 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:41:54,859 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:41:54,859 - root - INFO - HR Assistant started successfully
2025-07-14 22:42:28,354 - root - INFO - Starting HR Assistant...
2025-07-14 22:42:28,538 - root - INFO - Configuration validated successfully
2025-07-14 22:42:28,538 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:42:28,540 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:42:28,540 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:42:28,540 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:42:33,626 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:42:34,374 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:42:34,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:42:36,354 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:42:36,563 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:42:36,573 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:42:36,758 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:42:36,758 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:42:36,758 - root - INFO - Loading groq LLM...
2025-07-14 22:42:39,469 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:42:39,488 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:42:39,494 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:42:39,495 - root - INFO - HR Assistant started successfully
2025-07-14 22:43:54,640 - root - INFO - Starting HR Assistant...
2025-07-14 22:43:54,807 - root - INFO - Configuration validated successfully
2025-07-14 22:43:54,807 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:43:54,807 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:43:54,807 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:43:54,813 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:43:59,576 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:44:00,482 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:00,829 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:02,658 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:02,810 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:44:02,818 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:44:02,982 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:44:02,982 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:44:02,982 - root - INFO - Loading groq LLM...
2025-07-14 22:44:05,553 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:44:05,572 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:44:05,576 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:44:05,578 - root - INFO - HR Assistant started successfully
2025-07-14 22:44:17,781 - root - INFO - Starting HR Assistant...
2025-07-14 22:44:17,938 - root - INFO - Configuration validated successfully
2025-07-14 22:44:17,938 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:44:17,940 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:44:17,940 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:44:17,940 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:44:22,513 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:44:23,188 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:23,368 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:25,242 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:25,399 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:44:25,406 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:44:25,566 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:44:25,568 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:44:25,568 - root - INFO - Loading groq LLM...
2025-07-14 22:44:27,875 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:44:27,892 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:44:27,899 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:44:27,900 - root - INFO - HR Assistant started successfully
2025-07-14 22:44:45,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:44:45,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:44:46,463 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:44:46,466 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:44:46,751 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:46,751 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:44:46,765 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:44:46,765 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:44:46,767 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:44:46,768 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:45:03,877 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-14 22:45:03,879 - admin_interface - INFO - Successfully updated .env file
2025-07-14 22:46:30,032 - root - INFO - Starting HR Assistant...
2025-07-14 22:46:30,188 - root - INFO - Configuration validated successfully
2025-07-14 22:46:30,188 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:46:30,188 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:46:30,190 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:46:30,191 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:46:35,723 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:46:37,438 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:46:37,698 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:46:39,592 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:46:39,751 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:46:39,758 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:46:39,932 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:46:39,932 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:46:39,932 - root - INFO - Loading groq LLM...
2025-07-14 22:46:44,373 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:46:44,396 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:46:44,401 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:46:44,401 - root - INFO - HR Assistant started successfully
2025-07-14 22:47:05,389 - root - INFO - Starting HR Assistant...
2025-07-14 22:47:05,561 - root - INFO - Configuration validated successfully
2025-07-14 22:47:05,561 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:47:05,561 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:47:05,561 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:47:05,561 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:47:11,933 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:47:13,498 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:47:13,908 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:47:16,050 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:47:16,229 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:47:16,237 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:47:16,395 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:47:16,395 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:47:16,396 - root - INFO - Loading groq LLM...
2025-07-14 22:47:19,092 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:47:19,111 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:47:19,116 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:47:19,118 - root - INFO - HR Assistant started successfully
2025-07-14 22:47:31,198 - root - INFO - Starting HR Assistant...
2025-07-14 22:47:31,351 - root - INFO - Configuration validated successfully
2025-07-14 22:47:31,354 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:47:31,355 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:47:31,355 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:47:31,357 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:47:35,309 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:47:36,541 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:47:36,874 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:47:39,425 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:47:39,651 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:47:39,657 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:47:39,812 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:47:39,813 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:47:39,813 - root - INFO - Loading groq LLM...
2025-07-14 22:47:42,408 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:47:42,432 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:47:42,436 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:47:42,436 - root - INFO - HR Assistant started successfully
2025-07-14 22:49:29,312 - root - INFO - Starting HR Assistant...
2025-07-14 22:49:29,457 - root - INFO - Configuration validated successfully
2025-07-14 22:49:29,457 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 22:49:29,464 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 22:49:29,464 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 22:49:29,464 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 22:49:34,597 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:49:35,410 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:49:35,612 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:49:36,879 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:49:37,041 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 22:49:37,048 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 22:49:37,225 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:49:37,225 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:49:37,225 - root - INFO - Loading groq LLM...
2025-07-14 22:49:40,337 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 22:49:40,359 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 22:49:40,366 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 22:49:40,366 - root - INFO - HR Assistant started successfully
2025-07-14 22:49:49,610 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:49:49,611 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:49:49,613 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 22:49:49,615 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 22:49:52,107 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:49:52,443 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 22:49:53,343 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:49:53,605 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 22:49:53,605 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 22:49:54,083 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:03:31,318 - root - INFO - Starting HR Assistant...
2025-07-14 23:03:31,474 - root - INFO - Configuration validated successfully
2025-07-14 23:03:31,474 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:03:31,474 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:03:31,474 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:03:31,474 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:03:36,618 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:03:37,444 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:03:37,753 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:03:39,893 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:03:40,069 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:03:40,073 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:03:40,235 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:03:40,235 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:03:40,235 - root - INFO - Loading groq LLM...
2025-07-14 23:03:43,345 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:03:43,365 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:03:43,369 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:03:43,369 - root - INFO - HR Assistant started successfully
2025-07-14 23:03:52,734 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:03:53,319 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:03:53,553 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:03:53,568 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:03:53,570 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:03:53,571 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:03:53,574 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:03:55,772 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:03:56,593 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:03:57,088 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:05:17,124 - root - INFO - Starting HR Assistant...
2025-07-14 23:05:17,299 - root - INFO - Configuration validated successfully
2025-07-14 23:05:17,301 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:05:17,301 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:05:17,301 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:05:17,301 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:05:23,739 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:05:24,657 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:05:24,975 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:05:27,771 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:05:27,958 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:05:27,967 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:05:28,122 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:05:28,123 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:05:28,123 - root - INFO - Loading groq LLM...
2025-07-14 23:05:31,469 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:05:31,487 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:05:31,492 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:05:31,492 - root - INFO - HR Assistant started successfully
2025-07-14 23:07:38,882 - root - INFO - Starting HR Assistant...
2025-07-14 23:07:39,042 - root - INFO - Configuration validated successfully
2025-07-14 23:07:39,042 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:07:39,042 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:07:39,042 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:07:39,042 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:07:43,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:07:44,519 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:07:44,697 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:07:46,535 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:07:46,747 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:07:46,753 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:07:46,899 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:07:46,900 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:07:46,900 - root - INFO - Loading groq LLM...
2025-07-14 23:07:50,001 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:07:50,018 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:07:50,023 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:07:50,023 - root - INFO - HR Assistant started successfully
2025-07-14 23:08:13,357 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:08:13,379 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:08:13,382 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:08:13,387 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:08:15,832 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:08:15,866 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:08:16,586 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:08:16,586 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:08:16,818 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:08:16,818 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:08:49,783 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:08:50,488 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:08:50,675 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:08:50,682 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:08:50,692 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:08:50,695 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:08:50,696 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:08:52,632 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:08:53,630 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:08:53,868 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:10:30,597 - root - INFO - Starting HR Assistant...
2025-07-14 23:10:30,747 - root - INFO - Configuration validated successfully
2025-07-14 23:10:30,747 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:10:30,747 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:10:30,747 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:10:30,747 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:10:37,568 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:10:38,311 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:10:38,892 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:10:41,208 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:10:41,394 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:10:41,397 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:10:41,568 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:10:41,568 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:10:41,570 - root - INFO - Loading groq LLM...
2025-07-14 23:10:44,466 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:10:44,485 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:10:44,490 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:10:44,490 - root - INFO - HR Assistant started successfully
2025-07-14 23:15:14,507 - root - INFO - Starting HR Assistant...
2025-07-14 23:15:14,671 - root - INFO - Configuration validated successfully
2025-07-14 23:15:14,671 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:15:14,671 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:15:14,674 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:15:14,674 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:15:20,011 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:15:21,135 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:15:21,407 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:15:23,792 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:15:23,951 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:15:23,957 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:15:24,119 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:15:24,120 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:15:24,120 - root - INFO - Loading groq LLM...
2025-07-14 23:15:26,688 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:15:26,707 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:15:26,712 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:15:26,713 - root - INFO - HR Assistant started successfully
2025-07-14 23:15:34,741 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:15:34,981 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:15:35,719 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:15:36,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:15:36,135 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:15:36,381 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:15:36,387 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:15:36,388 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:15:36,388 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:15:36,388 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:16:06,250 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-14 23:16:06,250 - admin_interface - INFO - Successfully updated .env file
2025-07-14 23:16:29,860 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:16:30,516 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:16:30,768 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:16:30,776 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:16:30,778 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:16:30,778 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:16:30,779 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:16:32,901 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:16:33,725 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:16:33,908 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:16:42,118 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:16:42,451 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:16:43,364 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:16:43,599 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:16:43,601 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:16:43,849 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:16:43,854 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:16:43,856 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:16:43,856 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:16:43,856 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:16:52,738 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:16:52,738 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:16:53,718 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:16:53,718 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:16:53,895 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:16:53,905 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:16:53,912 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:16:53,912 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:16:53,912 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:16:53,916 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:17:06,629 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-14 23:17:06,631 - admin_interface - INFO - Successfully updated .env file
2025-07-14 23:17:17,989 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:17:17,989 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:17:18,815 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:17:18,816 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:17:19,059 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:17:19,059 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:17:19,071 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:17:19,071 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:17:19,072 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:17:19,073 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:17:27,593 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-14 23:17:27,594 - admin_interface - INFO - Successfully updated .env file
2025-07-14 23:17:53,607 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:17:53,607 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:17:55,007 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:17:55,044 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:17:55,254 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:17:55,324 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:17:55,332 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:17:55,334 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:17:55,334 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:17:55,334 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:18:58,808 - root - INFO - Starting HR Assistant...
2025-07-14 23:18:58,973 - root - INFO - Configuration validated successfully
2025-07-14 23:18:58,974 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:18:58,974 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:18:58,974 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:18:58,974 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:19:04,532 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:19:05,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:19:05,769 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:19:08,150 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:19:08,323 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:19:08,331 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:19:08,488 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:19:08,488 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:19:08,490 - root - INFO - Loading groq LLM...
2025-07-14 23:19:11,034 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:19:11,052 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:19:11,057 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:19:11,057 - root - INFO - HR Assistant started successfully
2025-07-14 23:19:18,269 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:19:18,269 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:19:19,092 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:19:19,092 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:19:19,424 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:19:19,424 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:19:19,437 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:19:19,438 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:19:19,438 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:19:19,439 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:19:58,313 - root - INFO - Starting HR Assistant...
2025-07-14 23:19:58,482 - root - INFO - Configuration validated successfully
2025-07-14 23:19:58,482 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:19:58,482 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:19:58,484 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:19:58,484 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:20:02,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:20:03,857 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:20:04,104 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:20:06,655 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:20:06,812 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:20:06,819 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:20:06,981 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:20:06,982 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:20:06,982 - root - INFO - Loading groq LLM...
2025-07-14 23:20:09,788 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:20:09,806 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:20:09,810 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:20:09,811 - root - INFO - HR Assistant started successfully
2025-07-14 23:20:23,440 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:20:23,441 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:20:24,018 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:20:24,032 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:20:24,239 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:20:24,240 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:20:24,251 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:20:24,251 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:20:24,251 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:20:24,251 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:20:53,760 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:20:54,755 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:20:55,242 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:20:55,248 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:20:55,248 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:20:55,251 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:20:55,251 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:21:23,715 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:21:24,868 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:21:25,369 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:21:25,379 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:21:25,379 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:21:25,379 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:21:25,381 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:21:53,755 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:21:54,735 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:21:54,982 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:21:54,986 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:21:54,987 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:21:54,988 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:21:54,988 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:22:23,342 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:22:24,392 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:22:24,715 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:22:24,725 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:22:24,725 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:22:24,726 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:22:24,727 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:22:53,702 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:22:54,373 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:22:54,608 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:22:54,616 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:22:54,617 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:22:54,617 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:22:54,618 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:23:23,582 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:23:24,324 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:23:24,736 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:23:24,744 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:23:24,744 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:23:24,744 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:23:24,744 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:23:53,693 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:23:55,017 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:23:55,200 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:23:55,206 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:23:55,207 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:23:55,207 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:23:55,208 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:24:07,780 - root - INFO - Starting HR Assistant...
2025-07-14 23:24:07,933 - root - INFO - Configuration validated successfully
2025-07-14 23:24:07,933 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:24:07,934 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:24:07,934 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:24:07,934 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:24:13,323 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:24:14,407 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:24:14,887 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:24:17,771 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:24:17,973 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:24:17,981 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:24:18,165 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:24:18,168 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:24:18,168 - root - INFO - Loading groq LLM...
2025-07-14 23:24:21,965 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:24:21,982 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:24:21,986 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:24:21,988 - root - INFO - HR Assistant started successfully
2025-07-14 23:24:32,016 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:24:33,154 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:24:33,466 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:24:33,477 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:24:33,478 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:24:33,479 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:24:33,479 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:24:34,572 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:24:34,585 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:24:34,587 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:24:34,588 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:24:36,939 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:24:36,940 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:24:37,609 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:24:37,609 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:24:37,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:24:37,852 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:24:45,473 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:24:45,475 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:27:00,118 - root - INFO - Starting HR Assistant...
2025-07-14 23:27:00,279 - root - INFO - Configuration validated successfully
2025-07-14 23:27:00,279 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:27:00,279 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:27:00,279 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:27:00,279 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:27:06,821 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:27:07,971 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:27:08,548 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:27:11,432 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:27:11,586 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:27:11,594 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:27:11,742 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:27:11,744 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:27:11,745 - root - INFO - Loading groq LLM...
2025-07-14 23:27:14,274 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:27:14,292 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:27:14,298 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:27:14,298 - root - INFO - HR Assistant started successfully
2025-07-14 23:27:23,372 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:27:23,372 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:27:24,525 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:27:24,525 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:27:25,017 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:27:25,018 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:27:25,032 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:27:25,032 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:27:25,035 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:27:25,036 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:29:02,048 - root - INFO - Starting HR Assistant...
2025-07-14 23:29:02,204 - root - INFO - Configuration validated successfully
2025-07-14 23:29:02,204 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:29:02,204 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:29:02,204 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:29:02,206 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:29:06,976 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:29:08,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:08,631 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:11,926 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:12,079 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:29:12,089 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:29:12,247 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:29:12,250 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:29:12,251 - root - INFO - Loading groq LLM...
2025-07-14 23:29:15,071 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:29:15,093 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:29:15,098 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:29:15,100 - root - INFO - HR Assistant started successfully
2025-07-14 23:29:27,461 - root - INFO - Starting HR Assistant...
2025-07-14 23:29:27,608 - root - INFO - Configuration validated successfully
2025-07-14 23:29:27,608 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:29:27,613 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:29:27,613 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:29:27,613 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:29:31,895 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:29:33,036 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:33,671 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:35,879 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:36,060 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:29:36,068 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:29:36,255 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:29:36,255 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:29:36,257 - root - INFO - Loading groq LLM...
2025-07-14 23:29:38,945 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:29:38,965 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:29:38,970 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:29:38,970 - root - INFO - HR Assistant started successfully
2025-07-14 23:29:51,325 - root - INFO - Starting HR Assistant...
2025-07-14 23:29:51,493 - root - INFO - Configuration validated successfully
2025-07-14 23:29:51,497 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:29:51,497 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:29:51,498 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:29:51,498 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:29:56,521 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:29:57,266 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:57,445 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:59,151 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:29:59,318 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:29:59,330 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:29:59,491 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:29:59,493 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:29:59,493 - root - INFO - Loading groq LLM...
2025-07-14 23:30:02,197 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:30:02,225 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:30:02,232 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:30:02,232 - root - INFO - HR Assistant started successfully
2025-07-14 23:30:14,974 - root - INFO - Starting HR Assistant...
2025-07-14 23:30:15,120 - root - INFO - Configuration validated successfully
2025-07-14 23:30:15,120 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:30:15,120 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:30:15,120 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:30:15,122 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:30:20,152 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:30:21,232 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:30:21,654 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:30:23,132 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:30:23,307 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:30:23,315 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:30:23,507 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:30:23,507 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:30:23,507 - root - INFO - Loading groq LLM...
2025-07-14 23:30:25,947 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:30:25,965 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:30:25,970 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:30:25,970 - root - INFO - HR Assistant started successfully
2025-07-14 23:30:31,954 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:30:32,197 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:30:33,518 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:30:33,762 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:30:33,928 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:30:34,131 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:30:34,138 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:30:34,138 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:30:34,138 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:30:34,140 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:31:35,982 - root - INFO - Starting HR Assistant...
2025-07-14 23:31:36,185 - root - INFO - Configuration validated successfully
2025-07-14 23:31:36,187 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:31:36,188 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:31:36,188 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:31:36,188 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:31:41,624 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:31:42,440 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:31:42,681 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:31:44,373 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:31:44,520 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:31:44,527 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:31:44,674 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:31:44,674 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:31:44,674 - root - INFO - Loading groq LLM...
2025-07-14 23:31:47,485 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:31:47,508 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:31:47,515 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:31:47,517 - root - INFO - HR Assistant started successfully
2025-07-14 23:31:47,702 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:31:47,702 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:32:06,189 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:32:06,190 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:32:06,905 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:32:06,907 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:32:07,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:32:07,304 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:32:07,316 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:32:07,318 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:32:07,318 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:32:07,319 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:32:39,547 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:32:39,710 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:32:40,290 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:32:40,619 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:32:40,620 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:32:40,631 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:32:40,631 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:32:40,632 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:32:40,632 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:32:40,948 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:33:01,976 - admin_interface - INFO - Created backup of .env file: .env.env.backup
2025-07-14 23:33:01,976 - admin_interface - INFO - Successfully updated .env file
2025-07-14 23:33:12,183 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:33:12,184 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:33:12,185 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:33:12,185 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:33:15,001 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:33:15,001 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:33:16,024 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:33:16,024 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:33:16,220 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:33:16,224 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:34:27,640 - root - INFO - Starting HR Assistant...
2025-07-14 23:34:27,796 - root - INFO - Configuration validated successfully
2025-07-14 23:34:27,796 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:34:27,797 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:34:27,797 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:34:27,797 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:34:32,662 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:34:33,557 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:34:33,814 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:34:35,302 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:34:35,468 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:34:35,474 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:34:35,640 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:34:35,640 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:34:35,640 - root - INFO - Loading groq LLM...
2025-07-14 23:34:38,924 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:34:38,942 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:34:38,947 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:34:38,947 - root - INFO - HR Assistant started successfully
2025-07-14 23:34:51,939 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:34:52,849 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:34:53,100 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:34:53,104 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:34:53,121 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:34:53,124 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:34:53,124 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:34:55,547 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:34:56,468 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:34:56,711 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:36:42,162 - root - INFO - Starting HR Assistant...
2025-07-14 23:36:42,318 - root - INFO - Configuration validated successfully
2025-07-14 23:36:42,318 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:36:42,318 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:36:42,324 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:36:42,324 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:36:47,294 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:36:48,119 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:36:48,365 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:36:51,088 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:36:51,221 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:36:51,229 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:36:51,369 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:36:51,369 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:36:51,369 - root - INFO - Loading groq LLM...
2025-07-14 23:36:53,907 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:36:53,926 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:36:53,929 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:36:53,929 - root - INFO - HR Assistant started successfully
2025-07-14 23:36:54,075 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:36:54,075 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:39:58,536 - root - INFO - Starting HR Assistant...
2025-07-14 23:39:58,694 - root - INFO - Configuration validated successfully
2025-07-14 23:39:58,694 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-14 23:39:58,694 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-14 23:39:58,694 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-14 23:39:58,696 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-14 23:40:04,709 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:40:05,627 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:40:05,968 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:40:08,429 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:40:08,586 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-14 23:40:08,592 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-14 23:40:08,747 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:40:08,747 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:40:08,747 - root - INFO - Loading groq LLM...
2025-07-14 23:40:11,774 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-14 23:40:11,797 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-14 23:40:11,802 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-14 23:40:11,803 - root - INFO - HR Assistant started successfully
2025-07-14 23:40:18,134 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:40:19,137 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:40:19,354 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-14 23:40:19,388 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:40:19,388 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:40:19,389 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-14 23:40:19,390 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-14 23:40:21,785 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-14 23:40:22,948 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-14 23:40:23,218 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 00:00:26,047 - root - INFO - Starting HR Assistant...
2025-07-15 00:00:26,216 - root - INFO - Configuration validated successfully
2025-07-15 00:00:26,218 - root - INFO - Pre-warming the model with optimized RAG pipeline...
2025-07-15 00:00:26,219 - ingestion_retrieval.retrieval - INFO - Invalidating all caches due to configuration change
2025-07-15 00:00:26,219 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] _llm_cache set to None; next request will reload LLM with latest config
2025-07-15 00:00:26,219 - ingestion_retrieval.retrieval - INFO - Initializing hybrid retriever...
2025-07-15 00:00:32,962 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 00:00:34,186 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 00:00:34,442 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 00:00:37,273 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 00:00:37,434 - ingestion_retrieval.retrieval - INFO - Using Google embedding model: models/embedding-001
2025-07-15 00:00:37,476 - ingestion_retrieval.retrieval - INFO - [LLM CACHE] Loading LLM due to cache miss or invalidation...
2025-07-15 00:00:37,632 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 00:00:37,633 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 00:00:37,633 - root - INFO - Loading groq LLM...
2025-07-15 00:00:40,500 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-15 00:00:40,535 - ingestion_retrieval.retrieval - INFO - LLM warm-up completed successfully
2025-07-15 00:00:40,540 - root - INFO - Skipping pipeline test during startup (will be tested on first query)
2025-07-15 00:00:40,540 - root - INFO - HR Assistant started successfully
2025-07-15 00:00:58,339 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 00:00:59,238 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 00:00:59,681 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
2025-07-15 00:00:59,697 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 00:00:59,710 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 00:00:59,710 - root - INFO - [LLM CONFIG] model_priority=1, groq_api_key=set, google_api_key=set
2025-07-15 00:00:59,710 - root - INFO - [LLM CONFIG] Selecting Groq as provider
2025-07-15 00:01:05,261 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333 "HTTP/1.1 200 OK"
2025-07-15 00:01:08,529 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections "HTTP/1.1 200 OK"
2025-07-15 00:01:08,822 - httpx - INFO - HTTP Request: GET https://17afb374-9be3-4555-8e22-0dca22560995.eu-west-1-0.aws.cloud.qdrant.io:6333/collections/HRDOCS "HTTP/1.1 200 OK"
